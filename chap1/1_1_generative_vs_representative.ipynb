{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far from this one. \" \n",
      " jack said, \" how do you explain this? \" \n",
      " \" you will be told soon enough. there are a myriad of levels of\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "input_ids = tokenizer.encode(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=42, do_sample=True) \n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far from this one, as the light of jupiter's new light reflected off the frozen, frozen ice. the star ships, which the inhabitants aboard the fleet had used before\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=42, do_sample=True) \n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far off, we were working on the same quantum project, the same concept of the universe where we were supposed to go. i think there was a long period of time\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "input_ = tokenizer(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs['input_ids'], max_length=42, do_sample=True, top_k=50, temperature=0.7) \n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far away. i didn't know how the people of this world were supposed to understand. i was in the way. i tried to warn them. i tried to help\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "inputs = tokenizer(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs['input_ids'], max_length=42, do_sample=True, top_k=50, temperature=0.7) \n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far away. the earth is called earth, and there was a small settlement in the area where we made our first home. i'm afraid we've never seen it,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "inputs = tokenizer(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs['input_ids'], max_length=42, do_sample=True) \n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a long time ago, in a galaxy far away. \n",
      " \" i'm sorry, \" i say. \" i'm sorry i didn't tell you. \" \n",
      " \" it's okay, \" he says.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-community/openai-gpt\")\n",
    "\n",
    "inputs = tokenizer(\"A long time ago, in a galaxy far\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=42) # the ** is used to unpack a dictionary into keyword arguments\n",
    "text_generated = tokenizer.decode(outputs[0])\n",
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With Sampling (do_sample=True):* The model doesn't always pick the most likely token. Instead, it randomly samples from the distribution of possible next tokens according to their probabilities. This introduces an element of randomness, allowing for more diverse and creative outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A long time ago, I was a little bit of a fan of the original Star Wars trilogy. I was a little bit of a fan of the original Star Wars trilogy. I was a little bit of a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can also use 'gpt2-medium', 'gpt2-large', etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Encode the input text\n",
    "input_text = \"A long time ago\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "pad_token_id = tokenizer.eos_token_id  \n",
    "\n",
    "# Generate text completion\n",
    "output = model.generate(input_ids, \n",
    "                        max_length=42, \n",
    "                        attention_mask=attention_mask,\n",
    "                        pad_token_id=pad_token_id,\n",
    "                        )\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0])\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous generation of NLP: representation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0.1459505707025528\n",
      "like 0.041577354073524475\n",
      "coding 0.03476494178175926\n",
      "enjoy 0.01915225386619568\n",
      "I 0.01613021455705166\n",
      "love 0.008826184086501598\n",
      "machine 0.004842504393309355\n",
      "analysis -0.11410722881555557\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define a list of sentences\n",
    "sentences = [['I', 'love', 'machine', 'learning'],\n",
    "             ['I', 'enjoy', 'coding'],\n",
    "             ['I', 'like', 'data', 'analysis']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get the word vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Find similar words\n",
    "similar_words = word_vectors.most_similar('learning')\n",
    "\n",
    "# Print the similar words\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.13227147e-03, -4.45733406e-03, -1.06835726e-03,  1.00636482e-03,\n",
       "       -1.91113955e-04,  1.14817743e-03,  6.11386076e-03, -2.02715401e-05,\n",
       "       -3.24596534e-03, -1.51072862e-03,  5.89729892e-03,  1.51410222e-03,\n",
       "       -7.24261976e-04,  9.33324732e-03, -4.92128357e-03, -8.38409644e-04,\n",
       "        9.17541143e-03,  6.74942741e-03,  1.50285603e-03, -8.88256077e-03,\n",
       "        1.14874600e-03, -2.28825561e-03,  9.36823711e-03,  1.20992784e-03,\n",
       "        1.49006362e-03,  2.40640994e-03, -1.83600665e-03, -4.99963388e-03,\n",
       "        2.32429506e-04, -2.01418041e-03,  6.60093315e-03,  8.94012302e-03,\n",
       "       -6.74754381e-04,  2.97701475e-03, -6.10765442e-03,  1.69932481e-03,\n",
       "       -6.92623248e-03, -8.69402662e-03, -5.90020278e-03, -8.95647518e-03,\n",
       "        7.27759488e-03, -5.77203138e-03,  8.27635173e-03, -7.24354526e-03,\n",
       "        3.42167495e-03,  9.67499893e-03, -7.78544787e-03, -9.94505733e-03,\n",
       "       -4.32914635e-03, -2.68313056e-03, -2.71289347e-04, -8.83155130e-03,\n",
       "       -8.61755759e-03,  2.80021061e-03, -8.20640661e-03, -9.06933658e-03,\n",
       "       -2.34046578e-03, -8.63180775e-03, -7.05664977e-03, -8.40115082e-03,\n",
       "       -3.01328895e-04, -4.56429832e-03,  6.62717456e-03,  1.52716041e-03,\n",
       "       -3.34147573e-03,  6.10897178e-03, -6.01328490e-03, -4.65616956e-03,\n",
       "       -7.20750913e-03, -4.33658017e-03, -1.80932996e-03,  6.48964290e-03,\n",
       "       -2.77039292e-03,  4.91896737e-03,  6.90444233e-03, -7.46370573e-03,\n",
       "        4.56485013e-03,  6.12697843e-03, -2.95447465e-03,  6.62502181e-03,\n",
       "        6.12587947e-03, -6.44348515e-03, -6.76455162e-03,  2.53895880e-03,\n",
       "       -1.62381888e-03, -6.06512791e-03,  9.49920900e-03, -5.13014663e-03,\n",
       "       -6.55409694e-03, -1.19885204e-04, -2.70142802e-03,  4.44400299e-04,\n",
       "       -3.53745813e-03, -4.19330609e-04, -7.08615757e-04,  8.22820642e-04,\n",
       "        8.19481723e-03, -5.73670724e-03, -1.65952800e-03,  5.57160750e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(\"This is a simple example.\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "tensor([[-0.2490, -0.1667, -0.2463,  ..., -0.4247,  0.4081,  0.8204],\n",
      "        [-0.4613,  0.0933,  0.3775,  ..., -0.5355, -0.0947,  0.5958]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is a simple example.\", \"Hugging Face makes it easy to use transformers.\"]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state (embeddings) of the [CLS] token is usually used as the sentence representation\n",
    "embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Print the shape of the embeddings (should be [batch_size, hidden_size])\n",
    "print(embeddings.shape)\n",
    "\n",
    "# Optionally, print the embeddings for each sentence\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of last hidden state: torch.Size([1, 15, 768])\n",
      "Shape of [CLS] token embedding: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, this is a simple representation model using RoBERTa.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract last hidden state\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the last hidden state\n",
    "print(\"Shape of last hidden state:\", last_hidden_state.shape)\n",
    "\n",
    "# Optionally, you can get the embeddings for the [CLS] token\n",
    "cls_embedding = last_hidden_state[:, 0, :]\n",
    "print(\"Shape of [CLS] token embedding:\", cls_embedding.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
