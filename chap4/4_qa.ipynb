{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai chromadb tiktoken pypdf llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomli, os\n",
    "with open(\"../.streamlit/secrets.toml\",\"rb\") as f:\n",
    "    secrets = tomli.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to load your OPENAI_API_KEY as env variable\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "def ask(question):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ask chatGPT directly for an evolving topic, like twitter, you will depend on the training set of the model. If the model was trained on a dataset that is not up to date, the model will not be able to answer your question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of September 2021, the CEO of Twitter is Jack Dorsey.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'who is the CEO of twitter?'\n",
    "ask(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a simple search engine that will be able to answer questions about an evolving topic. In the following example, we will parse the Google result page. It provides typically 10 results, sometimes prefaced by a “featured snippet”  followed by other questions that “people also ask”, and at the end of the list of results, a few “related searches”. We will then check the number of tokens on the page to make sure that it isn’t too long to “stuff” it with the initial question into a prompt for ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8162"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "prompt = 'who is the CEO of twitter?'\n",
    "\n",
    "def search(prompt):\n",
    "    url = f'https://www.google.com/search?q={prompt}'\n",
    "    html = requests.get(url).text\n",
    "    with open('search.html','w') as f:\n",
    "        f.write(html)\n",
    "    # Get the text of the webpage\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "text = search(prompt)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2116"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "def num_tokens(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding_name = 'cl100k_base'\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "num_tokens(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The google result page is typically dense enough that we can simply stuff it into a model and get a good answer. Sometimes, you might want to retrieve the top 3 or 5 pages from the search to get a more comprehensive answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CEO of Twitter is Linda Yaccarino.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = f\"\"\"Given the following context of Google search, answer the question:\n",
    "{prompt}\n",
    "---\n",
    "Here is the context retrieve from Google search:\n",
    "{text}\n",
    "\"\"\"\n",
    "ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: First paragraph of chapter 1 of [A Tale of Two Cities by Charles Dickens](https://www.gutenberg.org/ebooks/98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' It was the best of times',\n",
       " 'it was the worst of times',\n",
       " 'it was the age of wisdom',\n",
       " 'it was the age of foolishness',\n",
       " 'it was the epoch of belief',\n",
       " 'it was the epoch of incredulity',\n",
       " 'it was the season of Light',\n",
       " 'it was the season of Darkness',\n",
       " 'it was the spring of hope',\n",
       " 'it was the winter of despair',\n",
       " 'we had everything before us',\n",
       " 'we had nothing before us',\n",
       " 'we were all going direct to Heaven',\n",
       " 'we were all going direct the other way--in short',\n",
       " 'the period was so far like the present period',\n",
       " 'that some of its noisiest authorities insisted on its being received',\n",
       " 'for good or for evil',\n",
       " 'in the superlative degree of comparison only. ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "It was the best of times, it was the worst of times, it was the age of\n",
    "wisdom, it was the age of foolishness, it was the epoch of belief, it\n",
    "was the epoch of incredulity, it was the season of Light, it was the\n",
    "season of Darkness, it was the spring of hope, it was the winter of\n",
    "despair, we had everything before us, we had nothing before us, we were\n",
    "all going direct to Heaven, we were all going direct the other way--in\n",
    "short, the period was so far like the present period, that some of its\n",
    "noisiest authorities insisted on its being received, for good or for\n",
    "evil, in the superlative degree of comparison only.\n",
    "\"\"\"\n",
    "sentences = paragraph.replace(\"\\n\",\" \").split(\", \")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"it caught him off guard that space smelled of seared steak\",\n",
    "    \"she could not decide between painting her teeth or brushing her nails\",\n",
    "    \"he thought there'd be sufficient time is he hid his watch\",\n",
    "    \"the bees decided to have a mutiny against their queen\",\n",
    "    \"the sign said there was road work ahead so she decided to speed up\",\n",
    "    \"on a scale of one to ten, what's your favorite flavor of color?\",\n",
    "    \"flying stinging insects rebelled in opposition to the matriarch\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.007199400570243597, -0.01370294764637947, 0.02629520744085312]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "sentence = sentences[0]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=sentence,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "print(len(embedding))\n",
    "embedding[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.19940057e-03, -1.37029476e-02,  2.62952074e-02],\n",
       "       [-2.73702480e-02, -1.38540762e-02,  4.92458791e-02],\n",
       "       [ 4.99483123e-02,  4.94058040e-05,  6.35890104e-03]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "response = client.embeddings.create(\n",
    "    input=sentences,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "v = [d.embedding for d in response.data]\n",
    "v = np.array(v)\n",
    "v[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.674606291470473"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "cosine_similarity(v[0], v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' It was the best of times', 'it was the worst of times', 0.674606291470473)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0], sentences[1], cosine_similarity(v[0], v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it was the age of wisdom',\n",
       " 'it was the age of foolishness',\n",
       " 0.8072276532681985)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2], sentences[3], cosine_similarity(v[2], v[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides the cosine_similarity function in the sklearn.metrics.pairwise module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6746062914704734"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming vec1 and vec2 are 1-D numpy arrays\n",
    "cosine_similarity([v[0]],[v[1]])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy provides the spatial.distance.cosine function to compute the cosine distance, which can be converted to similarity by subtracting from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.674606291470473"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "similarity = 1 - cosine(v[0], v[1])\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative embedding models: Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n",
      "c:\\Users\\ydebray\\Downloads\\gpt-programming-book\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ydebray\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 5.25MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 35.1kB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<00:00, 683kB/s]\n",
      "model.safetensors: 100%|██████████| 90.9M/90.9M [00:01<00:00, 73.4MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 677kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 34.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 44.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-4.52113152e-02,  7.17497021e-02,  1.72493700e-02, -3.16724032e-02,\n",
       "        4.76625152e-02, -8.03213287e-03,  1.21740187e-02,  5.78135513e-02,\n",
       "       -2.35217176e-02,  5.41551213e-04, -5.85709698e-03,  1.64452136e-01,\n",
       "        8.72123465e-02,  5.75544350e-02, -9.70988721e-03,  2.56951135e-02,\n",
       "        3.18162329e-02,  4.86325705e-03, -2.24988852e-02, -3.80984396e-02,\n",
       "       -5.67632802e-02, -2.70305481e-02,  1.22782504e-02,  4.93002757e-02,\n",
       "        9.22550075e-03,  3.00662238e-02,  7.22770486e-03,  6.36119768e-02,\n",
       "       -2.89241765e-02,  3.43357697e-02, -3.90363559e-02, -2.01459415e-02,\n",
       "        9.26952867e-04,  1.15649151e-02, -5.66930957e-02,  3.26514547e-03,\n",
       "        1.08313095e-02, -6.76954165e-02,  2.46112198e-02, -8.44309398e-04,\n",
       "        3.83343524e-03, -3.42086218e-02, -2.42658071e-02, -8.44405144e-02,\n",
       "        1.69295315e-02,  3.31159658e-03,  8.05927143e-02, -8.86978805e-02,\n",
       "        5.58701269e-02,  2.43993625e-02,  4.77015637e-02, -1.13933217e-02,\n",
       "        4.62255739e-02, -8.06442946e-02,  7.73839504e-02,  4.92016003e-02,\n",
       "        2.02803332e-02, -2.39116922e-02,  5.23987673e-02, -9.84458532e-03,\n",
       "       -5.51757440e-02,  3.14025134e-02, -3.32782865e-02,  7.91446269e-02,\n",
       "        2.84673050e-02, -2.83041149e-02, -3.83780971e-02,  5.41518182e-02,\n",
       "       -1.50069417e-02,  1.18344091e-01, -6.50361404e-02,  1.73775349e-02,\n",
       "       -8.08141939e-03, -5.38706705e-02,  1.65582951e-02,  4.65463921e-02,\n",
       "        8.05103034e-03, -3.09553780e-02, -4.53093834e-02,  7.62139857e-02,\n",
       "        4.54177596e-02, -5.61302118e-02, -7.77680799e-03,  4.48390096e-02,\n",
       "       -4.10386035e-03, -3.02751083e-02,  2.92290915e-02, -5.23251779e-02,\n",
       "        5.65139316e-02, -3.53292264e-02, -2.07767766e-02,  1.59670841e-02,\n",
       "       -9.15671047e-03,  4.11214903e-02,  5.17296158e-02, -9.99907255e-02,\n",
       "       -3.56474072e-02,  1.87675580e-02,  2.39613429e-02,  1.13900304e-01,\n",
       "        3.48202011e-04, -5.07273823e-02, -3.68854925e-02, -4.19472270e-02,\n",
       "        5.11331484e-02, -5.20381555e-02,  6.42375723e-02,  1.74649991e-02,\n",
       "       -8.84147920e-03, -6.17159822e-04, -1.84453912e-02,  1.40904142e-02,\n",
       "       -4.27500159e-03,  3.47954705e-02,  2.22848281e-02, -5.83615154e-02,\n",
       "       -2.60917787e-02,  7.54645746e-03,  7.15776114e-03, -2.32884828e-02,\n",
       "        9.81088541e-03,  5.95434047e-02,  3.30175110e-03, -1.41881853e-02,\n",
       "       -3.65986228e-02, -6.85162321e-02,  1.12868302e-01, -6.69677809e-33,\n",
       "        1.13203200e-02, -3.30685526e-02, -6.33635074e-02, -3.75589505e-02,\n",
       "        8.00295845e-02,  7.88906962e-02, -7.07534421e-03, -1.08571546e-02,\n",
       "       -3.01044453e-02, -4.30687480e-02,  1.90710444e-02,  1.81565024e-02,\n",
       "       -5.45945838e-02, -1.59389332e-01, -3.12618837e-02, -2.19393545e-03,\n",
       "       -7.66789392e-02,  1.17116660e-01,  9.23946779e-03,  3.41383331e-02,\n",
       "       -7.08803758e-02,  4.39169258e-02,  3.86345573e-02, -3.51145081e-02,\n",
       "        7.05164392e-03,  1.96566153e-02, -8.02912470e-03, -4.89822030e-02,\n",
       "        7.13064149e-02,  2.80763768e-02,  4.72289100e-02, -3.29333209e-02,\n",
       "       -4.33145091e-02, -7.17069441e-03,  6.26487881e-02,  5.58011569e-02,\n",
       "        8.62208307e-02,  4.62085642e-02, -3.85327823e-02,  6.40223548e-02,\n",
       "        2.44101100e-02,  2.48048957e-02,  1.70958564e-02,  8.53385124e-03,\n",
       "        4.76603732e-02,  6.02798201e-02, -6.87845200e-02,  3.82208526e-02,\n",
       "        1.95514932e-02, -5.86133301e-02, -2.24246122e-02, -2.57482231e-02,\n",
       "        7.31614381e-02, -2.81049646e-02, -7.74884596e-03,  1.63750462e-02,\n",
       "        1.98300257e-02,  2.46481467e-02, -1.82359852e-02,  6.74157590e-02,\n",
       "       -2.78643612e-02, -1.12767797e-02,  5.81382075e-03, -3.17506641e-02,\n",
       "       -7.54593387e-02, -2.68214568e-02,  7.23884851e-02,  3.61224376e-02,\n",
       "       -3.53378430e-02,  1.36577105e-03, -5.64781539e-02, -1.95801910e-02,\n",
       "       -2.81536616e-02, -8.55070725e-02,  2.74197627e-02, -6.28455505e-02,\n",
       "        4.10031676e-02,  1.09096831e-02, -3.11086271e-02,  2.29233690e-02,\n",
       "       -1.32704701e-03, -5.71532212e-02, -2.75872033e-02, -7.93666393e-02,\n",
       "        1.05154134e-01,  1.55960834e-02,  4.84462492e-02, -1.36447370e-01,\n",
       "       -3.40966210e-02,  7.03741238e-02, -1.01598904e-01, -8.61682277e-03,\n",
       "        1.26939118e-01, -1.09175332e-01, -4.84451838e-02,  4.16595088e-33,\n",
       "        5.48702152e-03,  1.25136385e-02,  6.26616329e-02,  6.41082302e-02,\n",
       "       -5.84565103e-02, -2.87359525e-02, -7.82924592e-02,  1.02295568e-02,\n",
       "       -2.79635135e-02,  1.46740884e-01,  4.20651957e-02,  2.23299339e-02,\n",
       "        1.12029791e-01, -1.14244306e-02,  3.45524438e-02,  4.32388932e-02,\n",
       "        6.31906390e-02, -8.55078623e-02, -1.32303173e-02, -1.20346276e-02,\n",
       "        1.02028521e-02,  4.19555567e-02,  3.57611403e-02, -2.72470564e-02,\n",
       "       -4.08733487e-02,  1.92386471e-02, -5.52896261e-02,  1.56124318e-02,\n",
       "       -4.93106619e-02, -6.82239905e-02,  1.42685920e-02,  5.93141802e-02,\n",
       "       -4.84529771e-02,  4.00741138e-02, -1.70660508e-03,  4.14127037e-02,\n",
       "        2.99580954e-02,  1.39188934e-02, -7.90166706e-02,  6.56117359e-03,\n",
       "       -1.96050610e-02, -1.70776378e-02, -4.79945987e-02,  7.92683885e-02,\n",
       "       -1.75122786e-02,  1.62322167e-03, -3.13448273e-02, -4.89064446e-03,\n",
       "        1.05817124e-01,  4.54730541e-02, -6.07802048e-02, -7.24230036e-02,\n",
       "       -8.36912170e-02,  2.66641495e-03, -4.47358890e-03, -2.70283669e-02,\n",
       "        1.56610273e-03,  1.96771808e-02, -4.37960587e-02, -5.00933267e-02,\n",
       "       -1.10622257e-01, -6.18632846e-02, -1.00449726e-01,  1.24615496e-02,\n",
       "        1.00633614e-01,  3.94211411e-02,  2.77477913e-02, -6.95908666e-02,\n",
       "       -1.01023667e-01, -1.57454386e-02, -9.32348296e-02,  4.61642779e-02,\n",
       "       -1.71080172e-01,  8.34205449e-02, -2.98018083e-02,  4.22265641e-02,\n",
       "       -1.40163992e-02,  3.08868922e-02,  3.55567405e-04, -2.50651594e-02,\n",
       "       -3.58089954e-02,  4.61519137e-03, -4.91815954e-02,  2.07739770e-02,\n",
       "       -5.78580946e-02,  7.72403479e-02,  5.30931391e-02, -8.23195279e-02,\n",
       "        3.20399442e-04, -2.41079107e-02, -3.84561084e-02, -9.18847695e-03,\n",
       "        3.95091884e-02,  1.10604070e-01,  3.25212926e-02, -1.92879810e-08,\n",
       "       -5.99389616e-03,  4.20467146e-02, -9.59946066e-02, -5.79819009e-02,\n",
       "       -5.06616198e-02,  1.23541271e-02,  9.40970331e-02,  4.27266769e-03,\n",
       "        2.03807298e-02,  6.37772754e-02, -6.34023845e-02,  8.73347744e-02,\n",
       "       -3.29390764e-02,  7.92225674e-02,  4.45419736e-02, -6.22999780e-02,\n",
       "        8.59766528e-02, -1.00251466e-01, -4.08711731e-02,  3.84834334e-02,\n",
       "       -2.47430895e-02,  7.44822901e-03,  2.05848850e-02,  7.04647182e-03,\n",
       "       -3.97681296e-02,  3.53248939e-02, -3.13028954e-02, -4.92407791e-02,\n",
       "        6.57861168e-03,  5.68760037e-02, -3.30702737e-02, -1.25758909e-02,\n",
       "       -8.74012560e-02,  3.48217636e-02, -4.81590740e-02, -1.86604343e-03,\n",
       "       -1.61177199e-02,  2.82104556e-02,  8.62379819e-02, -3.93616445e-02,\n",
       "        1.88914370e-02,  5.96437566e-02, -4.53303382e-02,  1.10510709e-02,\n",
       "        2.48619560e-02,  5.89781292e-02,  3.41332778e-02,  2.91972794e-02,\n",
       "       -2.64591426e-02, -4.84790169e-02, -6.81259781e-02,  2.89801899e-02,\n",
       "        1.15712993e-02,  5.65522723e-02,  6.87648952e-02,  4.88551520e-02,\n",
       "       -7.85460323e-03, -3.94344553e-02, -6.77865371e-02, -3.16489637e-02,\n",
       "        3.75959054e-02,  7.56728947e-02, -4.07598354e-02,  1.11838952e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # or 'all-mpnet-base-v2'\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7910879850387573, ' It was the best of times', 'it was the worst of times')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "vec1 = model.encode(sentences[0])\n",
    "vec2 = model.encode(sentences[1])\n",
    "\n",
    "similarity = util.pytorch_cos_sim(vec1, vec2).tolist()\n",
    "similarity[0][0],sentences[0],sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex: building an index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Load data (PDF reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impromptu\n",
      "Amplifying Our Humanity \n",
      "Through AI\n",
      "By Reid Hoffman  \n",
      "with GPT-4\n"
     ]
    }
   ],
   "source": [
    "import requests, io, pypdf\n",
    "# get the impromptu book\n",
    "url = 'https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf'\n",
    "\n",
    "def pdf_to_pages(file):\n",
    "\t\"extract text (pages) from pdf file\"\n",
    "\tpages = []\n",
    "\tpdf = pypdf.PdfReader(file)\n",
    "\tfor p in range(len(pdf.pages)):\n",
    "\t\tpage = pdf.pages[p]\n",
    "\t\ttext = page.extract_text()\n",
    "\t\tpages += [text]\n",
    "\treturn pages\n",
    "\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "pages = pdf_to_pages(f)\n",
    "print(pages[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the content of the PDF into txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"impromptu\"):\n",
    "    os.mkdir(\"impromptu\")\n",
    "for i, page in enumerate(pages):\n",
    "    with open(f\"impromptu/{i}.txt\",\"w\", encoding='utf-8') as f:\n",
    "        f.write(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impromptu\n",
      "Amplifying Our Humanity \n"
     ]
    }
   ],
   "source": [
    "sep = '\\n'\n",
    "book = sep.join(pages)\n",
    "print(book[0:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83310"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "def num_tokens(string):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding_name = 'cl100k_base'\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: Build an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='864d60cc-71a5-48c8-9692-a837cd438ce8', embedding=None, metadata={'file_path': 'impromptu\\\\1.txt', 'file_name': '1.txt', 'file_type': 'text/plain', 'file_size': 78, 'creation_date': '2023-12-04', 'last_modified_date': '2024-04-22', 'last_accessed_date': '2024-04-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, hash='54b42932d386d64b974600eabc107b410fd93cf33967e87617626d44bec310f3', text='Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
    "documents = SimpleDirectoryReader(\"impromptu\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '864d60cc-71a5-48c8-9692-a837cd438ce8',\n",
       " 'embedding': None,\n",
       " 'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "  'file_name': '1.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 78,\n",
       "  'creation_date': '2023-12-04',\n",
       "  'last_modified_date': '2024-04-22',\n",
       "  'last_accessed_date': '2024-04-22'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'hash': '54b42932d386d64b974600eabc107b410fd93cf33967e87617626d44bec310f3',\n",
       " 'text': 'Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n',\n",
       " 'class_name': 'Document'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3: Query the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The potential of AI in education is significant. It has the capability to transform the way we learn and deliver instruction by providing personalized, individualized learning experiences tailored to each student's needs and interests. AI can identify the topics and skills students need to focus on, offer guidance and support, and enable more direct interaction between students and teachers for engaging and meaningful instruction. Additionally, AI-driven tools can automate mundane teaching tasks like grading and content creation, allowing teachers more time to focus on inspiring their students.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query('what is the potential of AI in education?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 621f0645-12b6-4422-bfc9-208ba5f1409c): 47Education\\nthe technology will also create an educational system \\nthat is less equitable and acc...\\n\\n> Source (Doc id: 90b23e79-3feb-4fc3-b511-4c81f7e34dc5): 46Impromptu: Amplifying Our Humanity Through AI\\nReid: GPT-4, there are so many more subjects I wa...'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47Education\n"
     ]
    }
   ],
   "source": [
    "sources = [s.node.get_text() for s in response.source_nodes]\n",
    "# print(len(sources))\n",
    "print(sources[0][0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optional: Look under the hood of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default__vector_store.json',\n",
       " 'docstore.json',\n",
       " 'graph_store.json',\n",
       " 'index_store.json']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents are assigned unique identifiers like `864d60cc-71a5-48c8-9692-a837cd438ce8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['docstore/metadata', 'docstore/data', 'docstore/ref_doc_info'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "with open('storage/docstore.json','r') as f:\n",
    "    docstore = json.load(f)\n",
    "docstore.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ids': ['6b45f3cd-7391-4631-a6b0-dbdd65de5f26'],\n",
       " 'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "  'file_name': '1.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 78,\n",
       "  'creation_date': '2023-12-04',\n",
       "  'last_modified_date': '2024-04-22',\n",
       "  'last_accessed_date': '2024-04-22'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docstore['docstore/ref_doc_info']['864d60cc-71a5-48c8-9692-a837cd438ce8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_hash': '8a044acf635232523b775ad356f1d9af9edfcf497b8daaec2af8ca3685c6f878',\n",
       " 'ref_doc_id': '864d60cc-71a5-48c8-9692-a837cd438ce8'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore['docstore/metadata']['6b45f3cd-7391-4631-a6b0-dbdd65de5f26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__data__': {'id_': '6b45f3cd-7391-4631-a6b0-dbdd65de5f26',\n",
       "  'embedding': None,\n",
       "  'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "   'file_name': '1.txt',\n",
       "   'file_type': 'text/plain',\n",
       "   'file_size': 78,\n",
       "   'creation_date': '2023-12-04',\n",
       "   'last_modified_date': '2024-04-22',\n",
       "   'last_accessed_date': '2024-04-22'},\n",
       "  'excluded_embed_metadata_keys': ['file_name',\n",
       "   'file_type',\n",
       "   'file_size',\n",
       "   'creation_date',\n",
       "   'last_modified_date',\n",
       "   'last_accessed_date'],\n",
       "  'excluded_llm_metadata_keys': ['file_name',\n",
       "   'file_type',\n",
       "   'file_size',\n",
       "   'creation_date',\n",
       "   'last_modified_date',\n",
       "   'last_accessed_date'],\n",
       "  'relationships': {'1': {'node_id': '864d60cc-71a5-48c8-9692-a837cd438ce8',\n",
       "    'node_type': '4',\n",
       "    'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "     'file_name': '1.txt',\n",
       "     'file_type': 'text/plain',\n",
       "     'file_size': 78,\n",
       "     'creation_date': '2023-12-04',\n",
       "     'last_modified_date': '2024-04-22',\n",
       "     'last_accessed_date': '2024-04-22'},\n",
       "    'hash': '54b42932d386d64b974600eabc107b410fd93cf33967e87617626d44bec310f3',\n",
       "    'class_name': 'RelatedNodeInfo'},\n",
       "   '2': {'node_id': '656b5110-9815-4d9f-a8d8-c20ab06ea28a',\n",
       "    'node_type': '1',\n",
       "    'metadata': {'file_path': 'impromptu\\\\0.txt',\n",
       "     'file_name': '0.txt',\n",
       "     'file_type': 'text/plain',\n",
       "     'file_size': 0,\n",
       "     'creation_date': '2023-12-04',\n",
       "     'last_modified_date': '2024-04-22',\n",
       "     'last_accessed_date': '2024-04-22'},\n",
       "    'hash': '44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a',\n",
       "    'class_name': 'RelatedNodeInfo'},\n",
       "   '3': {'node_id': 'a8a370fe-eff2-491e-805d-108613f59c87',\n",
       "    'node_type': '1',\n",
       "    'metadata': {},\n",
       "    'hash': '84bbeb5bc3aab826fe5da6e32320bfe19286db6b1e180a4c2dc2cb64b78ddcac',\n",
       "    'class_name': 'RelatedNodeInfo'}},\n",
       "  'hash': '8a044acf635232523b775ad356f1d9af9edfcf497b8daaec2af8ca3685c6f878',\n",
       "  'text': 'Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4',\n",
       "  'start_char_idx': 0,\n",
       "  'end_char_idx': 74,\n",
       "  'text_template': '{metadata_str}\\n\\n{content}',\n",
       "  'metadata_template': '{key}: {value}',\n",
       "  'metadata_seperator': '\\n',\n",
       "  'class_name': 'TextNode'},\n",
       " '__type__': '1'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore['docstore/data']['6b45f3cd-7391-4631-a6b0-dbdd65de5f26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__data__': {'id_': '6b45f3cd-7391-4631-a6b0-dbdd65de5f26',\n",
       "  'embedding': None,\n",
       "  'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "   'file_name': '1.txt',\n",
       "   'file_type': 'text/plain',\n",
       "   'file_size': 78,\n",
       "   'creation_date': '2023-12-04',\n",
       "   'last_modified_date': '2024-04-22',\n",
       "   'last_accessed_date': '2024-04-22'},\n",
       "  'excluded_embed_metadata_keys': ['file_name',\n",
       "   'file_type',\n",
       "   'file_size',\n",
       "   'creation_date',\n",
       "   'last_modified_date',\n",
       "   'last_accessed_date'],\n",
       "  'excluded_llm_metadata_keys': ['file_name',\n",
       "   'file_type',\n",
       "   'file_size',\n",
       "   'creation_date',\n",
       "   'last_modified_date',\n",
       "   'last_accessed_date'],\n",
       "  'relationships': {'1': {'node_id': '864d60cc-71a5-48c8-9692-a837cd438ce8',\n",
       "    'node_type': '4',\n",
       "    'metadata': {'file_path': 'impromptu\\\\1.txt',\n",
       "     'file_name': '1.txt',\n",
       "     'file_type': 'text/plain',\n",
       "     'file_size': 78,\n",
       "     'creation_date': '2023-12-04',\n",
       "     'last_modified_date': '2024-04-22',\n",
       "     'last_accessed_date': '2024-04-22'},\n",
       "    'hash': '54b42932d386d64b974600eabc107b410fd93cf33967e87617626d44bec310f3',\n",
       "    'class_name': 'RelatedNodeInfo'},\n",
       "   '2': {'node_id': '656b5110-9815-4d9f-a8d8-c20ab06ea28a',\n",
       "    'node_type': '1',\n",
       "    'metadata': {'file_path': 'impromptu\\\\0.txt',\n",
       "     'file_name': '0.txt',\n",
       "     'file_type': 'text/plain',\n",
       "     'file_size': 0,\n",
       "     'creation_date': '2023-12-04',\n",
       "     'last_modified_date': '2024-04-22',\n",
       "     'last_accessed_date': '2024-04-22'},\n",
       "    'hash': '44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a',\n",
       "    'class_name': 'RelatedNodeInfo'},\n",
       "   '3': {'node_id': 'a8a370fe-eff2-491e-805d-108613f59c87',\n",
       "    'node_type': '1',\n",
       "    'metadata': {},\n",
       "    'hash': '84bbeb5bc3aab826fe5da6e32320bfe19286db6b1e180a4c2dc2cb64b78ddcac',\n",
       "    'class_name': 'RelatedNodeInfo'}},\n",
       "  'hash': '8a044acf635232523b775ad356f1d9af9edfcf497b8daaec2af8ca3685c6f878',\n",
       "  'text': 'Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4',\n",
       "  'start_char_idx': 0,\n",
       "  'end_char_idx': 74,\n",
       "  'text_template': '{metadata_str}\\n\\n{content}',\n",
       "  'metadata_template': '{key}: {value}',\n",
       "  'metadata_seperator': '\\n',\n",
       "  'class_name': 'TextNode'},\n",
       " '__type__': '1'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore['docstore/data']['6b45f3cd-7391-4631-a6b0-dbdd65de5f26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embedding_dict', 'text_id_to_ref_doc_id', 'metadata_dict'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('storage/default__vector_store.json','r') as f:\n",
    "    data = json.load(f)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['embedding_dict']['6b45f3cd-7391-4631-a6b0-dbdd65de5f26'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try ChromaDB, and install all necessary dependencies\n",
    "```\n",
    "pip install -U langchain langchain-openai unstructured chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ydebray\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:08<00:00, 10.3MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc1', 'doc2']],\n",
       " 'distances': [[0.9026352763807001, 1.0358158255050436]],\n",
       " 'metadatas': [[{'source': 'notion'}, {'source': 'google-docs'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['This is document1', 'This is document2']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "# client = chromadb.HttpClient()\n",
    "client = chromadb.PersistentClient()\n",
    "collection = client.create_collection(\"sample_collection\")\n",
    "\n",
    "# Add docs to the collection. Can also update and delete. Row-based API coming soon!\n",
    "collection.add(\n",
    "    documents=[\"This is document1\", \"This is document2\"], # we embed for you, or bring your own\n",
    "    metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # filter on arbitrary metadata!\n",
    "    ids=[\"doc1\", \"doc2\"], # must be unique for each doc \n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document\"],\n",
    "    n_results=2,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    ")  \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration in LangChain: Example with chapter 1 of Impromptu on Education (not using the full book to avoid unnecessary cost to create embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pdf and extract chap 32-54\n",
    "import requests, io, pypdf\n",
    "url = 'https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf'\n",
    "r = requests.get(url)\n",
    "f = io.BytesIO(r.content)\n",
    "pdf = pypdf.PdfReader(f)\n",
    "writer = pypdf.PdfWriter()\n",
    "for p in range(31,54):\n",
    "    writer.add_page(pdf.pages[p])\n",
    "with open(\"impromptu_32-54.pdf\",\"wb\") as f:\n",
    "    writer.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the pdf\n",
    "pages = []\n",
    "for p in range(32,55):\n",
    "    page = pdf.pages[p]\n",
    "    text = page.extract_text()\n",
    "    pages += [text]\n",
    "\n",
    "print(pages[0][0:48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document and split it into pages\n",
    "loader = PyPDFLoader(\"impromptu_32-54.pdf\")\n",
    "# loader = TextLoader('impromptu/53.txt')\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# loader = DirectoryLoader('impromptu')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "query = 'what are the opportunities of using AI?'\n",
    "result = qa.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what are the opportunities of using AI?',\n",
       " 'result': 'The opportunities of using AI in education include automating and streamlining mundane tasks like grading and content creation, providing personalized and individualized learning experiences, giving teachers more time to focus on engaging students, and potentially transforming the way we learn and deliver instruction. AI can also help identify topics and skills students need to focus on and provide guidance and support accordingly.',\n",
       " 'source_documents': [Document(page_content='47Education\\nthe technology will also create an educational system \\nthat is less equitable and accessible.\\nMixed: Large language models will have an undeni-\\nable impact on education in the next fifty years, but its \\npotential for transforming the way we learn and deliver \\ninstruction will depend on a range of factors. AI-driven \\ntools will be used to automate and streamline some of \\nthe more mundane aspects of teaching, such as grading \\nand content creation, but their potential for more mean-\\ningful applications will be limited by cost, access, and \\nprivacy concerns. If these issues can be addressed, then \\nAI will become a powerful tool in education, transform-\\ning the way we learn and deliver instruction. AI-driven \\ntools will be used to provide personalized, individualized \\nlearning experiences, as well as to create and curate con-\\ntent, giving teachers more time to focus on engaging \\nand inspiring their students.\\nReid: Thank you, GPT-4! I know which one of those I \\nwant, and I am optimistic that we can work with tools \\nsuch as you to choose that path for ourselves.', metadata={'page': 22, 'source': 'impromptu_32-54.pdf'}),\n",
       "  Document(page_content='46Impromptu: Amplifying Our Humanity Through AI\\nReid: GPT-4, there are so many more subjects I want to \\ndiscuss with you, from reforms to higher education, to \\nrevolutions in language pedagogy, to the philosophy \\nof education. I’d like to ask for your help in taking us \\nhome. We have discussed how GPT paired with great \\nteachers could change lives and perhaps the world. \\nBut we have also acknowledged some of the concerns \\nand challenges that could get in the way. In 300 words \\nor less, please conclude this chapter by outlining an \\noptimistic, a pessimistic, and a mixed scenario for how \\nlarge language models will be used in education in fifty \\nyears’ time.\\nGPT-4:  Optimistic: Large language models will become \\na powerful tool in education in the next fifty years, \\ntransforming the way we learn and deliver instruction. \\nWith GPT, teachers will be able to provide personalized, \\nindividualized learning experiences that are tailored \\nto each student’s needs and interests. AI will be able \\nto identify the topics and skills that students need to \\nfocus on, and provide guidance and support as needed. \\nAI-driven tools will also enable students to interact more \\ndirectly with their teachers, allowing for more engaging \\nand meaningful instruction. AI will also be used to create \\nand curate content, giving teachers more time to focus \\non engaging and inspiring their students.\\nPessimistic: Large language models will be used in \\neducation in the next fifty years, but its potential will not \\nbe fully realized. The technology will be limited by cost, \\naccess, and privacy concerns, which will limit its use to a \\nsmall number of privileged institutions. AI-driven tools \\nwill be used to automate and streamline some of the \\nmore mundane aspects of teaching, such as grading \\nand content creation, but their potential for more mean-\\ningful applications will remain largely untapped. The \\nlack of access and the potential for misuse and abuse of', metadata={'page': 21, 'source': 'impromptu_32-54.pdf'}),\n",
       "  Document(page_content='28Impromptu: Amplifying Our Humanity Through AI\\npredictable or optimal for AI. Turning insights into \\nactions requires judgment, communication, and leader -\\nship, which are not always consistent or effective for AI.\\nReid: In light of that, how do you assess Professor Mintz’ \\nuse of ChatGPT as an instructional tool?\\nGPT-4:  I would grade Professor Mintz highly on his use \\nof ChatGPT, as he demonstrates a creative and critical \\napproach to integrating it into his pedagogy. He does \\nnot rely on ChatGPT to produce or evaluate the essays, \\nbut rather uses it as a catalyst and a contrast for human \\nwriting and thinking.\\nLarge language models in American K–12 \\npublic education\\nAt this point, a skeptic might note that Professor Mintz made \\nthe case for ChatGPT at the University of Texas at Austin, an \\nelite institution serving students on the cusp of adult careers. \\nBut what about high schools? Should they be “protected” from \\nLLMs? Should schools define the technology as “cheating” and \\neven ban it entirely, as New York City’s public schools recently \\ndid for their entire K–12 system?\\nReid: GPT-4, please summarize some benefits and costs \\nof the decision by the New York City Department of Edu-\\ncation to ban ChatGPT.\\nGPT-4:  The decision to ban the use of ChatGPT in New \\nYork City schools may have benefits as well as costs. Pos-\\nsible benefits of the ban include:\\n- It may prevent the misuse or abuse of ChatGPT by \\nmalicious or irresponsible actors.\\n- It may protect the privacy and autonomy of students \\nand teachers.', metadata={'page': 3, 'source': 'impromptu_32-54.pdf'})]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try Facebook AI Similarity Search (faiss), which is known to be insanely performant:\n",
    "```\n",
    "pip install faiss-cpu\n",
    "```\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22: 47Education\n",
      "the technology will also create an e\n",
      "21: 46Impromptu: Amplifying Our Humanity Through AI\n",
      "\n",
      "3: 28Impromptu: Amplifying Our Humanity Through AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"what are the opportunities of using AI?\", k=3)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = 64                           # dimension\n",
    "nb = 100000                      # database size\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xb[:, 0] += np.arange(nb) / 1000.\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "xq[:, 0] += np.arange(nq) / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import faiss                   # make faiss available\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(xb)                  # add vectors to the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 393 363  78]\n",
      " [  1 555 277 364]\n",
      " [  2 304 101  13]\n",
      " [  3 173  18 182]\n",
      " [  4 288 370 531]]\n",
      "[[0.        7.1751733 7.2076297 7.2511625]\n",
      " [0.        6.3235645 6.684581  6.799946 ]\n",
      " [0.        5.7964087 6.391736  7.2815123]\n",
      " [0.        7.2779055 7.5279875 7.662846 ]\n",
      " [0.        6.7638035 7.2951202 7.368815 ]]\n",
      "[[ 381  207  210  477]\n",
      " [ 526  911  142   72]\n",
      " [ 838  527 1290  425]\n",
      " [ 196  184  164  359]\n",
      " [ 526  377  120  425]]\n",
      "[[ 9900 10500  9309  9831]\n",
      " [11055 10895 10812 11321]\n",
      " [11353 11103 10164  9787]\n",
      " [10571 10664 10632  9638]\n",
      " [ 9628  9554 10036  9582]]\n"
     ]
    }
   ],
   "source": [
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity check\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])                  # neighbors of the 5 last queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Impromptu means done without advance preparation or planning; spur of the moment.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import LLMPredictor, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "llm_predictor = LLMPredictor(llm=chat)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "res = chat([HumanMessage(content='What does impromptu mean?')])\n",
    "res.dict()['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from llama_index import download_loader\n",
    "# PDFReader = download_loader(\"PDFReader\")\n",
    "# loader = PDFReader()\n",
    "# documents = loader.load_data(pdf)\n",
    "# documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
