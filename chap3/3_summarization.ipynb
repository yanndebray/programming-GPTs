{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Get your [API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) from OpenAI.com.\n",
    "\n",
    "- Save your API key in `.env` file as `OPENAI_API_KEY = 'sk-...XXXX'`\n",
    "\n",
    "- Load it with [python-dotenv](https://pypi.org/project/python-dotenv/) \n",
    "\n",
    "- or save it in `.streamlit/secrets.toml` and load it with [tomli](https://pypi.org/project/tomli/).\n",
    "\n",
    "\n",
    "Resources: \n",
    "- https://github.com/gkamradt/langchain-tutorials/blob/main/getting_started/Quickstart%20Guide.ipynb\n",
    "- https://www.youtube.com/watch?v=kYRB-vJFy38&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5&index=2\n",
    "- https://python.langchain.com/docs/use_cases/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "# Code of your application, which uses environment variables (e.g. from `os.environ` or\n",
    "# `os.getenv`) as if they came from the actual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomli, os\n",
    "with open(\"../.streamlit/secrets.toml\",\"rb\") as f:\n",
    "    secrets = tomli.load(f)\n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the OpenAI API key is correctly loaded as env variable\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: Why don't scientists trust atoms?\n",
      "A: Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(\n",
    "    # api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n",
    "joke = llm('tell me a joke')\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-davinci-003'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out your first call to OpenAI model `gpt-3.5-turbo` that powers ChatGPT. \n",
    "\n",
    "The cost of the API is: \n",
    "- input: $0.001 per 1k tokens\n",
    "- output: $0.002 per 1k tokens\n",
    "\n",
    "[(November 2023 updated prices)](https://openai.com/pricing#gpt-3-5-turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a classic one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "chat = ChatOpenAI()\n",
    "text = \"Tell me a joke\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "res = chat.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, webvtt\n",
    "files = os.listdir('../data/vtt')\n",
    "print(files) # ['captions.vtt', 'sample.vtt']\n",
    "file = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to start doing this experiment where.\n",
      "We have a conversation we record it's generating a VTT file.\n",
      "And I have a parser. I developed a small app in Python that can retrieve the VTT file process it.\n",
      "And then.\n"
     ]
    }
   ],
   "source": [
    "caption = webvtt.read('../data/vtt/'+file)\n",
    "for cap in caption[1:5]:\n",
    "    # print(f'From {caption.start} to {caption.end}')\n",
    "    # print(caption.raw_text)\n",
    "    print(cap.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the text from the conversation and save it as a plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = file.replace('.vtt','.txt')\n",
    "m = [cap.raw_text for cap in caption]\n",
    "sep = '\\n'\n",
    "convo = sep.join(m)\n",
    "with open('../data/txt/'+txt,mode='w') as f:\n",
    "    f.write(convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the numbers of token in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding_name = 'cl100k_base'\n",
    "encoding = tiktoken.get_encoding(encoding_name)\n",
    "num_tokens = len(encoding.encode(convo))\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yann wants to improve his French accent and plans to conduct an experiment where he records conversations and generates VTT files. He has developed a Python app to process the VTT files and wants to use the ChatGPT API to further analyze them. Mike has not yet used the API due to lack of time.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader('../data/txt/'+txt)\n",
    "docs = loader.load()\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content) # number of characters in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try summarizing a longer text, like the content of the LangChain doc page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(page, model = \"gpt-3.5-turbo\"):  \n",
    "    loader = WebBaseLoader(page)\n",
    "    docs = loader.load()\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "    return chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 7705 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "page = \"https://python.langchain.com/docs/use_cases/summarization\"\n",
    "try:\n",
    "    summary = summarize(page)\n",
    "    print(summary)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LangChain platform offers tools for document summarization using large language models (LLMs). There are three approaches to document summarization: \"stuff,\" \"map-reduce,\" and \"refine.\" The \"stuff\" approach involves inserting all documents into a single prompt, while the \"map-reduce\" approach summarizes each document individually and then combines the summaries into a final summary. The \"refine\" approach iteratively updates the summary by passing each document and the current summary through an LLM chain. The platform provides pre-built chains for each approach, and users can customize prompts and LLM models. Additionally, the platform offers the option to split long documents into chunks and summarize them in a single chain.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = \"https://python.langchain.com/docs/use_cases/summarization\"\n",
    "summarize(page,model = \"gpt-3.5-turbo-16k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce\n",
    "\n",
    "Youtube transcript\n",
    "\n",
    "https://www.geeksforgeeks.org/python-downloading-captions-from-youtube/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'so you got yourself in a bit of trouble',\n",
       "  'start': 0.06,\n",
       "  'duration': 3.96},\n",
       " {'text': 'because open AI returns an error to you',\n",
       "  'start': 1.8,\n",
       "  'duration': 4.62},\n",
       " {'text': 'and that error says that you have', 'start': 4.02, 'duration': 5.7},\n",
       " {'text': \"exceeded the token length that's an\",\n",
       "  'start': 6.42,\n",
       "  'duration': 4.62},\n",
       " {'text': \"issue and we're going to show you four\",\n",
       "  'start': 9.72,\n",
       "  'duration': 2.999},\n",
       " {'text': 'different ways on how to fix that issue',\n",
       "  'start': 11.04,\n",
       "  'duration': 3.42},\n",
       " {'text': \"so first let's set up the problem here\",\n",
       "  'start': 12.719,\n",
       "  'duration': 3.721},\n",
       " {'text': \"just one more time I'm going to copy and\",\n",
       "  'start': 14.46,\n",
       "  'duration': 4.62},\n",
       " {'text': 'paste a short passage into the', 'start': 16.44, 'duration': 4.2},\n",
       " {'text': 'playground on open AI this is the same',\n",
       "  'start': 19.08,\n",
       "  'duration': 3.06},\n",
       " {'text': \"for the API and I'm going to say hey\",\n",
       "  'start': 20.64,\n",
       "  'duration': 3.78},\n",
       " {'text': 'please summarize this thing for me and',\n",
       "  'start': 22.14,\n",
       "  'duration': 3.66},\n",
       " {'text': \"you'll notice it's thinking about it\",\n",
       "  'start': 24.42,\n",
       "  'duration': 4.679},\n",
       " {'text': \"it's trying oh no the mock the model can\",\n",
       "  'start': 25.8,\n",
       "  'duration': 5.58},\n",
       " {'text': 'only process a maximum of 4 000 token',\n",
       "  'start': 29.099,\n",
       "  'duration': 5.101},\n",
       " {'text': 'tokens in a single request this is an',\n",
       "  'start': 31.38,\n",
       "  'duration': 4.56},\n",
       " {'text': \"issue and if you're running a business\",\n",
       "  'start': 34.2,\n",
       "  'duration': 3.539},\n",
       " {'text': \"or you're making a product based on open\",\n",
       "  'start': 35.94,\n",
       "  'duration': 3.24},\n",
       " {'text': \"ad you're gonna have to work your way\",\n",
       "  'start': 37.739,\n",
       "  'duration': 3.721},\n",
       " {'text': \"around this now there's word of them\",\n",
       "  'start': 39.18,\n",
       "  'duration': 5.34},\n",
       " {'text': \"with open ai's Foundry to increase this\",\n",
       "  'start': 41.46,\n",
       "  'duration': 5.64},\n",
       " {'text': \"model or this token length however it's\",\n",
       "  'start': 44.52,\n",
       "  'duration': 4.98},\n",
       " {'text': 'going to cost five or six figures a year',\n",
       "  'start': 47.1,\n",
       "  'duration': 5.939},\n",
       " {'text': \"just to be able to use that one so I'm\",\n",
       "  'start': 49.5,\n",
       "  'duration': 4.739},\n",
       " {'text': 'not holding my breath for it and',\n",
       "  'start': 53.039,\n",
       "  'duration': 3.721},\n",
       " {'text': 'regardless I imagine that model links',\n",
       "  'start': 54.239,\n",
       "  'duration': 4.8},\n",
       " {'text': \"will always be a problem so it's good to\",\n",
       "  'start': 56.76,\n",
       "  'duration': 4.439},\n",
       " {'text': \"learn how to invest I'm going to learn\",\n",
       "  'start': 59.039,\n",
       "  'duration': 3.66},\n",
       " {'text': 'how to figure these out right now now',\n",
       "  'start': 61.199,\n",
       "  'duration': 3.061},\n",
       " {'text': 'the way that I want to talk about these',\n",
       "  'start': 62.699,\n",
       "  'duration': 2.64},\n",
       " {'text': 'four different methods is actually',\n",
       "  'start': 64.26,\n",
       "  'duration': 3.42},\n",
       " {'text': 'starting off with a diagram first and I',\n",
       "  'start': 65.339,\n",
       "  'duration': 5.701},\n",
       " {'text': 'find that this is helpful because',\n",
       "  'start': 67.68,\n",
       "  'duration': 6.24},\n",
       " {'text': \"um well when you get the code it's kind\",\n",
       "  'start': 71.04,\n",
       "  'duration': 4.5},\n",
       " {'text': \"of confusing sometimes and so let's\",\n",
       "  'start': 73.92,\n",
       "  'duration': 3.48},\n",
       " {'text': \"let's go let's go into the diagram first\",\n",
       "  'start': 75.54,\n",
       "  'duration': 4.56},\n",
       " {'text': 'and use some pictures I like pictures',\n",
       "  'start': 77.4,\n",
       "  'duration': 5.1},\n",
       " {'text': \"so let's reframe the problem one more\",\n",
       "  'start': 80.1,\n",
       "  'duration': 5.519},\n",
       " {'text': 'time so open AI has a 4K token limit',\n",
       "  'start': 82.5,\n",
       "  'duration': 5.82},\n",
       " {'text': 'okay now scenario one you give a prompt',\n",
       "  'start': 85.619,\n",
       "  'duration': 4.68},\n",
       " {'text': \"it gives you a response and it's still\",\n",
       "  'start': 88.32,\n",
       "  'duration': 4.26},\n",
       " {'text': \"below the 4K you're golden\", 'start': 90.299, 'duration': 3.781},\n",
       " {'text': \"let's say that you have a short prompt\",\n",
       "  'start': 92.58,\n",
       "  'duration': 3.3},\n",
       " {'text': \"and a longer response as long as it's\",\n",
       "  'start': 94.08,\n",
       "  'duration': 4.2},\n",
       " {'text': \"under the 4K you're still golden now\",\n",
       "  'start': 95.88,\n",
       "  'duration': 4.5},\n",
       " {'text': 'number three long prom short response',\n",
       "  'start': 98.28,\n",
       "  'duration': 4.019},\n",
       " {'text': \"you're still good to go the issue is\",\n",
       "  'start': 100.38,\n",
       "  'duration': 4.86},\n",
       " {'text': 'going to be when you go from a long',\n",
       "  'start': 102.299,\n",
       "  'duration': 4.801},\n",
       " {'text': 'prompt and a long response or any',\n",
       "  'start': 105.24,\n",
       "  'duration': 3.6},\n",
       " {'text': 'combination of the two and you exceed',\n",
       "  'start': 107.1,\n",
       "  'duration': 3.839},\n",
       " {'text': \"the 4K all right so let's figure out how\",\n",
       "  'start': 108.84,\n",
       "  'duration': 3.959},\n",
       " {'text': \"we're going to fix this here with\",\n",
       "  'start': 110.939,\n",
       "  'duration': 4.32},\n",
       " {'text': 'solution number one', 'start': 112.799, 'duration': 4.741},\n",
       " {'text': \"it's what we call stuffing well I guess\",\n",
       "  'start': 115.259,\n",
       "  'duration': 4.261},\n",
       " {'text': \"I shouldn't really call it a solution I\",\n",
       "  'start': 117.54,\n",
       "  'duration': 3.899},\n",
       " {'text': 'should just call this a method to prompt',\n",
       "  'start': 119.52,\n",
       "  'duration': 4.44},\n",
       " {'text': 'uh management if you will so in this',\n",
       "  'start': 121.439,\n",
       "  'duration': 3.78},\n",
       " {'text': \"case we have our document and we're\",\n",
       "  'start': 123.96,\n",
       "  'duration': 2.58},\n",
       " {'text': 'going to try to summarize this document',\n",
       "  'start': 125.219,\n",
       "  'duration': 3.54},\n",
       " {'text': 'our document is only 2K characters long',\n",
       "  'start': 126.54,\n",
       "  'duration': 4.079},\n",
       " {'text': 'so we can feed that right into open Ai',\n",
       "  'start': 128.759,\n",
       "  'duration': 4.441},\n",
       " {'text': 'and we can say hey uh please summarize',\n",
       "  'start': 130.619,\n",
       "  'duration': 3.84},\n",
       " {'text': \"this for me and it's going to give us\",\n",
       "  'start': 133.2,\n",
       "  'duration': 3.36},\n",
       " {'text': \"the response and we're going to stay\",\n",
       "  'start': 134.459,\n",
       "  'duration': 3.601},\n",
       " {'text': 'under the 4K limit which is a good thing',\n",
       "  'start': 136.56,\n",
       "  'duration': 4.2},\n",
       " {'text': 'however if our document is too long',\n",
       "  'start': 138.06,\n",
       "  'duration': 5.58},\n",
       " {'text': \"that's where we run into an issue\",\n",
       "  'start': 140.76,\n",
       "  'duration': 5.04},\n",
       " {'text': 'again we have the 4K limit but our',\n",
       "  'start': 143.64,\n",
       "  'duration': 5.16},\n",
       " {'text': 'document or documents is 8k characters',\n",
       "  'start': 145.8,\n",
       "  'duration': 7.799},\n",
       " {'text': \"to 8 000 tokens long right we can't feed\",\n",
       "  'start': 148.8,\n",
       "  'duration': 7.38},\n",
       " {'text': \"that all into open AI that's an issue\",\n",
       "  'start': 153.599,\n",
       "  'duration': 4.441},\n",
       " {'text': \"and we won't be able to do it it's going\",\n",
       "  'start': 156.18,\n",
       "  'duration': 3.48},\n",
       " {'text': 'to throw an error to us so how do we get',\n",
       "  'start': 158.04,\n",
       "  'duration': 3.24},\n",
       " {'text': \"around this well let's look at prompt\",\n",
       "  'start': 159.66,\n",
       "  'duration': 4.799},\n",
       " {'text': 'management method number two which is',\n",
       "  'start': 161.28,\n",
       "  'duration': 4.2},\n",
       " {'text': 'called', 'start': 164.459, 'duration': 3.301},\n",
       " {'text': 'oh well first of all the pros of this',\n",
       "  'start': 165.48,\n",
       "  'duration': 4.5},\n",
       " {'text': 'one is you get one API call and all of',\n",
       "  'start': 167.76,\n",
       "  'duration': 3.54},\n",
       " {'text': 'your data is in the prompt which is a',\n",
       "  'start': 169.98,\n",
       "  'duration': 2.399},\n",
       " {'text': 'good thing because you have all the',\n",
       "  'start': 171.3,\n",
       "  'duration': 2.48},\n",
       " {'text': 'contacts that you need', 'start': 172.379, 'duration': 4.261},\n",
       " {'text': 'and the language model can uh can use it',\n",
       "  'start': 173.78,\n",
       "  'duration': 5.14},\n",
       " {'text': \"the cons is that there's going to be the\",\n",
       "  'start': 176.64,\n",
       "  'duration': 3.72},\n",
       " {'text': \"limited context length you're going to\",\n",
       "  'start': 178.92,\n",
       "  'duration': 3.42},\n",
       " {'text': 'run into that error limit', 'start': 180.36, 'duration': 3.42},\n",
       " {'text': \"the second method we're going to look at\",\n",
       "  'start': 182.34,\n",
       "  'duration': 3.36},\n",
       " {'text': 'is called mapreduce this is an',\n",
       "  'start': 183.78,\n",
       "  'duration': 3.959},\n",
       " {'text': 'interesting one because again we still',\n",
       "  'start': 185.7,\n",
       "  'duration': 4.8},\n",
       " {'text': 'have our 4K token limit but our document',\n",
       "  'start': 187.739,\n",
       "  'duration': 6.121},\n",
       " {'text': 'is 8k tokens long so what do we do for',\n",
       "  'start': 190.5,\n",
       "  'duration': 5.22},\n",
       " {'text': \"this one well in this case what we're\",\n",
       "  'start': 193.86,\n",
       "  'duration': 3.239},\n",
       " {'text': \"first going to do is we're first going\",\n",
       "  'start': 195.72,\n",
       "  'duration': 3.78},\n",
       " {'text': 'to slice up our document into individual',\n",
       "  'start': 197.099,\n",
       "  'duration': 3.42},\n",
       " {'text': 'pieces', 'start': 199.5, 'duration': 3.18},\n",
       " {'text': \"and with those individual pieces we're\",\n",
       "  'start': 200.519,\n",
       "  'duration': 5.58},\n",
       " {'text': 'going to pass each one over', 'start': 202.68, 'duration': 5.82},\n",
       " {'text': 'come on now', 'start': 206.099, 'duration': 5.041},\n",
       " {'text': \"we're going to pass each one over to\",\n",
       "  'start': 208.5,\n",
       "  'duration': 4.5},\n",
       " {'text': \"open Ai and we're going to say hey\",\n",
       "  'start': 211.14,\n",
       "  'duration': 4.379},\n",
       " {'text': \"here's your prompt and well here's your\",\n",
       "  'start': 213.0,\n",
       "  'duration': 4.2},\n",
       " {'text': 'prompt uh right then and there and',\n",
       "  'start': 215.519,\n",
       "  'duration': 3.481},\n",
       " {'text': 'instead of just giving it one API call',\n",
       "  'start': 217.2,\n",
       "  'duration': 3.42},\n",
       " {'text': \"and one prompt we're going to give it\",\n",
       "  'start': 219.0,\n",
       "  'duration': 4.08},\n",
       " {'text': 'four prompts in four separate API calls',\n",
       "  'start': 220.62,\n",
       "  'duration': 5.16},\n",
       " {'text': \"and we're going to say to open AI hey it\",\n",
       "  'start': 223.08,\n",
       "  'duration': 4.32},\n",
       " {'text': 'would be great if you could please',\n",
       "  'start': 225.78,\n",
       "  'duration': 3.9},\n",
       " {'text': 'summarize this for me and in response',\n",
       "  'start': 227.4,\n",
       "  'duration': 3.24},\n",
       " {'text': \"we're going to get four different\",\n",
       "  'start': 229.68,\n",
       "  'duration': 2.58},\n",
       " {'text': 'summaries because we split it up into',\n",
       "  'start': 230.64,\n",
       "  'duration': 3.48},\n",
       " {'text': \"four different chunks and then we're\",\n",
       "  'start': 232.26,\n",
       "  'duration': 3.839},\n",
       " {'text': 'going to make a fifth call on top of',\n",
       "  'start': 234.12,\n",
       "  'duration': 4.259},\n",
       " {'text': \"that and we're going to say hey given\",\n",
       "  'start': 236.099,\n",
       "  'duration': 3.541},\n",
       " {'text': 'all these summaries that you just have',\n",
       "  'start': 238.379,\n",
       "  'duration': 4.021},\n",
       " {'text': 'give me a final summary or give me a',\n",
       "  'start': 239.64,\n",
       "  'duration': 4.739},\n",
       " {'text': 'summary of the summaries in this case',\n",
       "  'start': 242.4,\n",
       "  'duration': 4.559},\n",
       " {'text': 'and so this is mapreduce now the pros',\n",
       "  'start': 244.379,\n",
       "  'duration': 4.621},\n",
       " {'text': 'about this one is you can scale it to',\n",
       "  'start': 246.959,\n",
       "  'duration': 4.321},\n",
       " {'text': 'pretty large documents which is cool not',\n",
       "  'start': 249.0,\n",
       "  'duration': 4.56},\n",
       " {'text': 'only that it can be parallelized meaning',\n",
       "  'start': 251.28,\n",
       "  'duration': 4.739},\n",
       " {'text': 'you can make all four of these API calls',\n",
       "  'start': 253.56,\n",
       "  'duration': 5.16},\n",
       " {'text': \"in parallel you don't they're not um you\",\n",
       "  'start': 256.019,\n",
       "  'duration': 4.081},\n",
       " {'text': \"don't need to wait for one to return for\",\n",
       "  'start': 258.72,\n",
       "  'duration': 3.84},\n",
       " {'text': 'you to make the next one now the cons of',\n",
       "  'start': 260.1,\n",
       "  'duration': 4.26},\n",
       " {'text': \"this one is you're going to start to\",\n",
       "  'start': 262.56,\n",
       "  'duration': 3.84},\n",
       " {'text': 'increase uh more API calls compared to',\n",
       "  'start': 264.36,\n",
       "  'duration': 4.86},\n",
       " {'text': 'the stuffing method and you might lose a',\n",
       "  'start': 266.4,\n",
       "  'duration': 4.38},\n",
       " {'text': \"little bit of information because you're\",\n",
       "  'start': 269.22,\n",
       "  'duration': 3.3},\n",
       " {'text': 'doing summaries on top of summaries on',\n",
       "  'start': 270.78,\n",
       "  'duration': 3.479},\n",
       " {'text': 'top of summaries in some cases', 'start': 272.52, 'duration': 4.26},\n",
       " {'text': \"yeah that's method number two let's go\",\n",
       "  'start': 274.259,\n",
       "  'duration': 4.38},\n",
       " {'text': 'ahead and look at method number three',\n",
       "  'start': 276.78,\n",
       "  'duration': 4.38},\n",
       " {'text': \"and in this case there's the refine\",\n",
       "  'start': 278.639,\n",
       "  'duration': 4.741},\n",
       " {'text': \"method now with this one what we're\",\n",
       "  'start': 281.16,\n",
       "  'duration': 3.36},\n",
       " {'text': \"going to do is we're still going to\",\n",
       "  'start': 283.38,\n",
       "  'duration': 3.18},\n",
       " {'text': 'split up our document but in this case',\n",
       "  'start': 284.52,\n",
       "  'duration': 3.72},\n",
       " {'text': \"we're just going to pass it chunk number\",\n",
       "  'start': 286.56,\n",
       "  'duration': 3.66},\n",
       " {'text': \"one and we're going to say hey please\",\n",
       "  'start': 288.24,\n",
       "  'duration': 5.1},\n",
       " {'text': 'generate me a summary okay cool well',\n",
       "  'start': 290.22,\n",
       "  'duration': 5.039},\n",
       " {'text': \"with chunk number two what we're gonna\",\n",
       "  'start': 293.34,\n",
       "  'duration': 3.48},\n",
       " {'text': \"give it is we're gonna give it that\",\n",
       "  'start': 295.259,\n",
       "  'duration': 2.94},\n",
       " {'text': \"summary number one that we've already\",\n",
       "  'start': 296.82,\n",
       "  'duration': 4.2},\n",
       " {'text': \"gotten and then we're gonna say given\",\n",
       "  'start': 298.199,\n",
       "  'duration': 4.921},\n",
       " {'text': 'this summary number one given this',\n",
       "  'start': 301.02,\n",
       "  'duration': 3.959},\n",
       " {'text': 'context from this chunk number two',\n",
       "  'start': 303.12,\n",
       "  'duration': 4.2},\n",
       " {'text': 'please combine the two and give us a new',\n",
       "  'start': 304.979,\n",
       "  'duration': 4.321},\n",
       " {'text': 'refined summary', 'start': 307.32, 'duration': 3.42},\n",
       " {'text': 'and then this is going to keep on going',\n",
       "  'start': 309.3,\n",
       "  'duration': 3.48},\n",
       " {'text': 'on and on and on until you get to the',\n",
       "  'start': 310.74,\n",
       "  'duration': 4.08},\n",
       " {'text': 'end of your chunks that you have there',\n",
       "  'start': 312.78,\n",
       "  'duration': 4.199},\n",
       " {'text': 'and then that final piece that you have',\n",
       "  'start': 314.82,\n",
       "  'duration': 4.86},\n",
       " {'text': 'would be the fully refined summary if',\n",
       "  'start': 316.979,\n",
       "  'duration': 4.081},\n",
       " {'text': \"you will and then that'll be your final\",\n",
       "  'start': 319.68,\n",
       "  'duration': 4.32},\n",
       " {'text': 'output now the pros about this one',\n",
       "  'start': 321.06,\n",
       "  'duration': 4.859},\n",
       " {'text': 'is you get pretty relevant context',\n",
       "  'start': 324.0,\n",
       "  'duration': 3.18},\n",
       " {'text': 'because you can kind of carry the',\n",
       "  'start': 325.919,\n",
       "  'duration': 3.961},\n",
       " {'text': 'important parts across your chain there',\n",
       "  'start': 327.18,\n",
       "  'duration': 6.0},\n",
       " {'text': \"the cons is that they're all independent\",\n",
       "  'start': 329.88,\n",
       "  'duration': 6.42},\n",
       " {'text': \"calls right and so it's a synchronous uh\",\n",
       "  'start': 333.18,\n",
       "  'duration': 4.68},\n",
       " {'text': 'process here where you need to wait for',\n",
       "  'start': 336.3,\n",
       "  'duration': 3.119},\n",
       " {'text': 'one wait for the other wait for the',\n",
       "  'start': 337.86,\n",
       "  'duration': 3.36},\n",
       " {'text': 'other and so it could take a long time',\n",
       "  'start': 339.419,\n",
       "  'duration': 3.301},\n",
       " {'text': 'okay', 'start': 341.22, 'duration': 4.56},\n",
       " {'text': \"now method number four that we're gonna\",\n",
       "  'start': 342.72,\n",
       "  'duration': 5.28},\n",
       " {'text': \"do is one that's gonna be called map\",\n",
       "  'start': 345.78,\n",
       "  'duration': 5.16},\n",
       " {'text': 're-rank and this one is more for uh',\n",
       "  'start': 348.0,\n",
       "  'duration': 4.86},\n",
       " {'text': 'specific questions rather than uh',\n",
       "  'start': 350.94,\n",
       "  'duration': 4.68},\n",
       " {'text': 'summaries and in fact the library that',\n",
       "  'start': 352.86,\n",
       "  'duration': 4.679},\n",
       " {'text': \"we're going to be using today uh doesn't\",\n",
       "  'start': 355.62,\n",
       "  'duration': 3.48},\n",
       " {'text': 'even support this for some reason they',\n",
       "  'start': 357.539,\n",
       "  'duration': 3.181},\n",
       " {'text': 'only do questions and the way that this',\n",
       "  'start': 359.1,\n",
       "  'duration': 2.819},\n",
       " {'text': \"is going to work here is we're still\",\n",
       "  'start': 360.72,\n",
       "  'duration': 3.0},\n",
       " {'text': 'going to split our documents but this',\n",
       "  'start': 361.919,\n",
       "  'duration': 3.78},\n",
       " {'text': \"time we're going to pose a question to\",\n",
       "  'start': 363.72,\n",
       "  'duration': 5.52},\n",
       " {'text': 'our different chunks and what the um the',\n",
       "  'start': 365.699,\n",
       "  'duration': 5.041},\n",
       " {'text': \"method is going to do here is it's going\",\n",
       "  'start': 369.24,\n",
       "  'duration': 4.019},\n",
       " {'text': 'to say hey how confident are you that',\n",
       "  'start': 370.74,\n",
       "  'duration': 4.98},\n",
       " {'text': \"this answer that you've given from the\",\n",
       "  'start': 373.259,\n",
       "  'duration': 4.621},\n",
       " {'text': 'chunk is the final answer that we',\n",
       "  'start': 375.72,\n",
       "  'duration': 4.319},\n",
       " {'text': 'actually need so in this case we asked',\n",
       "  'start': 377.88,\n",
       "  'duration': 3.48},\n",
       " {'text': 'that on our first Chunk we asked that a',\n",
       "  'start': 380.039,\n",
       "  'duration': 2.94},\n",
       " {'text': 'question and it has an 80 confidence',\n",
       "  'start': 381.36,\n",
       "  'duration': 3.899},\n",
       " {'text': 'that this is the right answer then okay',\n",
       "  'start': 382.979,\n",
       "  'duration': 4.201},\n",
       " {'text': 'we do it for chunk number two and',\n",
       "  'start': 385.259,\n",
       "  'duration': 5.101},\n",
       " {'text': \"there's only a 25 chance that this is\",\n",
       "  'start': 387.18,\n",
       "  'duration': 5.76},\n",
       " {'text': 'the right answer and this is all just',\n",
       "  'start': 390.36,\n",
       "  'duration': 4.02},\n",
       " {'text': 'the language model interpreting the',\n",
       "  'start': 392.94,\n",
       "  'duration': 2.879},\n",
       " {'text': \"right answer so this isn't a scientific\",\n",
       "  'start': 394.38,\n",
       "  'duration': 3.72},\n",
       " {'text': 'process here and then what you do at the',\n",
       "  'start': 395.819,\n",
       "  'duration': 3.901},\n",
       " {'text': \"very end of that is you're going to rank\",\n",
       "  'start': 398.1,\n",
       "  'duration': 3.84},\n",
       " {'text': \"whereas the re-rank part comes in you're\",\n",
       "  'start': 399.72,\n",
       "  'duration': 3.84},\n",
       " {'text': 'going to rank the top scores that you',\n",
       "  'start': 401.94,\n",
       "  'duration': 2.879},\n",
       " {'text': \"have there and you're going to return\",\n",
       "  'start': 403.56,\n",
       "  'duration': 4.139},\n",
       " {'text': 'the answer that had the highest score so',\n",
       "  'start': 404.819,\n",
       "  'duration': 4.201},\n",
       " {'text': \"it'd be difficult to do this as a\",\n",
       "  'start': 407.699,\n",
       "  'duration': 2.821},\n",
       " {'text': \"summary which is why we don't do it you\",\n",
       "  'start': 409.02,\n",
       "  'duration': 3.179},\n",
       " {'text': 'only do it for a question and answer',\n",
       "  'start': 410.52,\n",
       "  'duration': 2.82},\n",
       " {'text': 'there', 'start': 412.199, 'duration': 4.801},\n",
       " {'text': 'so the pros for this one is is it scales',\n",
       "  'start': 413.34,\n",
       "  'duration': 6.12},\n",
       " {'text': \"well and it's but it's better for single\",\n",
       "  'start': 417.0,\n",
       "  'duration': 5.22},\n",
       " {'text': 'answer questions so not very complex',\n",
       "  'start': 419.46,\n",
       "  'duration': 5.28},\n",
       " {'text': \"questions and then the cons is you're\",\n",
       "  'start': 422.22,\n",
       "  'duration': 4.259},\n",
       " {'text': 'not combining any information in between',\n",
       "  'start': 424.74,\n",
       "  'duration': 3.239},\n",
       " {'text': 'documents because when you compare',\n",
       "  'start': 426.479,\n",
       "  'duration': 3.301},\n",
       " {'text': \"prompt one and prompt two there's no\",\n",
       "  'start': 427.979,\n",
       "  'duration': 4.021},\n",
       " {'text': 'sharing of that information across there',\n",
       "  'start': 429.78,\n",
       "  'duration': 5.699},\n",
       " {'text': 'all right now that is the four methods',\n",
       "  'start': 432.0,\n",
       "  'duration': 4.62},\n",
       " {'text': \"that we're going to look at in diagram\",\n",
       "  'start': 435.479,\n",
       "  'duration': 3.181},\n",
       " {'text': \"form let's go ahead and check these out\",\n",
       "  'start': 436.62,\n",
       "  'duration': 3.96},\n",
       " {'text': 'in code form', 'start': 438.66, 'duration': 3.42},\n",
       " {'text': \"all right let's look at some code here\",\n",
       "  'start': 440.58,\n",
       "  'duration': 2.76},\n",
       " {'text': \"so we're going to be using the lane\",\n",
       "  'start': 442.08,\n",
       "  'duration': 3.72},\n",
       " {'text': \"chain library now I don't think I need\",\n",
       "  'start': 443.34,\n",
       "  'duration': 4.079},\n",
       " {'text': 'to tell you but link chain is extremely',\n",
       "  'start': 445.8,\n",
       "  'duration': 3.54},\n",
       " {'text': 'good at file loading document management',\n",
       "  'start': 447.419,\n",
       "  'duration': 3.78},\n",
       " {'text': 'prompt management chaining all these',\n",
       "  'start': 449.34,\n",
       "  'duration': 3.6},\n",
       " {'text': \"things together and it's really the\",\n",
       "  'start': 451.199,\n",
       "  'duration': 4.081},\n",
       " {'text': \"magic behind how we're doing everything\",\n",
       "  'start': 452.94,\n",
       "  'duration': 4.02},\n",
       " {'text': \"we're doing here so if you haven't\",\n",
       "  'start': 455.28,\n",
       "  'duration': 3.06},\n",
       " {'text': 'checked it out please go check it out',\n",
       "  'start': 456.96,\n",
       "  'duration': 3.48},\n",
       " {'text': \"I'm going to load up some libraries for\",\n",
       "  'start': 458.34,\n",
       "  'duration': 4.02},\n",
       " {'text': 'us this includes the file loader the',\n",
       "  'start': 460.44,\n",
       "  'duration': 3.3},\n",
       " {'text': 'summarize chain which is going to do the',\n",
       "  'start': 462.36,\n",
       "  'duration': 3.36},\n",
       " {'text': 'summarizing for us and then a QA chain',\n",
       "  'start': 463.74,\n",
       "  'duration': 3.42},\n",
       " {'text': 'which is going to do question answer for',\n",
       "  'start': 465.72,\n",
       "  'duration': 3.66},\n",
       " {'text': \"us let's load up some documents we have\",\n",
       "  'start': 467.16,\n",
       "  'duration': 4.14},\n",
       " {'text': 'a John mere essay about Lake Tahoe and',\n",
       "  'start': 469.38,\n",
       "  'duration': 5.659},\n",
       " {'text': 'we have a Paul Graham essay about work',\n",
       "  'start': 471.3,\n",
       "  'duration': 3.739},\n",
       " {'text': 'exciting', 'start': 475.86, 'duration': 3.959},\n",
       " {'text': 'um no I like programming so we do a',\n",
       "  'start': 477.539,\n",
       "  'duration': 3.72},\n",
       " {'text': 'summary on our docs here I just made a',\n",
       "  'start': 479.819,\n",
       "  'duration': 3.181},\n",
       " {'text': 'quick function we have one document',\n",
       "  'start': 481.259,\n",
       "  'duration': 4.921},\n",
       " {'text': 'about 2200 2300 words and we have a',\n",
       "  'start': 483.0,\n",
       "  'duration': 6.24},\n",
       " {'text': 'preview the glory of the Sierra', 'start': 486.18, 'duration': 6.0},\n",
       " {'text': \"how beautiful how poetic and then let's\",\n",
       "  'start': 489.24,\n",
       "  'duration': 4.739},\n",
       " {'text': \"look at Paul's essay we have one\",\n",
       "  'start': 492.18,\n",
       "  'duration': 4.139},\n",
       " {'text': \"document it's about 22 and a half\",\n",
       "  'start': 493.979,\n",
       "  'duration': 4.62},\n",
       " {'text': 'thousand or Twenty twelve and a half',\n",
       "  'start': 496.319,\n",
       "  'duration': 4.201},\n",
       " {'text': \"thousand words so it's quite quite\",\n",
       "  'start': 498.599,\n",
       "  'duration': 5.88},\n",
       " {'text': 'bigger quite larger and a preview before',\n",
       "  'start': 500.52,\n",
       "  'duration': 5.459},\n",
       " {'text': 'College the two main things I worked on',\n",
       "  'start': 504.479,\n",
       "  'duration': 3.0},\n",
       " {'text': 'outside of school were writing and',\n",
       "  'start': 505.979,\n",
       "  'duration': 4.801},\n",
       " {'text': 'programming not quite as poetic as Mr',\n",
       "  'start': 507.479,\n",
       "  'duration': 5.041},\n",
       " {'text': \"mirror but uh we'll let it slide here\",\n",
       "  'start': 510.78,\n",
       "  'duration': 4.199},\n",
       " {'text': \"let's load up our llm in this case we're\",\n",
       "  'start': 512.52,\n",
       "  'duration': 5.699},\n",
       " {'text': 'doing open AI pass in our key okay cool',\n",
       "  'start': 514.979,\n",
       "  'duration': 4.68},\n",
       " {'text': \"and then we're going to load our\",\n",
       "  'start': 518.219,\n",
       "  'duration': 2.88},\n",
       " {'text': 'summarize chain and this is going to be',\n",
       "  'start': 519.659,\n",
       "  'duration': 2.88},\n",
       " {'text': 'with the stuff method so the first',\n",
       "  'start': 521.099,\n",
       "  'duration': 3.0},\n",
       " {'text': 'method that we talked about and in this',\n",
       "  'start': 522.539,\n",
       "  'duration': 2.941},\n",
       " {'text': \"case we're going to take our entire\",\n",
       "  'start': 524.099,\n",
       "  'duration': 3.061},\n",
       " {'text': \"document and we're going to stuff it\",\n",
       "  'start': 525.48,\n",
       "  'duration': 3.84},\n",
       " {'text': 'into the prompt I like how visceral that',\n",
       "  'start': 527.16,\n",
       "  'duration': 3.179},\n",
       " {'text': 'one sounds', 'start': 529.32, 'duration': 2.699},\n",
       " {'text': \"and so let's do it for our small dock\",\n",
       "  'start': 530.339,\n",
       "  'duration': 3.481},\n",
       " {'text': 'that we have here and I did verbose',\n",
       "  'start': 532.019,\n",
       "  'duration': 3.181},\n",
       " {'text': \"equals true because that's going to show\",\n",
       "  'start': 533.82,\n",
       "  'duration': 3.18},\n",
       " {'text': \"us what's underneath the covers and what\",\n",
       "  'start': 535.2,\n",
       "  'duration': 3.6},\n",
       " {'text': 'Lang chain is actually doing here so',\n",
       "  'start': 537.0,\n",
       "  'duration': 3.899},\n",
       " {'text': 'write a concise summary of the following',\n",
       "  'start': 538.8,\n",
       "  'duration': 4.68},\n",
       " {'text': 'which this is a lang chain prompt by the',\n",
       "  'start': 540.899,\n",
       "  'duration': 4.021},\n",
       " {'text': 'way and then the inserts our own text',\n",
       "  'start': 543.48,\n",
       "  'duration': 4.26},\n",
       " {'text': 'and then we give it a text okay cool',\n",
       "  'start': 544.92,\n",
       "  'duration': 5.099},\n",
       " {'text': 'and then Lange says give us a concise',\n",
       "  'start': 547.74,\n",
       "  'duration': 4.56},\n",
       " {'text': 'summary so in this article blah blah',\n",
       "  'start': 550.019,\n",
       "  'duration': 3.841},\n",
       " {'text': 'blah and so we have our summary about',\n",
       "  'start': 552.3,\n",
       "  'duration': 3.36},\n",
       " {'text': 'our small doc which is cool now if we',\n",
       "  'start': 553.86,\n",
       "  'duration': 3.96},\n",
       " {'text': 'did this with a large dock Well Lane',\n",
       "  'start': 555.66,\n",
       "  'duration': 3.42},\n",
       " {'text': 'chain is going to do the same exact',\n",
       "  'start': 557.82,\n",
       "  'duration': 2.459},\n",
       " {'text': \"thing and it's going to say write a\",\n",
       "  'start': 559.08,\n",
       "  'duration': 2.759},\n",
       " {'text': 'concise summary of the following and',\n",
       "  'start': 560.279,\n",
       "  'duration': 3.901},\n",
       " {'text': 'then we have the following but this is',\n",
       "  'start': 561.839,\n",
       "  'duration': 4.201},\n",
       " {'text': 'quite large and this is where the issue',\n",
       "  'start': 564.18,\n",
       "  'duration': 3.659},\n",
       " {'text': 'is going to be because down at the',\n",
       "  'start': 566.04,\n",
       "  'duration': 4.02},\n",
       " {'text': \"bottom oh no this model's maximum\",\n",
       "  'start': 567.839,\n",
       "  'duration': 5.881},\n",
       " {'text': 'context length is about 40 97 tokens now',\n",
       "  'start': 570.06,\n",
       "  'duration': 5.339},\n",
       " {'text': \"that's where issue is so how do we get\",\n",
       "  'start': 573.72,\n",
       "  'duration': 4.02},\n",
       " {'text': 'the summary of this larger dock well',\n",
       "  'start': 575.399,\n",
       "  'duration': 3.421},\n",
       " {'text': \"that's where the other methods come in\",\n",
       "  'start': 577.74,\n",
       "  'duration': 2.7},\n",
       " {'text': \"so let's talk about those first one\",\n",
       "  'start': 578.82,\n",
       "  'duration': 3.48},\n",
       " {'text': \"we're going to look at is mapreduce so\",\n",
       "  'start': 580.44,\n",
       "  'duration': 3.839},\n",
       " {'text': \"again I'm going to say chain type equals\",\n",
       "  'start': 582.3,\n",
       "  'duration': 3.719},\n",
       " {'text': \"equals mapreduce and then we're going to\",\n",
       "  'start': 584.279,\n",
       "  'duration': 4.321},\n",
       " {'text': 'say for both equals true okay now if',\n",
       "  'start': 586.019,\n",
       "  'duration': 3.601},\n",
       " {'text': \"we're going to run this on the small\",\n",
       "  'start': 588.6,\n",
       "  'duration': 2.22},\n",
       " {'text': 'dock', 'start': 589.62, 'duration': 3.96},\n",
       " {'text': \"uh I mean no surprise here it's more or\",\n",
       "  'start': 590.82,\n",
       "  'duration': 3.9},\n",
       " {'text': 'less the same exact thing that we have',\n",
       "  'start': 593.58,\n",
       "  'duration': 2.34},\n",
       " {'text': 'with the stuffing because stuffing',\n",
       "  'start': 594.72,\n",
       "  'duration': 3.119},\n",
       " {'text': 'worked and mapreduce just has one',\n",
       "  'start': 595.92,\n",
       "  'duration': 3.599},\n",
       " {'text': \"document to work with so let's not even\",\n",
       "  'start': 597.839,\n",
       "  'duration': 3.901},\n",
       " {'text': 'worry about that but to prove it to you',\n",
       "  'start': 599.519,\n",
       "  'duration': 3.661},\n",
       " {'text': 'uh', 'start': 601.74, 'duration': 2.94},\n",
       " {'text': 'you can see down here at the bottom we',\n",
       "  'start': 603.18,\n",
       "  'duration': 3.62},\n",
       " {'text': 'get more or less the same summary',\n",
       "  'start': 604.68,\n",
       "  'duration': 4.98},\n",
       " {'text': 'now the problem with our large dock is',\n",
       "  'start': 606.8,\n",
       "  'duration': 5.26},\n",
       " {'text': \"that it's just one document or it's one\",\n",
       "  'start': 609.66,\n",
       "  'duration': 4.859},\n",
       " {'text': 'big huge chunk and we need to split that',\n",
       "  'start': 612.06,\n",
       "  'duration': 4.08},\n",
       " {'text': \"into smaller chunks and the way I'm\",\n",
       "  'start': 614.519,\n",
       "  'duration': 3.301},\n",
       " {'text': 'going to do that is with Lang chains of',\n",
       "  'start': 616.14,\n",
       "  'duration': 4.1},\n",
       " {'text': 'recursive text splitter', 'start': 617.82, 'duration': 5.1},\n",
       " {'text': \"and okay cool we're gonna set this up\",\n",
       "  'start': 620.24,\n",
       "  'duration': 4.12},\n",
       " {'text': \"and I'm going to say chunk size equals\",\n",
       "  'start': 622.92,\n",
       "  'duration': 3.3},\n",
       " {'text': '400 I would normally make this much',\n",
       "  'start': 624.36,\n",
       "  'duration': 4.32},\n",
       " {'text': 'bigger but uh just to show you how it',\n",
       "  'start': 626.22,\n",
       "  'duration': 4.26},\n",
       " {'text': \"works I'm going to make it smaller chunk\",\n",
       "  'start': 628.68,\n",
       "  'duration': 3.18},\n",
       " {'text': \"overlap I'm going to put it zero meaning\",\n",
       "  'start': 630.48,\n",
       "  'duration': 2.78},\n",
       " {'text': \"I don't need any Venn diagram\", 'start': 631.86, 'duration': 4.14},\n",
       " {'text': \"similarities going on there and I'm\",\n",
       "  'start': 633.26,\n",
       "  'duration': 5.62},\n",
       " {'text': 'going to say hey split my documents the',\n",
       "  'start': 636.0,\n",
       "  'duration': 4.08},\n",
       " {'text': \"large stock and I'm going to put this\",\n",
       "  'start': 638.88,\n",
       "  'duration': 3.72},\n",
       " {'text': \"into a large Docs I know it's not a\",\n",
       "  'start': 640.08,\n",
       "  'duration': 4.62},\n",
       " {'text': \"wonderful way naming but that's what\",\n",
       "  'start': 642.6,\n",
       "  'duration': 4.14},\n",
       " {'text': \"we're going to do let me do a summary of\",\n",
       "  'start': 644.7,\n",
       "  'duration': 5.22},\n",
       " {'text': 'that so now I have 201 documents',\n",
       "  'start': 646.74,\n",
       "  'duration': 5.279},\n",
       " {'text': 'um with the same with roughly the same',\n",
       "  'start': 649.92,\n",
       "  'duration': 4.5},\n",
       " {'text': 'amount of words that which is too many',\n",
       "  'start': 652.019,\n",
       "  'duration': 4.5},\n",
       " {'text': 'from beforehand and we still have our',\n",
       "  'start': 654.42,\n",
       "  'duration': 4.2},\n",
       " {'text': 'preview okay cool but the important part',\n",
       "  'start': 656.519,\n",
       "  'duration': 3.721},\n",
       " {'text': 'is that we now have instead of one big',\n",
       "  'start': 658.62,\n",
       "  'duration': 4.98},\n",
       " {'text': 'dock we have 200 smaller docs right and',\n",
       "  'start': 660.24,\n",
       "  'duration': 7.68},\n",
       " {'text': 'if I were to run uh the mapreduce chain',\n",
       "  'start': 663.6,\n",
       "  'duration': 5.76},\n",
       " {'text': 'that we just made', 'start': 667.92, 'duration': 3.18},\n",
       " {'text': \"but I'm only gonna do it on the first\",\n",
       "  'start': 669.36,\n",
       "  'duration': 3.719},\n",
       " {'text': 'five documents because 200 is way too',\n",
       "  'start': 671.1,\n",
       "  'duration': 3.179},\n",
       " {'text': \"many and I don't want to spend all that\",\n",
       "  'start': 673.079,\n",
       "  'duration': 3.601},\n",
       " {'text': 'cash to query the API for that now',\n",
       "  'start': 674.279,\n",
       "  'duration': 3.781},\n",
       " {'text': \"here's where the cool part starts to\",\n",
       "  'start': 676.68,\n",
       "  'duration': 3.659},\n",
       " {'text': 'happen so what Lang chain is doing is it',\n",
       "  'start': 678.06,\n",
       "  'duration': 4.2},\n",
       " {'text': 'saying write a concise summary of the',\n",
       "  'start': 680.339,\n",
       "  'duration': 4.5},\n",
       " {'text': 'following and then it gives it a shorter',\n",
       "  'start': 682.26,\n",
       "  'duration': 4.259},\n",
       " {'text': \"chunk so it's not passing the entire\",\n",
       "  'start': 684.839,\n",
       "  'duration': 3.481},\n",
       " {'text': \"thing down there it's just this one\",\n",
       "  'start': 686.519,\n",
       "  'duration': 3.721},\n",
       " {'text': \"chunk Okay cool so there's section\",\n",
       "  'start': 688.32,\n",
       "  'duration': 3.0},\n",
       " {'text': 'number one', 'start': 690.24, 'duration': 3.42},\n",
       " {'text': \"here's section number two section number\",\n",
       "  'start': 691.32,\n",
       "  'duration': 4.92},\n",
       " {'text': 'three section number four and section',\n",
       "  'start': 693.66,\n",
       "  'duration': 4.14},\n",
       " {'text': 'number five because I said just give it',\n",
       "  'start': 696.24,\n",
       "  'duration': 3.42},\n",
       " {'text': 'the first five sections and then what',\n",
       "  'start': 697.8,\n",
       "  'duration': 3.599},\n",
       " {'text': \"it's doing is it's taking all those\",\n",
       "  'start': 699.66,\n",
       "  'duration': 3.72},\n",
       " {'text': \"summaries so here's summary One summary\",\n",
       "  'start': 701.399,\n",
       "  'duration': 3.841},\n",
       " {'text': 'two summary three summary four summary',\n",
       "  'start': 703.38,\n",
       "  'duration': 4.44},\n",
       " {'text': \"five and it's saying write a concise\",\n",
       "  'start': 705.24,\n",
       "  'duration': 4.32},\n",
       " {'text': 'summary of the following so give me a',\n",
       "  'start': 707.82,\n",
       "  'duration': 3.84},\n",
       " {'text': 'summary of the summaries and so we',\n",
       "  'start': 709.56,\n",
       "  'duration': 4.62},\n",
       " {'text': 'finally get a summary of our entire',\n",
       "  'start': 711.66,\n",
       "  'duration': 3.96},\n",
       " {'text': 'document that was way too big for the',\n",
       "  'start': 714.18,\n",
       "  'duration': 2.399},\n",
       " {'text': 'prompt', 'start': 715.62, 'duration': 2.88},\n",
       " {'text': 'um via the mapreduce method which is',\n",
       "  'start': 716.579,\n",
       "  'duration': 2.641},\n",
       " {'text': 'cool', 'start': 718.5, 'duration': 3.06},\n",
       " {'text': \"awesome so let's look at this one more\",\n",
       "  'start': 719.22,\n",
       "  'duration': 4.679},\n",
       " {'text': \"time but let's use the refine method so\",\n",
       "  'start': 721.56,\n",
       "  'duration': 4.26},\n",
       " {'text': \"in this case I'll do refined do verbose\",\n",
       "  'start': 723.899,\n",
       "  'duration': 3.601},\n",
       " {'text': \"equals true and again I'm just going to\",\n",
       "  'start': 725.82,\n",
       "  'duration': 3.959},\n",
       " {'text': 'do this on the first five documents',\n",
       "  'start': 727.5,\n",
       "  'duration': 4.019},\n",
       " {'text': 'and so this is where it gets kind of',\n",
       "  'start': 729.779,\n",
       "  'duration': 3.961},\n",
       " {'text': 'interesting the very first call that it',\n",
       "  'start': 731.519,\n",
       "  'duration': 4.021},\n",
       " {'text': 'makes remember this is uh not in',\n",
       "  'start': 733.74,\n",
       "  'duration': 4.02},\n",
       " {'text': 'parallel the first call that it makes is',\n",
       "  'start': 735.54,\n",
       "  'duration': 4.08},\n",
       " {'text': 'write a concise summary of the following',\n",
       "  'start': 737.76,\n",
       "  'duration': 3.9},\n",
       " {'text': 'and then we have all the different we',\n",
       "  'start': 739.62,\n",
       "  'duration': 3.959},\n",
       " {'text': 'have the first chunk here and then',\n",
       "  'start': 741.66,\n",
       "  'duration': 3.84},\n",
       " {'text': \"here's where it gets kind of interesting\",\n",
       "  'start': 743.579,\n",
       "  'duration': 4.141},\n",
       " {'text': 'this is link chain inserting this prompt',\n",
       "  'start': 745.5,\n",
       "  'duration': 2.88},\n",
       " {'text': 'here', 'start': 747.72, 'duration': 3.78},\n",
       " {'text': 'in talking to open AI your job is to',\n",
       "  'start': 748.38,\n",
       "  'duration': 5.639},\n",
       " {'text': 'produce a final summary we have provided',\n",
       "  'start': 751.5,\n",
       "  'duration': 4.98},\n",
       " {'text': 'the existing summary up until a certain',\n",
       "  'start': 754.019,\n",
       "  'duration': 4.621},\n",
       " {'text': 'point so here is the summary that it',\n",
       "  'start': 756.48,\n",
       "  'duration': 4.02},\n",
       " {'text': 'pulled from chunk number one', 'start': 758.64, 'duration': 3.0},\n",
       " {'text': 'and then we have the opportunity',\n",
       "  'start': 760.5,\n",
       "  'duration': 4.079},\n",
       " {'text': 'opportunity to refine using this extra',\n",
       "  'start': 761.64,\n",
       "  'duration': 4.68},\n",
       " {'text': 'context', 'start': 764.579, 'duration': 3.541},\n",
       " {'text': 'and then given the new new context',\n",
       "  'start': 766.32,\n",
       "  'duration': 4.56},\n",
       " {'text': 'refine the summary all right cool so',\n",
       "  'start': 768.12,\n",
       "  'duration': 4.38},\n",
       " {'text': \"that's chunk number two well chunk\",\n",
       "  'start': 770.88,\n",
       "  'duration': 3.959},\n",
       " {'text': 'number three oh interesting now we have',\n",
       "  'start': 772.5,\n",
       "  'duration': 3.779},\n",
       " {'text': 'a longer summary because it had two',\n",
       "  'start': 774.839,\n",
       "  'duration': 3.421},\n",
       " {'text': 'chunks to go off of and then it had part',\n",
       "  'start': 776.279,\n",
       "  'duration': 3.661},\n",
       " {'text': 'number three and it says give me a new',\n",
       "  'start': 778.26,\n",
       "  'duration': 3.48},\n",
       " {'text': 'summary give me a new summary give me a',\n",
       "  'start': 779.94,\n",
       "  'duration': 3.72},\n",
       " {'text': 'new summary blah blah blah and so now we',\n",
       "  'start': 781.74,\n",
       "  'duration': 4.14},\n",
       " {'text': 'have a longer summary between the two',\n",
       "  'start': 783.66,\n",
       "  'duration': 4.56},\n",
       " {'text': 'and you can see the last one and so we',\n",
       "  'start': 785.88,\n",
       "  'duration': 3.6},\n",
       " {'text': 'keep on refining and refining and',\n",
       "  'start': 788.22,\n",
       "  'duration': 2.88},\n",
       " {'text': \"refining this is why this one's a little\",\n",
       "  'start': 789.48,\n",
       "  'duration': 2.94},\n",
       " {'text': 'bit longer here', 'start': 791.1, 'duration': 2.94},\n",
       " {'text': \"so that's the refine method on how you\",\n",
       "  'start': 792.42,\n",
       "  'duration': 3.479},\n",
       " {'text': \"do it an alternative you're gonna have\",\n",
       "  'start': 794.04,\n",
       "  'duration': 3.359},\n",
       " {'text': 'to see if it works for use case I',\n",
       "  'start': 795.899,\n",
       "  'duration': 2.88},\n",
       " {'text': 'suggest you try them out and see how it',\n",
       "  'start': 797.399,\n",
       "  'duration': 3.06},\n",
       " {'text': \"goes and then the final one we're going\",\n",
       "  'start': 798.779,\n",
       "  'duration': 2.581},\n",
       " {'text': \"to do is we're going to switch over\",\n",
       "  'start': 800.459,\n",
       "  'duration': 2.281},\n",
       " {'text': \"instead of summarization we're going to\",\n",
       "  'start': 801.36,\n",
       "  'duration': 3.24},\n",
       " {'text': 'do a question and answer which is for',\n",
       "  'start': 802.74,\n",
       "  'duration': 4.26},\n",
       " {'text': \"map re-rank so again we'll say for both\",\n",
       "  'start': 804.6,\n",
       "  'duration': 4.32},\n",
       " {'text': 'equals true but in this case I want to',\n",
       "  'start': 807.0,\n",
       "  'duration': 3.54},\n",
       " {'text': 'return the intermediate steps which is',\n",
       "  'start': 808.92,\n",
       "  'duration': 3.539},\n",
       " {'text': 'just a fancy way of saying hey show me',\n",
       "  'start': 810.54,\n",
       "  'duration': 4.38},\n",
       " {'text': \"even more what's underneath the hood\",\n",
       "  'start': 812.459,\n",
       "  'duration': 4.021},\n",
       " {'text': 'so we got our chain there and we got our',\n",
       "  'start': 814.92,\n",
       "  'duration': 5.76},\n",
       " {'text': 'query so who oops uh who was the',\n",
       "  'start': 816.48,\n",
       "  'duration': 7.14},\n",
       " {'text': \"author's friend who got who he got\",\n",
       "  'start': 820.68,\n",
       "  'duration': 7.2},\n",
       " {'text': 'permission from to use the IBM 1401',\n",
       "  'start': 823.62,\n",
       "  'duration': 6.659},\n",
       " {'text': 'um I saw this referenced in the document',\n",
       "  'start': 827.88,\n",
       "  'duration': 3.959},\n",
       " {'text': \"so which is why I'm pulling it out so\",\n",
       "  'start': 830.279,\n",
       "  'duration': 3.3},\n",
       " {'text': \"I'm going to input my only the first\",\n",
       "  'start': 831.839,\n",
       "  'duration': 3.18},\n",
       " {'text': \"five docs again I'm going to give it my\",\n",
       "  'start': 833.579,\n",
       "  'duration': 2.82},\n",
       " {'text': \"question and I'm going to return the\",\n",
       "  'start': 835.019,\n",
       "  'duration': 4.141},\n",
       " {'text': \"outputs so let's go ahead and run this\",\n",
       "  'start': 836.399,\n",
       "  'duration': 5.461},\n",
       " {'text': \"so now it's going through and what it's\",\n",
       "  'start': 839.16,\n",
       "  'duration': 4.5},\n",
       " {'text': \"doing is it's kind of a complicated\",\n",
       "  'start': 841.86,\n",
       "  'duration': 4.08},\n",
       " {'text': \"prompt but it's cool to see\", 'start': 843.66, 'duration': 4.08},\n",
       " {'text': 'use the following piece of context to',\n",
       "  'start': 845.94,\n",
       "  'duration': 4.019},\n",
       " {'text': 'help answer the question at the end in',\n",
       "  'start': 847.74,\n",
       "  'duration': 4.32},\n",
       " {'text': 'addition to the answer also return a',\n",
       "  'start': 849.959,\n",
       "  'duration': 5.161},\n",
       " {'text': 'score of how fully it answered the',\n",
       "  'start': 852.06,\n",
       "  'duration': 5.16},\n",
       " {'text': \"user's question and then so not only\",\n",
       "  'start': 855.12,\n",
       "  'duration': 3.6},\n",
       " {'text': \"does it say hey here's the format we're\",\n",
       "  'start': 857.22,\n",
       "  'duration': 3.54},\n",
       " {'text': 'going to use how to determine the score',\n",
       "  'start': 858.72,\n",
       "  'duration': 4.559},\n",
       " {'text': 'but then it also gives it examples about',\n",
       "  'start': 860.76,\n",
       "  'duration': 4.319},\n",
       " {'text': 'how to score which is kind of', 'start': 863.279, 'duration': 3.24},\n",
       " {'text': 'interesting so it gives it a couple',\n",
       "  'start': 865.079,\n",
       "  'duration': 4.621},\n",
       " {'text': 'examples here and then begin', 'start': 866.519, 'duration': 5.101},\n",
       " {'text': 'all right so just by the way this is',\n",
       "  'start': 869.7,\n",
       "  'duration': 4.139},\n",
       " {'text': 'pretty good prompt engineering if this',\n",
       "  'start': 871.62,\n",
       "  'duration': 4.56},\n",
       " {'text': 'is a fine example of it we have context',\n",
       "  'start': 873.839,\n",
       "  'duration': 4.261},\n",
       " {'text': 'right here about the question and then',\n",
       "  'start': 876.18,\n",
       "  'duration': 3.54},\n",
       " {'text': \"here's the final question who is the\",\n",
       "  'start': 878.1,\n",
       "  'duration': 3.78},\n",
       " {'text': \"author's friend blah blah blah blah and\",\n",
       "  'start': 879.72,\n",
       "  'duration': 3.419},\n",
       " {'text': 'then it does the same thing for chunk',\n",
       "  'start': 881.88,\n",
       "  'duration': 3.6},\n",
       " {'text': 'number two chunk number three blah blah',\n",
       "  'start': 883.139,\n",
       "  'duration': 5.041},\n",
       " {'text': 'blah blah blah blah and then we go down',\n",
       "  'start': 885.48,\n",
       "  'duration': 4.74},\n",
       " {'text': 'and it finished the chain so it went',\n",
       "  'start': 888.18,\n",
       "  'duration': 3.36},\n",
       " {'text': 'through all those different five chunks',\n",
       "  'start': 890.22,\n",
       "  'duration': 3.299},\n",
       " {'text': 'ask the question ranked the answer for',\n",
       "  'start': 891.54,\n",
       "  'duration': 3.419},\n",
       " {'text': \"each one of those questions and let's\",\n",
       "  'start': 893.519,\n",
       "  'duration': 3.361},\n",
       " {'text': 'see what we got here what we got rich',\n",
       "  'start': 894.959,\n",
       "  'duration': 3.901},\n",
       " {'text': 'Draves which', 'start': 896.88, 'duration': 3.78},\n",
       " {'text': \"um I won't show but it is in the essay\",\n",
       "  'start': 898.86,\n",
       "  'duration': 4.2},\n",
       " {'text': 'yes this is the right answer in fact I',\n",
       "  'start': 900.66,\n",
       "  'duration': 5.34},\n",
       " {'text': 'bet you could even go in yeah so my',\n",
       "  'start': 903.06,\n",
       "  'duration': 4.68},\n",
       " {'text': 'friend Rich Draves is pulled out of one',\n",
       "  'start': 906.0,\n",
       "  'duration': 4.019},\n",
       " {'text': 'of the chunks which is cool', 'start': 907.74, 'duration': 3.839},\n",
       " {'text': \"um cool now let's take a look at the\",\n",
       "  'start': 910.019,\n",
       "  'duration': 4.021},\n",
       " {'text': 'intermediate steps so what it did was I',\n",
       "  'start': 911.579,\n",
       "  'duration': 3.781},\n",
       " {'text': 'went through the different five docs',\n",
       "  'start': 914.04,\n",
       "  'duration': 3.419},\n",
       " {'text': 'that we passed it and for the first DOC',\n",
       "  'start': 915.36,\n",
       "  'duration': 3.24},\n",
       " {'text': \"or for one of the docs I don't know\",\n",
       "  'start': 917.459,\n",
       "  'duration': 3.06},\n",
       " {'text': 'which number this was this document does',\n",
       "  'start': 918.6,\n",
       "  'duration': 4.26},\n",
       " {'text': 'not answer the question score of zero it',\n",
       "  'start': 920.519,\n",
       "  'duration': 3.721},\n",
       " {'text': 'does not answer does not answer does not',\n",
       "  'start': 922.86,\n",
       "  'duration': 3.18},\n",
       " {'text': 'answer but for this document it did and',\n",
       "  'start': 924.24,\n",
       "  'duration': 3.899},\n",
       " {'text': 'it gave it a score of 100 which is why',\n",
       "  'start': 926.04,\n",
       "  'duration': 5.94},\n",
       " {'text': 'it was uh it returned that answer',\n",
       "  'start': 928.139,\n",
       "  'duration': 7.081},\n",
       " {'text': 'super cool that is the ref that is the',\n",
       "  'start': 931.98,\n",
       "  'duration': 6.84},\n",
       " {'text': 'map re-rank method so in the end there',\n",
       "  'start': 935.22,\n",
       "  'duration': 5.4},\n",
       " {'text': 'are four different methods of prompt',\n",
       "  'start': 938.82,\n",
       "  'duration': 2.94},\n",
       " {'text': \"management it's not it's kind of like\",\n",
       "  'start': 940.62,\n",
       "  'duration': 2.88},\n",
       " {'text': 'query management if you will about how',\n",
       "  'start': 941.76,\n",
       "  'duration': 3.12},\n",
       " {'text': 'to chain your different commands',\n",
       "  'start': 943.5,\n",
       "  'duration': 3.42},\n",
       " {'text': 'together in order to fit your use case',\n",
       "  'start': 944.88,\n",
       "  'duration': 5.28},\n",
       " {'text': 'now have fun and uh let me know which',\n",
       "  'start': 946.92,\n",
       "  'duration': 6.74},\n",
       " {'text': \"ones work for you we'll see you later\",\n",
       "  'start': 950.16,\n",
       "  'duration': 3.5}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "video_id = \"f9_BWhCI4Zo\"\n",
    "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding_name = 'cl100k_base'\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4150"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = '\\n'\n",
    "caption = sep.join([s['text'] for s in srt])\n",
    "num_tokens(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save srt as a webvtt file\n",
    "import webvtt\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a new WebVTT file\n",
    "vtt = webvtt.WebVTT()\n",
    "\n",
    "# Convert each subtitle in the srt to a WebVTT caption\n",
    "for subtitle in srt:\n",
    "    start = datetime.fromtimestamp(subtitle['start']).strftime('%H:%M:%S.%f')[:-3]\n",
    "    end = datetime.fromtimestamp(subtitle['start'] + subtitle['duration']).strftime('%H:%M:%S.%f')[:-3]\n",
    "    text = subtitle['text']\n",
    "    caption = webvtt.Caption(start, end, text)\n",
    "    vtt.captions.append(caption)\n",
    "\n",
    "# Save the WebVTT file\n",
    "vtt.save('../data/vtt/subtitles.vtt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api.formatters import WebVTTFormatter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "video_id = \"2xxziIWmaSA\" # https://www.youtube.com/watch?v=2xxziIWmaSA\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "formatter = WebVTTFormatter()\n",
    "formatted_captions = formatter.format_transcript(transcript)\n",
    "vtt_file = f'../data/vtt/subtitles-{video_id}.vtt'\n",
    "with open(vtt_file, 'w') as f:\n",
    "    f.write(formatted_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10336"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = '\\n'\n",
    "caption = sep.join([s['text'] for s in transcript])\n",
    "num_tokens(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn VTT file into TXT file\n",
    "txt_file = vtt_file.replace('vtt','txt')\n",
    "with open(txt_file,mode='w') as f:\n",
    "    f.write(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(txt_file)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_summary(docs):\n",
    "    print (f'You have {len(docs)} document(s)')\n",
    "    num_words = sum([len(doc.page_content.split(' ')) for doc in docs])\n",
    "    print (f'You have roughly {num_words} words in your docs')\n",
    "    print ()\n",
    "    print (f'Preview: \\n{docs[0].page_content.split(\". \")[0][0:42]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s)\n",
      "You have roughly 7443 words in your docs\n",
      "\n",
      "Preview: \n",
      "hello good people have you ever wondered\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "doc_summary(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 11 document(s)\n",
      "You have roughly 7453 words in your docs\n",
      "\n",
      "Preview: \n",
      "hello good people have you ever wondered\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = num_tokens,\n",
    ")\n",
    "docs = text_splitter.split_documents(doc)\n",
    "doc_summary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hello good people have you ever wondered\n",
      "what Lang chain was or maybe you've\n",
      "heard about it and you've played around\n",
      "with a few sections but you're not quite\n",
      "sure where to look next well in this\n",
      "video we're going to be covering all of\n",
      "the lane chain Basics with the goal of\n",
      "getting you building and having fun as\n",
      "quick as possible my name is Greg and\n",
      "I've been having a ton of fun building\n",
      "out apps in langchain now I share most\n",
      "of my work on Twitter so if you want to\n",
      "go check it out links in the description\n",
      "you can go follow along with me now this\n",
      "video is going to be based off of the\n",
      "new conceptual docs from lanechain and\n",
      "the reason why I'm doing a video here is\n",
      "because it takes all the technical\n",
      "pieces and abstracts them up into more\n",
      "theoretical qualitative aspects of Lane\n",
      "chain which I think is extremely helpful\n",
      "for it and in order to understand this a\n",
      "little bit better I've created a\n",
      "companion for this video and that is the\n",
      "Lang chain cookbook links in the\n",
      "description if you want to go check that\n",
      "out please go and check out the GitHub\n",
      "and you can follow along here I'm gonna\n",
      "put a lot of time stamps in the\n",
      "description as well there's gonna be a\n",
      "fair amount of content in this one so\n",
      "you can watch it all the way through or\n",
      "if you want to skip to a certain section\n",
      "feel free to jump to that time stamp all\n",
      "right without further Ado let's jump\n",
      "into it all right here are the new\n",
      "conceptual docs from Lang chain now the\n",
      "reason why these are different is\n",
      "because there are the python docs which\n",
      "are going to be the more technical\n",
      "focused one or the JavaScript docs as\n",
      "well which is also more technical\n",
      "documentation however these concepts are\n",
      "more qualitative so you can understand\n",
      "what is going on in the background of\n",
      "these different sections here now we're\n",
      "going to focus on these components of\n",
      "Lang chain there's an entire section on\n",
      "use cases which is when you actually put\n",
      "these into practice and that is going to\n",
      "be a part two of this video so we won't\n",
      "jump into this today that would be a too\n",
      "long for us we're going to run through\n",
      "schema models prompts indexes indexes\n",
      "memory chains and agents with a working\n",
      "code sample for each one of those well\n",
      "without further Ado let's jump into some\n",
      "code here we are with the Lang chain\n",
      "cookbook now my goal is to make this a\n",
      "dense document with a ton of links so\n",
      "you can go and self-service right into\n",
      "their links in the description and if\n",
      "you want to follow along I encourage you\n",
      "to get this on your computer and go from\n",
      "it go for it from there so the goal of\n",
      "this dock is to provide an introductory\n",
      "understanding of the components and use\n",
      "cases of Lang chain in a explain like M5\n",
      "way with examples and code Snippets for\n",
      "use cases check out part two which is\n",
      "not made yet that is coming soon\n",
      "hopefully by the time you see this it\n",
      "will be a bunch of links here we go into\n",
      "what is Lang chain so Lang chain is\n",
      "going to be a framework for developing\n",
      "applications powered by language models\n",
      "well Greg openai just came out with\n",
      "plugins yes but there is a whole lot of\n",
      "other things that you can do with\n",
      "language models outside of those and\n",
      "Lang chain helps abstract a ton of that\n",
      "so that you're able to work with it more\n",
      "easily and intermix different pieces and\n",
      "customize really how you need to\n",
      "so Lane chain makes the complicated\n",
      "parts of working and building with AI\n",
      "models easier it does this in two main\n",
      "ways the first big way is going to be\n",
      "through integration so you can bring\n",
      "external data such as your files other\n",
      "applications API data to your language\n",
      "models which is cool the other big way\n",
      "that it helps do this is through agency\n",
      "so it allows your language models to\n",
      "interact with its environment via\n",
      "decision making basically you're using\n",
      "the language model to help decide which\n",
      "action to take next and you do this when\n",
      "the path isn't so clear or it may be\n",
      "unknown and we'll get into more more of\n",
      "that later\n",
      "so why link chain specifically there are\n",
      "four big reasons why I like Lane chain\n",
      "the first one is going to be for the\n",
      "components Lang chain makes it easy to\n",
      "swap out abstractions and components\n",
      "necessary to work with language models\n",
      "basically they've created a ton of tools\n",
      "that make it super simple to work with\n",
      "language models like chat GPT or\n",
      "anything on how you face how you may\n",
      "want also because it allows you to\n",
      "customize chains really easily so\n",
      "there's a ton of out of the box of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"support for using and customizing chains\n",
      "basically combining series of actions\n",
      "together\n",
      "on the qualitative side of why LinkedIn\n",
      "is awesome is because the speed is great\n",
      "almost every day I need to go and make\n",
      "sure that I'm on the latest branch of\n",
      "Lang Chan and I go and I update it every\n",
      "time so the speed is awesome the other\n",
      "really cool part is the community so\n",
      "there's a ton of meetups there's a\n",
      "Discord Channel and there's a ton of\n",
      "events like webinars that go on\n",
      "throughout the week that are really\n",
      "awesome learning resources for us\n",
      "cool now again to summarize all this why\n",
      "do we need Lang chain well because\n",
      "language models can be pretty\n",
      "straightforward it is text in text app\n",
      "and you may have experienced this\n",
      "yourself however once you just start\n",
      "developing applications there's a ton of\n",
      "friction points that Lang chain is going\n",
      "to help you develop uh there's a ton of\n",
      "friction points that they're going to\n",
      "help you with basically now the last\n",
      "thing that I'll say about this before we\n",
      "jump into it is that this cookbook isn't\n",
      "going to cover all of the aspects of\n",
      "Lang chain this isn't meant to be a\n",
      "replacement for the documentation online\n",
      "this is meant to show you a very broad\n",
      "overview about the capabilities that\n",
      "there are with my interpretation of them\n",
      "and my voice over with it and with that\n",
      "I'm hoping that you can get to building\n",
      "and impact as quick as possible\n",
      "I'm super curious to see what you build\n",
      "so please let me know and uh I will\n",
      "hopefully uh I would love to see it\n",
      "first thing we're going to do is we're\n",
      "going to import our openai API key now I\n",
      "have a hidden cell here but you're going\n",
      "to replace your API key API key right\n",
      "here just throw that in there the first\n",
      "aspect of link chain components that\n",
      "we're going to look at is the schema now\n",
      "I almost didn't even include this one\n",
      "but the first one is going to be text\n",
      "now what's really cool about these\n",
      "language models is that text is the new\n",
      "programming language not verbatim not\n",
      "per se but we're using a lot more\n",
      "English language to tell language models\n",
      "what to do in this case what day comes\n",
      "after Friday is an example of something\n",
      "I may go tell a language model and it is\n",
      "going to respond back to me with a\n",
      "natural language response very cool next\n",
      "up is going to be chat messages so like\n",
      "text chat messages are similar but they\n",
      "have different types the first type is\n",
      "going to be system and this is helpful\n",
      "background context that tell the AI what\n",
      "to do all right like your helpful\n",
      "teacher assistant bot or something then\n",
      "we have human messages and these are\n",
      "messages that are intended to represent\n",
      "the user and so literally user input or\n",
      "something that I may text from it then\n",
      "we have ai messages and these are\n",
      "messages that show what the AI responded\n",
      "with and the cool part about this is the\n",
      "AI may or may not have actually\n",
      "responded with it but you can tell it\n",
      "that it did so that it has additional\n",
      "context on how to answer you okay so\n",
      "what I'm going to do here is I'm going\n",
      "to import chat open Ai and my three\n",
      "message types and then I'm going to\n",
      "create my chat model I'm going to do\n",
      "that and then I'm going to type in two\n",
      "messages the first system message is you\n",
      "are a nice AI bot that helps a user\n",
      "figure out what to eat in a short\n",
      "sentence and then a human message I like\n",
      "tomatoes what should I eat let me go\n",
      "ahead and run this\n",
      "and you get an AI message back because\n",
      "this is what it responds with you could\n",
      "try making a tomato salad with fresh\n",
      "basil and mozzarella cheese thanks AI\n",
      "That's cool what you can also do is you\n",
      "can also pass more chat history and get\n",
      "responses from the AI so in this case\n",
      "you're a nice AI bot that helps a user\n",
      "figure out where to travel to in one\n",
      "short sentence\n",
      "I'm saying I like the beaches where\n",
      "should I go\n",
      "I'm telling it that it responded to me\n",
      "it didn't actually do this but I'm\n",
      "telling it that it did you should go to\n",
      "Nice France cool what else should I do\n",
      "when\n",
      "when I'm there and so the reason why I\n",
      "did this one is because you'll notice\n",
      "that I didn't say where I went it's\n",
      "going to have to infer from the history\n",
      "on what where I went and it says wow and\n",
      "nice and so it picked up where I was\n",
      "because it gets the history of the chat\n",
      "messages now if you're making a chatbot\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"you could see how you could append\n",
      "different messages that have been back\n",
      "and forth uh I'm not sure if that's a\n",
      "verb but back and forth through the user\n",
      "okay\n",
      "the next model that we're the next model\n",
      "that we're going to look at is going to\n",
      "be documents so documents are important\n",
      "because this represents a piece of text\n",
      "along with Associated metadata now\n",
      "metadata is just a fancy word for things\n",
      "about that document and in this case\n",
      "this document or the text is held within\n",
      "a field called page content so this is\n",
      "my document it's full text that I've\n",
      "gathered from other places awesome and\n",
      "then I'm going to pass in some metadata\n",
      "and this metadata is a dictionary of key\n",
      "value pairs my document ID which is my\n",
      "key here and then some random document\n",
      "ID here that happens to be an INT it\n",
      "could be whatever you want it to be my\n",
      "document Source this is the Lang chain\n",
      "papers and then my document create time\n",
      "is going to be some timestamp whatever\n",
      "you want it to be and this is going to\n",
      "be can be whatever format you want this\n",
      "is extremely helpful for when you're\n",
      "making a large repositories of\n",
      "information and you want to be able to\n",
      "filter by it so instead of just going\n",
      "and asking link chain to look at all\n",
      "your documents in your database you can\n",
      "go ahead and filter these by a certain\n",
      "metadata go ahead and run this and you\n",
      "can see here I get a document object\n",
      "with a bunch of metadata on it from\n",
      "there cool if those are the schemas that\n",
      "we work with the next thing we're going\n",
      "to look at is the different models now\n",
      "these are the ways of interacting with\n",
      "well different models\n",
      "um but the reason why this is important\n",
      "is because they're different model types\n",
      "let me just show an example here the\n",
      "normal one that we're looking at is\n",
      "going to be the language model and this\n",
      "is when text goes in and text comes out\n",
      "okay now the first thing I'll do is I'll\n",
      "import open Ai and I'll make my model\n",
      "and you'll notice here that I changed my\n",
      "model in case you ever want to change\n",
      "your model as well and so I'm going to\n",
      "pass in a regular string into this one\n",
      "into my language model what day comes\n",
      "after Friday\n",
      "go ahead and run this and I get Saturday\n",
      "comes out the other end but not all\n",
      "models are like this you actually have\n",
      "chat models as well and we looked at\n",
      "this in the previous example but I\n",
      "didn't call it out specifically so for\n",
      "this one I'm going to import chat open\n",
      "AI I'm going to import my messages again\n",
      "I'm going to put temperature equals one\n",
      "which means the model is going to get a\n",
      "little spicy on me no but really it just\n",
      "means it's going to have more creativity\n",
      "and it's it's a little bit more\n",
      "exaggerated and so in this case I'm\n",
      "going to say you are an unhelpful AI bot\n",
      "that makes jokes at whatever the user\n",
      "says and in this case the user says I\n",
      "would like to go to New York how should\n",
      "I do this I'm going to go ahead and run\n",
      "this model\n",
      "you could try walking but I don't\n",
      "recommend it unless you have a lot of\n",
      "time on your hands\n",
      "maybe try flapping your arms really hard\n",
      "to see if it can fly there so as you can\n",
      "see it took that system message\n",
      "and it understood those directions and\n",
      "it uh it wasn't very helpful for me well\n",
      "because I told her not to be very\n",
      "helpful\n",
      "the last type of model that we're going\n",
      "to look at is going to be your text and\n",
      "betting model the reason why this one is\n",
      "important is because we do a lot of\n",
      "similarity searches and a lot of\n",
      "comparing texts when working with\n",
      "language models now in this case openai\n",
      "also has an AI embeddings model that\n",
      "we're going to use there's a lot of\n",
      "embedding models out there you can use\n",
      "whatever you want I just use open AI\n",
      "because it feels like it's a standard\n",
      "and it's very simple right now so I'm\n",
      "going to pass in my API key I'm going to\n",
      "get my embeddings engine ready and then\n",
      "I'm going to define a piece of text hi\n",
      "it's time for the beach let me go ahead\n",
      "and do that text\n",
      "and what I'm going to do is I'm going to\n",
      "pass that text and I'm going to embed\n",
      "that text so what that means is is it's\n",
      "going to take this string which is just\n",
      "a series of letters and it's going to\n",
      "convert it into a vector and in this\n",
      "case a vector is just simply a\n",
      "one-dimensional array meaning a list of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"numbers and that'll be a semantic\n",
      "representation of that text that's a\n",
      "fancy way of saying is that meaning of\n",
      "that text is going to be embedded in\n",
      "those numbers right there which makes it\n",
      "really easy to compare\n",
      "across other as others as well so I'm\n",
      "going to put that in a variable called\n",
      "text embeddings I'm going to see how\n",
      "long my text embeddings is and I'm going\n",
      "to get a preview of it so you'll notice\n",
      "here that my text embedding length is\n",
      "1536 this means that there are 1536\n",
      "different numbers within that list that\n",
      "represent the meaning of my text\n",
      "that's a lot of numbers and I'm glad I\n",
      "don't have to deal with them I'm glad\n",
      "the computer can so here's a sample of\n",
      "what those look like in case you're\n",
      "curious I only show the first uh five\n",
      "here but I put a dot dot dots you know\n",
      "that there are\n",
      "1531 other numbers out there next let's\n",
      "look at prompts so prompts are going to\n",
      "be the text that you send over to your\n",
      "language model we've already sent some\n",
      "prompts over to the language model but\n",
      "they've been pretty simple in this case\n",
      "we're going to start doing more\n",
      "instructional prompts and passing those\n",
      "to our model so again a prompt is what\n",
      "we pass to our language model I'm going\n",
      "to import open AI in this case I'm using\n",
      "DaVinci as my model and I'm going to say\n",
      "prompt equals this string now I use\n",
      "three three double quotes because\n",
      "um well I think it looks fancier no but\n",
      "really it's just easier to use which is\n",
      "why I like it in this case I'm not doing\n",
      "anything fancy and I could have passed\n",
      "this string right within my language\n",
      "model but in this case I\n",
      "made a variable for it because it's a\n",
      "little bit easier to understand so\n",
      "today's Monday tomorrow's Wednesday what\n",
      "is wrong with the statement the\n",
      "statement is incorrect tomorrow's\n",
      "Tuesday not Wednesday so you can see how\n",
      "it picked it up from there now why\n",
      "prompts are cool is because we start to\n",
      "get into the prompt template world\n",
      "the reason why prompt templates are\n",
      "important is because most of the time\n",
      "you're going to be dynamically\n",
      "generating your prompts meaning they\n",
      "won't just be static strings that you\n",
      "type out but you're actually going to be\n",
      "inputting tokens or inputting\n",
      "placeholders based off of the scenario\n",
      "that we're you're working with\n",
      "so in this case what I'm doing here is\n",
      "I'm importing my packages again in this\n",
      "case prompt template is going to be the\n",
      "new one I'm going to do DaVinci again\n",
      "okay great and in this case I'm going to\n",
      "create a template to start so I really\n",
      "want to travel to location you'll notice\n",
      "my opened and closed brackets around\n",
      "location which means that this is going\n",
      "to be a token that I'm going to be\n",
      "replacing later what should I do there\n",
      "respond in one short sentence because\n",
      "we're also just responded too much I'm\n",
      "going to create a prompt template in\n",
      "this case I'm going to put it in this\n",
      "variable prompt my input variable is\n",
      "going to be location which matches the\n",
      "same name that we had up here and then\n",
      "the template is this this whole thing\n",
      "that I had here the final prompt is\n",
      "going to be prompt.format which means\n",
      "going to insert the values I tell you go\n",
      "and insert the value Rome into where it\n",
      "says location right here let's go ahead\n",
      "and run this\n",
      "so final prompt I really want to travel\n",
      "to Rome which replace location up above\n",
      "and here we have our prompt template\n",
      "that's finally filled out and then in\n",
      "terms of the output it tells me what I\n",
      "should do so it took that information in\n",
      "with Rome and responded one short\n",
      "sentence it gives me this which is cool\n",
      "all right the next cool part that we're\n",
      "going to look at is the example\n",
      "selectors so often when you're\n",
      "constructing your prompts you're going\n",
      "to do something called in context\n",
      "learning this means that you're going to\n",
      "show you're going to show the language\n",
      "model what you want it to do and one of\n",
      "the main ways that people do this is\n",
      "through examples this could be about how\n",
      "to answer a customer service request or\n",
      "it could be how to respond to some\n",
      "nuanced question and in this case I'm\n",
      "going to pick examples however we have\n",
      "example selectors because say you had 10\n",
      "000 different examples you don't want to\n",
      "throw all those into your uh into your\n",
      "prompt they may not fit and they may not\n",
      "be as relevant so you want to select\n",
      "which ones you want and in this case\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"what I'm going to do is I'm going to\n",
      "import a lot of things here but the one\n",
      "I'm the main star of the show is going\n",
      "to be the semantic similarity example\n",
      "selector that's a long name for a\n",
      "functionality that's going to select\n",
      "similar examples so I'm going to get my\n",
      "language model going again I'm going to\n",
      "get my example prompt and this is just a\n",
      "prompt template like we saw up above and\n",
      "then I'm going to define a list of\n",
      "different examples so in this case I\n",
      "want to name a noun and then I want the\n",
      "language model to tell me where this\n",
      "noun is usually found so in this case a\n",
      "pirate on a ship a pilot on a plane\n",
      "driver in a car a tree\n",
      "oh that's not true a tree in the ground\n",
      "or a bird in a nest so I'll go ahead and\n",
      "run that one\n",
      "and then what we're going to do is we're\n",
      "going to get our example selector ready\n",
      "so we have our similar example selector\n",
      "we're going to pass it the list of\n",
      "examples that I just defined above but\n",
      "then we're also going to pass it our\n",
      "embedding engine and the reason why we\n",
      "do this is because we're actually going\n",
      "to match examples on their semantic\n",
      "meaning so not just matching them off of\n",
      "similar strings but off of what they\n",
      "actually mean so in this case we're\n",
      "going to use the open AI embeddings\n",
      "which is one of the models that has been\n",
      "shared by Facebook which is really cool\n",
      "and this is going to help store our\n",
      "embeddings and then we're going to tell\n",
      "it\n",
      "um how many we want how many examples we\n",
      "want back in this case I want k equals\n",
      "two let me go ahead and run that and\n",
      "then we're going to have a new prompt\n",
      "template here and this is going to be\n",
      "the few shot prompt template meaning the\n",
      "few shot part means that there's going\n",
      "to be a few examples in there for us so\n",
      "we give it our example selector we give\n",
      "our example prompt which we made up\n",
      "above and then we're going to add on\n",
      "just some little strings before and\n",
      "after to make it easier for the model so\n",
      "give the location that an item is\n",
      "usually found in cool and then the\n",
      "suffix will be the input and the output\n",
      "that we have from here based off of what\n",
      "the user inputs then the input variable\n",
      "go ahead and do that so here I'm going\n",
      "to say my noun is student So based off\n",
      "of the noun of student it's going to go\n",
      "and find me the examples up above that\n",
      "are most closely related to student and\n",
      "we're going to use those examples so if\n",
      "I would go ahead and do this I'm going\n",
      "to say print and it's going to print me\n",
      "the prompt that we're actually going to\n",
      "use within or give to our language model\n",
      "in this case it found the driver and it\n",
      "found the pilot one being most similar\n",
      "to student which is cool now if I were\n",
      "to do a different one say flower it's\n",
      "going to give me the the tree and the\n",
      "bird examples okay but I'm going to\n",
      "stick this with student and what I'm\n",
      "going to do is I'm going to take this\n",
      "prompter that we just made and I'm going\n",
      "to pass that into the language model and\n",
      "all of a sudden you get classroom the\n",
      "next thing we're going to look at is\n",
      "output parsers now that's kind of a\n",
      "complicated way to say\n",
      "um we need some structured output like\n",
      "we want the language model to return a\n",
      "Json object back to us why well because\n",
      "it makes it a heck of a lot easier to go\n",
      "deal with and work with on the other\n",
      "side\n",
      "there's two big Concepts when we talk\n",
      "about output parsers first it's going to\n",
      "be the formatting instructions piece so\n",
      "this is the prompt template that is\n",
      "going to tell your language model how to\n",
      "respond back to you and Lang chain\n",
      "provides us some conventions to do this\n",
      "automatically which is cool and then the\n",
      "second thing we're going to have is\n",
      "going to be the parser and so this is\n",
      "going to be the tool that is going to\n",
      "parse the output of your language model\n",
      "so the language model can only return\n",
      "back a string but if we want a Json\n",
      "object well we need to go and parse that\n",
      "string and extract the Json Json from\n",
      "that okay so we're going to get a\n",
      "structured output parsing and we're\n",
      "going to get the response schema from\n",
      "there let's import our language model\n",
      "again and we're going to have a response\n",
      "schema so in this case I just want it to\n",
      "be a two field Json object it I'm going\n",
      "to have a bad string which is a poorly\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"formatted user input string and then a\n",
      "good string this is your response a\n",
      "formatted response and so the really\n",
      "nice response from the um from the\n",
      "language model there and in this case\n",
      "I'm going to go ahead and create my\n",
      "output parser which is going to read the\n",
      "response schema and it's going to be\n",
      "able to parse it for us but we won't use\n",
      "that until just a second here\n",
      "so first thing we're going to have is\n",
      "our format instructions so on the output\n",
      "and parser we're going to say get format\n",
      "instructions and then let's print those\n",
      "out\n",
      "in fact I don't need to do that I could\n",
      "just print this out directly right here\n",
      "cool and so this is a piece of text that\n",
      "is going to be input or insert put into\n",
      "the prompt the output should be a\n",
      "markdown code snippet format it in the\n",
      "following schema Json and then the two\n",
      "fields that I input up above but it did\n",
      "the formatting for me or at least Lane\n",
      "chain did for him to put it into here\n",
      "so let's go ahead and create a prompt\n",
      "template we're going to do a placeholder\n",
      "variable for our format instructions and\n",
      "then we're also going to do a\n",
      "placeholder for user input this will be\n",
      "the poorly formatted string that the\n",
      "user is going to input and then finally\n",
      "I put your response here just to tell it\n",
      "it's like hey I'm done telling you\n",
      "instructions give me a response we go\n",
      "ahead and we get the prompt template we\n",
      "have the user input we have a partial\n",
      "variable of format instructions and this\n",
      "will be the format instructions we had\n",
      "up above we have our template which is\n",
      "the string up above here and then we\n",
      "have our prompt value so this will be\n",
      "the actual value that is filled out with\n",
      "the variables I tell it and I'm going to\n",
      "say welcome to California with an\n",
      "exclamation point let's go ahead and do\n",
      "that one and here I print out the final\n",
      "prompt that is going to be sent to the\n",
      "llm we have user input Welcome to\n",
      "California with everything we had up\n",
      "above let's go ahead and run this\n",
      "let's see what it responds back to us so\n",
      "we get a string here it kind of looks\n",
      "like gobbledygook but if we were to\n",
      "print this out it'd make more sense but\n",
      "before printing out let's just go ahead\n",
      "and parse this and now we can actually\n",
      "parse this and we get a nice uh Json\n",
      "object back well in this case it's going\n",
      "to be addict but um you can see here\n",
      "it's typed\n",
      "the next thing we're going to look at is\n",
      "different indexes so in this case we're\n",
      "going to be structuring documents in a\n",
      "way that language models have a better\n",
      "time working with them and one of the\n",
      "main ways that lanechain does this it's\n",
      "going to be through document loaders now\n",
      "this is very similar to the open AI\n",
      "plugins that just were released however\n",
      "there's a lot of support for a lot of\n",
      "really cool data sources in langchain\n",
      "that aren't yet supported within the\n",
      "plug-in World in this case I'm going to\n",
      "be doing a Hacker News data loader so\n",
      "all I'm doing is just passing a simple\n",
      "URL to this data loader I'm going to say\n",
      "hey go get me that data and so I'm\n",
      "asking hey how many pieces of data did\n",
      "you find\n",
      "uh and in this case it found 76\n",
      "different comments within this Hacker\n",
      "News Post and I asked it to print me out\n",
      "a sample and here we see uh one of the\n",
      "responses by the moderator dang within\n",
      "uh Hacker News and we see the response\n",
      "there we see different comments uh you\n",
      "can go and work with these within your\n",
      "language model now which is pretty cool\n",
      "another big piece of what we do a ton of\n",
      "is text splitting so oftentimes your\n",
      "document like your book or your essay or\n",
      "whatever is going to be too long for\n",
      "your language model you need to split it\n",
      "up into chunks and text Splitters will\n",
      "help with this now the reason why you do\n",
      "this is because if you want a single\n",
      "answer out of a book it wouldn't behoove\n",
      "you too much to input that entire book\n",
      "Into The Prompt one because it's too\n",
      "long but two is because the signal to\n",
      "noise ratio is too much or it's too\n",
      "little for your language model to\n",
      "effectively do its job it'd be a lot\n",
      "better if you just put in a few pieces\n",
      "of text into there and in order to get\n",
      "those few pieces of text we need to do\n",
      "splitting or chunking of those so in\n",
      "this case I'm going to do text splitting\n",
      "and the one that I use most often is\n",
      "going to be the recursive character text\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"splitter there's a lot of different\n",
      "types of text Splitters depending on\n",
      "your use case I encourage you to go\n",
      "check those out and in this case I'm\n",
      "going to pull in a Paul Graham essay his\n",
      "worked essay this one is quite long it\n",
      "may be as long as actually so if I were\n",
      "to read his document\n",
      "I just have one big long document right\n",
      "now which means it's a really long piece\n",
      "of text but in this case what I want to\n",
      "do is I want to have the recursive\n",
      "character text splitter and I'm going to\n",
      "say chunk size equals 150. this means\n",
      "that I'm going to have a size of 150\n",
      "when I end up splitting my star document\n",
      "there and if you want chunk overlap that\n",
      "means that the Venn diagram of your docs\n",
      "is going to overlap just a little bit I\n",
      "encourage you to play with these\n",
      "variables to see which one works best\n",
      "for your use case normally I wouldn't do\n",
      "150 I'd probably do a thousand or two\n",
      "thousand but for demonstration purposes\n",
      "I'm doing 150. go ahead and run that and\n",
      "so we had one document up above but\n",
      "after I split it I now have 606\n",
      "documents all right and if I wanted to\n",
      "preview those I can go ahead and preview\n",
      "these and see how they're nice and small\n",
      "they're super small and if I wanted to\n",
      "make this 50 for example well then my\n",
      "chunks will be a whole lot smaller but\n",
      "let me go ahead and make that bigger the\n",
      "next thing we're going to look at is\n",
      "going to be retrievers now retrievers\n",
      "are easy ways to combine your documents\n",
      "with your language models there's going\n",
      "to be a lot of different types of\n",
      "retrievers and the most widely supported\n",
      "one is going to be the vector store\n",
      "Retriever and it's most widely supported\n",
      "because we're doing so much similarity\n",
      "search within embeddings let's look at\n",
      "an example here we're going to load up a\n",
      "hologram essay just like how we had\n",
      "beforehand I'm going to do some\n",
      "splitting of it and so we're going to\n",
      "get a whole bunch of documents\n",
      "we're going to split the documents and\n",
      "then I'm going to create embeddings out\n",
      "of those documents and so all those\n",
      "little chunks we're going to create\n",
      "vectors out of them which is the\n",
      "semantic meaning of them and then I'm\n",
      "going to store those vectors within a\n",
      "document store here okay and I'm going\n",
      "to call that within a my DB there and\n",
      "then I'm going to say hey this retriever\n",
      "is going to be the DB but we're going to\n",
      "set it as the retriever okay so it knows\n",
      "to go get stuff and if I were to look at\n",
      "this you can see here that we have our\n",
      "Vector store retriever that's output\n",
      "right here\n",
      "okay we're going to take our Retriever\n",
      "and I'm going to say hey go get me the\n",
      "relevant documents what types of things\n",
      "did the author want to build now in the\n",
      "background what it's doing here is it's\n",
      "taking the string and it's converting it\n",
      "to a vector it's taking that vector and\n",
      "it's going to go compare it to the\n",
      "vector store that you have and find the\n",
      "similar documents that come from there\n",
      "so what I'm going to do here is I'm just\n",
      "going to print out this is a one-liner\n",
      "kind of complicated one just to print\n",
      "out the preview of the documents that we\n",
      "have here I'm just going to have it\n",
      "print out the first two\n",
      "docs is not defined great let's go ahead\n",
      "and run those so all of a sudden these\n",
      "are the previews of the docs that it\n",
      "found there\n",
      "um what I wanted was to not just build\n",
      "things but build things that would last\n",
      "so you can see here that out of all\n",
      "those documents that I found it found\n",
      "the two that were most similar to what I\n",
      "was looking for which is really cool I\n",
      "wanted to build things nice next let's\n",
      "look at Vector stores so we briefly just\n",
      "talked about Vector stores right before\n",
      "this but to go into it a little bit\n",
      "further think of a vector store really\n",
      "the way that I think about it is a table\n",
      "with rows with your embeddings and\n",
      "Associated metadata that comes with it\n",
      "an example of it is right here two main\n",
      "players in the space are now are going\n",
      "to be pine cone and weeviate however if\n",
      "you want to you can go check out open\n",
      "ai's retriever documentation and they\n",
      "list a whole bunch of other ones that\n",
      "you may find awesome for you\n",
      "okay so let's go ahead and look at these\n",
      "again we're going to import our models\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"we got our embeddings okay cool now with\n",
      "these embeddings I'm gonna look at that\n",
      "and based off of how I split my document\n",
      "up above with a thousand chunks or a\n",
      "thousand as a chunk size we get 78\n",
      "documents Auto programs worked essay\n",
      "okay what I'm going to do is I'm going\n",
      "to create those as embeddings and I'm\n",
      "going to get my embeddings list from\n",
      "there and I'm going to let's look at the\n",
      "length of the embedding list I have 78\n",
      "embeddings reason why is because I have\n",
      "one vector for each one of my documents\n",
      "so all right makes sense and here's a\n",
      "sample of one so here's an example of\n",
      "what the embedding would look like it's\n",
      "a numerical representation of the\n",
      "semantic meaning of your document there\n",
      "so your vector store is going to be\n",
      "storing your embeddings and it makes\n",
      "them easily searchable so in this case\n",
      "it is going to take my embedding and\n",
      "it's going to store it like a database\n",
      "the next topic I want to look at is\n",
      "going to be Memory so this is going to\n",
      "be how you help your language models\n",
      "remember things the most common use case\n",
      "for this is going to be your chat\n",
      "history so if you're making a chat bot\n",
      "then you can tell up the history\n",
      "messages that you've had beforehand\n",
      "which makes it a whole lot better at\n",
      "helping your user do whatever it needs\n",
      "to do so in this case I'm going to\n",
      "import chat message history and I'm\n",
      "going to import my chat open AI again\n",
      "and so I'm going to create my chat model\n",
      "and then I'm going to create my history\n",
      "model and to my history model I'm going\n",
      "to add an AI message Hi and then I'm\n",
      "going to add a user message what is the\n",
      "capital of France so let me go ahead and\n",
      "run that and if I were to take a look at\n",
      "my history messages I get my two that\n",
      "are input right there they're in the\n",
      "right order as we would expect there to\n",
      "be\n",
      "so what's cool is that I can pass my\n",
      "history of messages to the language\n",
      "model and so in this case it is going to\n",
      "read oh I said hi to start and then the\n",
      "human message was what's the capital of\n",
      "France and let's see what it responds\n",
      "back to us the capital France is Paris\n",
      "and it gives us an AI message which is\n",
      "cool and so what I want to do here is I\n",
      "want to add an AI message to my history\n",
      "which uh I shouldn't repeat this but I\n",
      "am actually no I'm not repeating it I'm\n",
      "taking the AI response and I'm just\n",
      "putting out the content let me print out\n",
      "those messages again and you can see\n",
      "here that it adds uh the capital Francis\n",
      "Paris to the end of my chat history\n",
      "which makes it easy for me to work with\n",
      "and another cool functionality of this\n",
      "too is link chain makes it extremely\n",
      "simple to save this chat history so you\n",
      "can go ahead and load it later a lot of\n",
      "really cool functionality I encourage\n",
      "you to go check out the next concept\n",
      "we're going to look at is chains so in\n",
      "this case we're going to be combining\n",
      "different llm calls and actions\n",
      "automatically so say you have one input\n",
      "but then the output of that language\n",
      "model you want to use as the input to\n",
      "another call and then another call and\n",
      "then another call well in that case\n",
      "you're going to be using chains which is\n",
      "where the chain and Lang chain comes\n",
      "from so in this case we're going to\n",
      "cover two of them there's a lot of\n",
      "really complicated examples here I\n",
      "encourage you again to go check out the\n",
      "documentation to see if one of them\n",
      "would cover your use case better than\n",
      "what you're seeing here the first one is\n",
      "going to be a simple sequential chain\n",
      "and in this case I'm going to go ahead\n",
      "and tell it hey I want you to do X and\n",
      "then Y and then Z now the reason why\n",
      "this is important\n",
      "or why I like to do it is because it\n",
      "helps break up the tasks Now language\n",
      "models can get distracted sometimes and\n",
      "if you ask it to do too many things in a\n",
      "row it could get it could get confused\n",
      "it could start to hallucinate and that's\n",
      "not good for anybody Plus\n",
      "I want to make sure that my thinking is\n",
      "sound and that way I can kind of check\n",
      "out the different outputs of each one of\n",
      "my different actions here so in this\n",
      "case I'm going to import the simple\n",
      "sequential chain let me go ahead and run\n",
      "this and I'm going to put two different\n",
      "things to here I'm going to use two\n",
      "different prompt templates so your job\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"is to come up with a classic dish from\n",
      "the area that the user suggests I'm\n",
      "going to input the user location and I'm\n",
      "going to give it the user location which\n",
      "we'll we'll do in a second here\n",
      "and I'm going to create a llm chain with\n",
      "this and I'm going to call it location\n",
      "chain which basically is going to take\n",
      "my language model it's going to take a\n",
      "prompt template okay\n",
      "and then the next one we're going to\n",
      "look at given a meal give a short and\n",
      "simple recipe on how to make that dish\n",
      "at home so in this case we have the user\n",
      "location which that's not actually what\n",
      "we want we want user meal output this\n",
      "wouldn't have mattered because I had the\n",
      "variables the same but it just to make\n",
      "it more clear\n",
      "given a meal okay cool your response I'm\n",
      "going to do the same thing I'm going to\n",
      "put that into a meal chain so what it's\n",
      "going to do is it's going to Output a\n",
      "meal a classic dish and then it's going\n",
      "to Output a simple recipe for that\n",
      "classic dish\n",
      "okay I'm gonna create my simple\n",
      "sequential chain and in this case I'm\n",
      "going to specify My Chains as my\n",
      "location chain and then the meal chain\n",
      "order matters be careful on that I'm\n",
      "going to set verbose equals true which\n",
      "means that it's going to tell us what\n",
      "it's thinking and it's actually going to\n",
      "print those statements out so it's\n",
      "easier to debug what's going on\n",
      "let's go ahead and create that and then\n",
      "I'm going to say My overall chain I want\n",
      "you to run and in this case I only have\n",
      "one input variable which is going to be\n",
      "Rome which is going to be the user\n",
      "location that I start in the first place\n",
      "let me go ahead and run this so you can\n",
      "see here that it's entering the new\n",
      "sequential chain and it ran Rome against\n",
      "the first prompt template and got me a\n",
      "classic dish which is really cool\n",
      "and then it gave me a recipe to on how\n",
      "to make that classic dish which is\n",
      "really cool so all of a sudden it just\n",
      "did two different runs for me all in one\n",
      "go and I didn't have to run any\n",
      "complicated code I could just use\n",
      "langtain for that it's pretty sweet\n",
      "now the next one that I want to show is\n",
      "one that I use quite often which is\n",
      "going to be the summarization chain the\n",
      "reason why this one was so cool is\n",
      "because if you have a long piece of text\n",
      "and you want it summarized or say you\n",
      "have an article you want summarized or a\n",
      "tweet thread or a Hacker News Post or\n",
      "whatever it may be you're going to want\n",
      "to Chunk Up Your longer piece of text\n",
      "and you're going to want to find you're\n",
      "going to want to find summaries of those\n",
      "different chunks and then get a final\n",
      "summary and in that case what we're\n",
      "going to do is we're going to load in\n",
      "load summarize chain and we're going to\n",
      "do Paul Graham's essay disk not even\n",
      "sure what that one's about then we're\n",
      "going to split it up into different\n",
      "texts right here the chunk size is going\n",
      "to be 700 and then I'm going to load\n",
      "summarize chain and the chain type that\n",
      "I'm going to do is going to be that one\n",
      "that I mentioned beforehand which is\n",
      "where you get the small summaries of the\n",
      "individual sections and then you get a\n",
      "summary of the small summaries I have a\n",
      "whole video on different chain types and\n",
      "so if you're curious go check out the\n",
      "video up above and you can go see it\n",
      "let me go ahead and run this and so as\n",
      "you can see here the language model is\n",
      "asking I'm sorry the chain or Lang chain\n",
      "is asking the language model to\n",
      "summarize this piece of text right here\n",
      "and then this piece of text right here\n",
      "because we only had two chunks that we\n",
      "wanted to summarize and then it's asking\n",
      "for a final concise summary so here's\n",
      "the summary of the chunk number one\n",
      "here's the summary of Chunk number two\n",
      "and it's asking for a summary of the\n",
      "summaries and we finally get a summary\n",
      "of the summaries which is really cool\n",
      "because all built into this one liner\n",
      "right here was all the different calls\n",
      "back and forth to figure out how to do\n",
      "the summary of the summaries which is\n",
      "one of the powers of Lane chain which is\n",
      "really sweet the last thing we're going\n",
      "to look at is agents and this is one of\n",
      "the most complicated Concepts within\n",
      "link chain which is why we're talking\n",
      "about it last here but I thought that\n",
      "the official link chain documentation\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"did a great job describing what agents\n",
      "are\n",
      "some applications will not require just\n",
      "a predetermined chain of calls to llms\n",
      "and other tools what we did up above was\n",
      "a predetermined chain here but\n",
      "potentially an unknown chain that\n",
      "depends on the user input an unknown\n",
      "chain emphasis mine means that we're not\n",
      "really sure what route we want to take\n",
      "but we want the language model to tell\n",
      "us which route it thinks that it should\n",
      "take\n",
      "in these types of chains there is an\n",
      "agent which has an access to a suite of\n",
      "tools depending on the user input the\n",
      "agent can then decide which if any of\n",
      "these tools to call\n",
      "so for example hey you have two\n",
      "databases you could pick information\n",
      "from they're on completely different\n",
      "topics the user just asked you a\n",
      "question about uh trees\n",
      "which database should you go looking to\n",
      "find your tree information well an agent\n",
      "can decide that which is really sweet so\n",
      "I'm going to go over the vocabulary\n",
      "first and then we're going to look at an\n",
      "example so an agent is the language\n",
      "model that is going to be driving the\n",
      "decision making cool\n",
      "tools or tool is going to be a\n",
      "capability of the agent so you can think\n",
      "of this as similar to the open AI\n",
      "plugins that just came out you can also\n",
      "think of this as the ability to go\n",
      "search Google the ability to go lick\n",
      "your email whatever it may be\n",
      "a tool kit is going to be a collection\n",
      "of tools so an agent will have a toolkit\n",
      "of tools uh\n",
      "an agent will have a toolkit of tools\n",
      "and that's what that's what it's going\n",
      "to do there I'm going to import load\n",
      "tools I'm going to initialize the agent\n",
      "I'm going to import openai as well\n",
      "with that I'm going to create my\n",
      "language model now I've made I've insert\n",
      "my serp API key because that's the\n",
      "example that we're going to be running\n",
      "through here which is an easy way to\n",
      "search Google\n",
      "and then with the toolkit I'm going to\n",
      "go ahead and load the tools now in this\n",
      "case I'm only loading one tool and it's\n",
      "the server API however you could load in\n",
      "a lot of tools here and you may\n",
      "naturally think well let me just load it\n",
      "up with all the tools in the world you\n",
      "could it's just going to get difficult\n",
      "for the model or the agent to know which\n",
      "tool to use at which time so you kind of\n",
      "only want to use the ones that you know\n",
      "you're going to um\n",
      "be needing at that at that point so I'm\n",
      "going to pass in my language model and\n",
      "I'm going to pass in my serve API here\n",
      "API key then I'm going to create my\n",
      "agent so I'm going to pass in the\n",
      "toolkit that I just made I'm going to\n",
      "pass in the language model again I'm\n",
      "going to say what type of agent are you\n",
      "now there's different agent types for\n",
      "different types of tasks and I encourage\n",
      "you to go check out the language or the\n",
      "documentation to see which would be best\n",
      "for you I'm going to say verbose equals\n",
      "true so we can see it thinking I'm also\n",
      "going to return the intermediate steps\n",
      "which just means that we get more\n",
      "granularity into what it's actually\n",
      "doing\n",
      "with this I'm going to say response uh\n",
      "oh agent is not defined then what I'm\n",
      "going to do here is I'm going to pass in\n",
      "my query to the agent itself so what was\n",
      "the first album of the band that Natalie\n",
      "Bergman is a part of the reason why I\n",
      "asked this question specifically is\n",
      "because keep in mind I haven't uploaded\n",
      "any documents here so there's no\n",
      "information pre-loaded and it's kind of\n",
      "a complicated question that has multiple\n",
      "steps that need to be answered for it\n",
      "this is a perfect question for an agent\n",
      "here so let's go ahead and run this and\n",
      "let's see how the agent is thinking\n",
      "about it entering the new agent executor\n",
      "class and it said I should try to find\n",
      "out what band Natalie Bergman is a part\n",
      "of so it needs to it knows that it needs\n",
      "to go search which it has a Search tool\n",
      "up above which I gave it and it's saying\n",
      "Natalie Bergman banded so it's searching\n",
      "for that one and it says observation\n",
      "which is what it observed from its\n",
      "action Natalie Bergman is an American\n",
      "singer-songwriter she has one half the\n",
      "duo of wild Belle okay cool I should\n",
      "search for the debut album of wild Belle\n",
      "it understood the band that she's a part\n",
      "of and now it now it knows and needs to\n",
      "go search for that band so it's going to\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"search again it's going to say wild\n",
      "Belle debut album and it observes that\n",
      "the debut album is Isles I know the\n",
      "final answer which is good we want it to\n",
      "know the finance\n",
      "uh Isles is the debut album of wild\n",
      "Belle the band that Natalie Bergman is a\n",
      "part of that is really cool because that\n",
      "is a multi-step question and uh the\n",
      "agent knew what it needed to go find out\n",
      "without me telling it the chain so this\n",
      "chain could have been a whole lot longer\n",
      "if it needed more steps with it but uh\n",
      "it dynamically figured that out along\n",
      "the way which is really really cool and\n",
      "so if we were to print out the\n",
      "intermediate steps you get more\n",
      "information about what it actually did\n",
      "and how it searched and all that good\n",
      "information from there\n",
      "um and if we were to confirm this let's\n",
      "go ahead and run this WOW yep wild Belle\n",
      "there's Natalie Bergman brother and\n",
      "sister Duo band\n",
      "um beautiful I would play their song If\n",
      "it wasn't going to give me copyright\n",
      "trouble but I encourage you to go look\n",
      "it up link to my favorite song of theirs\n",
      "is in the description well my friends\n",
      "that was a very broad overview of all of\n",
      "the nuts and bolts of lion chain the\n",
      "Tactical nuts and bolts I congratulate\n",
      "you for making it to the end of this\n",
      "video and if you have any questions\n",
      "please let me know I encourage you to\n",
      "subscribe to check out for part two when\n",
      "we go through actual use cases for these\n",
      "nuts and bolts and again I share a lot\n",
      "of tools on Twitter so I encourage you\n",
      "to follow me there like always please\n",
      "leave comments let me know what you\n",
      "think of the video and let me know if\n",
      "you have any questions we'll see you\n",
      "later\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The video provides an introduction to Lang chain, a framework for developing applications powered by language models. It explains the qualitative concepts and components of Lang chain, such as integration and agency. The video also highlights the benefits of using Lang chain, including its ability to swap out components and customize chains easily. A companion Lang chain cookbook is mentioned, which provides further examples and code snippets.\n",
      "\n",
      "The summary explains that the support for using and customizing chains in LinkedIn allows for the combination of series of actions. The speed of LinkedIn is praised, along with its community and various learning resources. The Lang chain is introduced as a tool for developing applications and overcoming friction points. The summary also mentions that the provided cookbook is not a replacement for online documentation but serves as an overview of Lang chain capabilities. The concept of text as a programming language and the different types of chat messages are discussed. Examples of using the chat model and obtaining AI responses are provided.\n",
      "\n",
      "The summary is about different models and how they interact with text. It discusses language models, chat models, and text embedding models. It explains their differences and provides examples of how they are used.\n",
      "\n",
      "The author discusses text embeddings, which are numerical representations of text that capture its meaning. They explain how prompts are used to send instructions to language models, and introduce the concept of prompt templates for dynamically generating prompts. The author also mentions the use of example selectors to choose relevant examples when constructing prompts.\n",
      "\n",
      "The summary describes the process of importing a semantic similarity example selector and using it to select similar examples based on their semantic meaning. The OpenAI embeddings model is used to match examples, and a prompt template is created with the selected examples. The prompt is then passed into the language model, and the output is parsed to return a structured JSON object.\n",
      "\n",
      "The summary discusses the process of formatting user input, creating a prompt template, and parsing the response in a language model. It also mentions the use of document loaders to work with external data sources and the importance of text splitting for effective language model processing.\n",
      "\n",
      "The text discusses different types of text splitters and encourages the reader to explore them. It also demonstrates the use of a recursive character text splitter with a chunk size of 150. The concept of retrievers is introduced, specifically the vector store retriever, which combines documents with language models. The process of splitting documents, creating embeddings, and storing vectors in a document store is explained. The retrieval of relevant documents based on similarity is demonstrated. The concept of vector stores is further explained as a table with rows of embeddings and associated metadata. Mention is made of various vector store platforms like Pinecone and Weaviate.\n",
      "\n",
      "The summary describes the use of embeddings to represent documents and make them easily searchable. It also explains how chat history can be used to improve language models in chatbots. The concept of chains is introduced, which allows for sequential actions to be performed by language models.\n",
      "\n",
      "The summary discusses the use of language models and chain types in Link Chain. It demonstrates how to create a location chain and a meal chain to generate classic dishes and recipes based on user input. It also shows how to use a summarization chain to summarize longer pieces of text and obtain a concise summary. The concept of agents in Link Chain is briefly mentioned as a more complex topic.\n",
      "\n",
      "This passage discusses the concept of agents and their applications in decision making. It highlights the importance of an agent's ability to access and utilize different tools based on user input. The passage also explains the process of creating an agent and loading tools into its toolkit. An example is provided to demonstrate how an agent can use its tools to answer a complex question.\n",
      "\n",
      "The speaker discusses their experience using a software called Lion Chain and highlights its ability to dynamically search for information and provide accurate results. They mention the debut album of a band called Wild Belle and express admiration for the software's capability to find the answer without needing additional instructions. The speaker also mentions the band's members and encourages viewers to look up their music. They conclude by mentioning future videos on the topic and encourage viewers to subscribe and provide feedback.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The video introduces Lang chain, a framework for developing applications powered by language models. It explains the components and benefits of Lang chain, and mentions a companion cookbook for further examples. The video discusses different types of models and how they interact with text, including language models, chat models, and text embedding models. It explains the use of prompts, prompt templates, and example selectors. The process of importing and using a semantic similarity example selector is described. The video also discusses the use of text splitters, document loaders, and retrievers. The concept of vector stores and various platforms are mentioned. The video explains how chat history can improve language models and introduces the concept of chains. It demonstrates the creation of different chain types in Lang chain, such as location, meal, and summarization chains. The concept of agents and their application in decision making is discussed, along with the process of creating an agent and loading tools into its toolkit. The speaker shares their positive experience using Lion Chain software and its ability to dynamically search for information. They mention the debut album of Wild Belle and encourage viewers to check out the band's music. The video concludes by mentioning future videos on the topic and encouraging viewers to subscribe and provide feedback.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hello good people have you ever wondered\n",
      "what Lang chain was or maybe you've\n",
      "heard about it and you've played around\n",
      "with a few sections but you're not quite\n",
      "sure where to look next well in this\n",
      "video we're going to be covering all of\n",
      "the lane chain Basics with the goal of\n",
      "getting you building and having fun as\n",
      "quick as possible my name is Greg and\n",
      "I've been having a ton of fun building\n",
      "out apps in langchain now I share most\n",
      "of my work on Twitter so if you want to\n",
      "go check it out links in the description\n",
      "you can go follow along with me now this\n",
      "video is going to be based off of the\n",
      "new conceptual docs from lanechain and\n",
      "the reason why I'm doing a video here is\n",
      "because it takes all the technical\n",
      "pieces and abstracts them up into more\n",
      "theoretical qualitative aspects of Lane\n",
      "chain which I think is extremely helpful\n",
      "for it and in order to understand this a\n",
      "little bit better I've created a\n",
      "companion for this video and that is the\n",
      "Lang chain cookbook links in the\n",
      "description if you want to go check that\n",
      "out please go and check out the GitHub\n",
      "and you can follow along here I'm gonna\n",
      "put a lot of time stamps in the\n",
      "description as well there's gonna be a\n",
      "fair amount of content in this one so\n",
      "you can watch it all the way through or\n",
      "if you want to skip to a certain section\n",
      "feel free to jump to that time stamp all\n",
      "right without further Ado let's jump\n",
      "into it all right here are the new\n",
      "conceptual docs from Lang chain now the\n",
      "reason why these are different is\n",
      "because there are the python docs which\n",
      "are going to be the more technical\n",
      "focused one or the JavaScript docs as\n",
      "well which is also more technical\n",
      "documentation however these concepts are\n",
      "more qualitative so you can understand\n",
      "what is going on in the background of\n",
      "these different sections here now we're\n",
      "going to focus on these components of\n",
      "Lang chain there's an entire section on\n",
      "use cases which is when you actually put\n",
      "these into practice and that is going to\n",
      "be a part two of this video so we won't\n",
      "jump into this today that would be a too\n",
      "long for us we're going to run through\n",
      "schema models prompts indexes indexes\n",
      "memory chains and agents with a working\n",
      "code sample for each one of those well\n",
      "without further Ado let's jump into some\n",
      "code here we are with the Lang chain\n",
      "cookbook now my goal is to make this a\n",
      "dense document with a ton of links so\n",
      "you can go and self-service right into\n",
      "their links in the description and if\n",
      "you want to follow along I encourage you\n",
      "to get this on your computer and go from\n",
      "it go for it from there so the goal of\n",
      "this dock is to provide an introductory\n",
      "understanding of the components and use\n",
      "cases of Lang chain in a explain like M5\n",
      "way with examples and code Snippets for\n",
      "use cases check out part two which is\n",
      "not made yet that is coming soon\n",
      "hopefully by the time you see this it\n",
      "will be a bunch of links here we go into\n",
      "what is Lang chain so Lang chain is\n",
      "going to be a framework for developing\n",
      "applications powered by language models\n",
      "well Greg openai just came out with\n",
      "plugins yes but there is a whole lot of\n",
      "other things that you can do with\n",
      "language models outside of those and\n",
      "Lang chain helps abstract a ton of that\n",
      "so that you're able to work with it more\n",
      "easily and intermix different pieces and\n",
      "customize really how you need to\n",
      "so Lane chain makes the complicated\n",
      "parts of working and building with AI\n",
      "models easier it does this in two main\n",
      "ways the first big way is going to be\n",
      "through integration so you can bring\n",
      "external data such as your files other\n",
      "applications API data to your language\n",
      "models which is cool the other big way\n",
      "that it helps do this is through agency\n",
      "so it allows your language models to\n",
      "interact with its environment via\n",
      "decision making basically you're using\n",
      "the language model to help decide which\n",
      "action to take next and you do this when\n",
      "the path isn't so clear or it may be\n",
      "unknown and we'll get into more more of\n",
      "that later\n",
      "so why link chain specifically there are\n",
      "four big reasons why I like Lane chain\n",
      "the first one is going to be for the\n",
      "components Lang chain makes it easy to\n",
      "swap out abstractions and components\n",
      "necessary to work with language models\n",
      "basically they've created a ton of tools\n",
      "that make it super simple to work with\n",
      "language models like chat GPT or\n",
      "anything on how you face how you may\n",
      "want also because it allows you to\n",
      "customize chains really easily so\n",
      "there's a ton of out of the box of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The summary is about a video discussing the basics of Lang chain, a framework for developing applications using language models. The video covers the different components of Lang chain, such as schema models, prompts, indexes, memory chains, and agents, with code samples provided. The video also explains how Lang chain simplifies working with AI models through integration of external data and decision-making capabilities. The video highlights four reasons to use Lang chain, including its flexibility in swapping out components and its ability to customize chains easily.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "support for using and customizing chains\n",
      "basically combining series of actions\n",
      "together\n",
      "on the qualitative side of why LinkedIn\n",
      "is awesome is because the speed is great\n",
      "almost every day I need to go and make\n",
      "sure that I'm on the latest branch of\n",
      "Lang Chan and I go and I update it every\n",
      "time so the speed is awesome the other\n",
      "really cool part is the community so\n",
      "there's a ton of meetups there's a\n",
      "Discord Channel and there's a ton of\n",
      "events like webinars that go on\n",
      "throughout the week that are really\n",
      "awesome learning resources for us\n",
      "cool now again to summarize all this why\n",
      "do we need Lang chain well because\n",
      "language models can be pretty\n",
      "straightforward it is text in text app\n",
      "and you may have experienced this\n",
      "yourself however once you just start\n",
      "developing applications there's a ton of\n",
      "friction points that Lang chain is going\n",
      "to help you develop uh there's a ton of\n",
      "friction points that they're going to\n",
      "help you with basically now the last\n",
      "thing that I'll say about this before we\n",
      "jump into it is that this cookbook isn't\n",
      "going to cover all of the aspects of\n",
      "Lang chain this isn't meant to be a\n",
      "replacement for the documentation online\n",
      "this is meant to show you a very broad\n",
      "overview about the capabilities that\n",
      "there are with my interpretation of them\n",
      "and my voice over with it and with that\n",
      "I'm hoping that you can get to building\n",
      "and impact as quick as possible\n",
      "I'm super curious to see what you build\n",
      "so please let me know and uh I will\n",
      "hopefully uh I would love to see it\n",
      "first thing we're going to do is we're\n",
      "going to import our openai API key now I\n",
      "have a hidden cell here but you're going\n",
      "to replace your API key API key right\n",
      "here just throw that in there the first\n",
      "aspect of link chain components that\n",
      "we're going to look at is the schema now\n",
      "I almost didn't even include this one\n",
      "but the first one is going to be text\n",
      "now what's really cool about these\n",
      "language models is that text is the new\n",
      "programming language not verbatim not\n",
      "per se but we're using a lot more\n",
      "English language to tell language models\n",
      "what to do in this case what day comes\n",
      "after Friday is an example of something\n",
      "I may go tell a language model and it is\n",
      "going to respond back to me with a\n",
      "natural language response very cool next\n",
      "up is going to be chat messages so like\n",
      "text chat messages are similar but they\n",
      "have different types the first type is\n",
      "going to be system and this is helpful\n",
      "background context that tell the AI what\n",
      "to do all right like your helpful\n",
      "teacher assistant bot or something then\n",
      "we have human messages and these are\n",
      "messages that are intended to represent\n",
      "the user and so literally user input or\n",
      "something that I may text from it then\n",
      "we have ai messages and these are\n",
      "messages that show what the AI responded\n",
      "with and the cool part about this is the\n",
      "AI may or may not have actually\n",
      "responded with it but you can tell it\n",
      "that it did so that it has additional\n",
      "context on how to answer you okay so\n",
      "what I'm going to do here is I'm going\n",
      "to import chat open Ai and my three\n",
      "message types and then I'm going to\n",
      "create my chat model I'm going to do\n",
      "that and then I'm going to type in two\n",
      "messages the first system message is you\n",
      "are a nice AI bot that helps a user\n",
      "figure out what to eat in a short\n",
      "sentence and then a human message I like\n",
      "tomatoes what should I eat let me go\n",
      "ahead and run this\n",
      "and you get an AI message back because\n",
      "this is what it responds with you could\n",
      "try making a tomato salad with fresh\n",
      "basil and mozzarella cheese thanks AI\n",
      "That's cool what you can also do is you\n",
      "can also pass more chat history and get\n",
      "responses from the AI so in this case\n",
      "you're a nice AI bot that helps a user\n",
      "figure out where to travel to in one\n",
      "short sentence\n",
      "I'm saying I like the beaches where\n",
      "should I go\n",
      "I'm telling it that it responded to me\n",
      "it didn't actually do this but I'm\n",
      "telling it that it did you should go to\n",
      "Nice France cool what else should I do\n",
      "when\n",
      "when I'm there and so the reason why I\n",
      "did this one is because you'll notice\n",
      "that I didn't say where I went it's\n",
      "going to have to infer from the history\n",
      "on what where I went and it says wow and\n",
      "nice and so it picked up where I was\n",
      "because it gets the history of the chat\n",
      "messages now if you're making a chatbot\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video discusses the basics of Lang chain, a framework for developing applications using language models. It covers components such as schema models, prompts, indexes, memory chains, and agents, with code samples provided. Lang chain simplifies working with AI models through integration of external data and decision-making capabilities. The video highlights four reasons to use Lang chain, including its flexibility in swapping out components and its ability to customize chains easily.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "you could see how you could append\n",
      "different messages that have been back\n",
      "and forth uh I'm not sure if that's a\n",
      "verb but back and forth through the user\n",
      "okay\n",
      "the next model that we're the next model\n",
      "that we're going to look at is going to\n",
      "be documents so documents are important\n",
      "because this represents a piece of text\n",
      "along with Associated metadata now\n",
      "metadata is just a fancy word for things\n",
      "about that document and in this case\n",
      "this document or the text is held within\n",
      "a field called page content so this is\n",
      "my document it's full text that I've\n",
      "gathered from other places awesome and\n",
      "then I'm going to pass in some metadata\n",
      "and this metadata is a dictionary of key\n",
      "value pairs my document ID which is my\n",
      "key here and then some random document\n",
      "ID here that happens to be an INT it\n",
      "could be whatever you want it to be my\n",
      "document Source this is the Lang chain\n",
      "papers and then my document create time\n",
      "is going to be some timestamp whatever\n",
      "you want it to be and this is going to\n",
      "be can be whatever format you want this\n",
      "is extremely helpful for when you're\n",
      "making a large repositories of\n",
      "information and you want to be able to\n",
      "filter by it so instead of just going\n",
      "and asking link chain to look at all\n",
      "your documents in your database you can\n",
      "go ahead and filter these by a certain\n",
      "metadata go ahead and run this and you\n",
      "can see here I get a document object\n",
      "with a bunch of metadata on it from\n",
      "there cool if those are the schemas that\n",
      "we work with the next thing we're going\n",
      "to look at is the different models now\n",
      "these are the ways of interacting with\n",
      "well different models\n",
      "um but the reason why this is important\n",
      "is because they're different model types\n",
      "let me just show an example here the\n",
      "normal one that we're looking at is\n",
      "going to be the language model and this\n",
      "is when text goes in and text comes out\n",
      "okay now the first thing I'll do is I'll\n",
      "import open Ai and I'll make my model\n",
      "and you'll notice here that I changed my\n",
      "model in case you ever want to change\n",
      "your model as well and so I'm going to\n",
      "pass in a regular string into this one\n",
      "into my language model what day comes\n",
      "after Friday\n",
      "go ahead and run this and I get Saturday\n",
      "comes out the other end but not all\n",
      "models are like this you actually have\n",
      "chat models as well and we looked at\n",
      "this in the previous example but I\n",
      "didn't call it out specifically so for\n",
      "this one I'm going to import chat open\n",
      "AI I'm going to import my messages again\n",
      "I'm going to put temperature equals one\n",
      "which means the model is going to get a\n",
      "little spicy on me no but really it just\n",
      "means it's going to have more creativity\n",
      "and it's it's a little bit more\n",
      "exaggerated and so in this case I'm\n",
      "going to say you are an unhelpful AI bot\n",
      "that makes jokes at whatever the user\n",
      "says and in this case the user says I\n",
      "would like to go to New York how should\n",
      "I do this I'm going to go ahead and run\n",
      "this model\n",
      "you could try walking but I don't\n",
      "recommend it unless you have a lot of\n",
      "time on your hands\n",
      "maybe try flapping your arms really hard\n",
      "to see if it can fly there so as you can\n",
      "see it took that system message\n",
      "and it understood those directions and\n",
      "it uh it wasn't very helpful for me well\n",
      "because I told her not to be very\n",
      "helpful\n",
      "the last type of model that we're going\n",
      "to look at is going to be your text and\n",
      "betting model the reason why this one is\n",
      "important is because we do a lot of\n",
      "similarity searches and a lot of\n",
      "comparing texts when working with\n",
      "language models now in this case openai\n",
      "also has an AI embeddings model that\n",
      "we're going to use there's a lot of\n",
      "embedding models out there you can use\n",
      "whatever you want I just use open AI\n",
      "because it feels like it's a standard\n",
      "and it's very simple right now so I'm\n",
      "going to pass in my API key I'm going to\n",
      "get my embeddings engine ready and then\n",
      "I'm going to define a piece of text hi\n",
      "it's time for the beach let me go ahead\n",
      "and do that text\n",
      "and what I'm going to do is I'm going to\n",
      "pass that text and I'm going to embed\n",
      "that text so what that means is is it's\n",
      "going to take this string which is just\n",
      "a series of letters and it's going to\n",
      "convert it into a vector and in this\n",
      "case a vector is just simply a\n",
      "one-dimensional array meaning a list of\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video discusses the basics of Lang chain, a framework for developing applications using language models. It covers components such as schema models, prompts, indexes, memory chains, and agents, with code samples provided. Lang chain simplifies working with AI models through integration of external data and decision-making capabilities. The video highlights four reasons to use Lang chain, including its flexibility in swapping out components, its ability to customize chains easily, and the use of documents for storing text and associated metadata. It also explores different model types, including language models, chat models, and text embedding models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "numbers and that'll be a semantic\n",
      "representation of that text that's a\n",
      "fancy way of saying is that meaning of\n",
      "that text is going to be embedded in\n",
      "those numbers right there which makes it\n",
      "really easy to compare\n",
      "across other as others as well so I'm\n",
      "going to put that in a variable called\n",
      "text embeddings I'm going to see how\n",
      "long my text embeddings is and I'm going\n",
      "to get a preview of it so you'll notice\n",
      "here that my text embedding length is\n",
      "1536 this means that there are 1536\n",
      "different numbers within that list that\n",
      "represent the meaning of my text\n",
      "that's a lot of numbers and I'm glad I\n",
      "don't have to deal with them I'm glad\n",
      "the computer can so here's a sample of\n",
      "what those look like in case you're\n",
      "curious I only show the first uh five\n",
      "here but I put a dot dot dots you know\n",
      "that there are\n",
      "1531 other numbers out there next let's\n",
      "look at prompts so prompts are going to\n",
      "be the text that you send over to your\n",
      "language model we've already sent some\n",
      "prompts over to the language model but\n",
      "they've been pretty simple in this case\n",
      "we're going to start doing more\n",
      "instructional prompts and passing those\n",
      "to our model so again a prompt is what\n",
      "we pass to our language model I'm going\n",
      "to import open AI in this case I'm using\n",
      "DaVinci as my model and I'm going to say\n",
      "prompt equals this string now I use\n",
      "three three double quotes because\n",
      "um well I think it looks fancier no but\n",
      "really it's just easier to use which is\n",
      "why I like it in this case I'm not doing\n",
      "anything fancy and I could have passed\n",
      "this string right within my language\n",
      "model but in this case I\n",
      "made a variable for it because it's a\n",
      "little bit easier to understand so\n",
      "today's Monday tomorrow's Wednesday what\n",
      "is wrong with the statement the\n",
      "statement is incorrect tomorrow's\n",
      "Tuesday not Wednesday so you can see how\n",
      "it picked it up from there now why\n",
      "prompts are cool is because we start to\n",
      "get into the prompt template world\n",
      "the reason why prompt templates are\n",
      "important is because most of the time\n",
      "you're going to be dynamically\n",
      "generating your prompts meaning they\n",
      "won't just be static strings that you\n",
      "type out but you're actually going to be\n",
      "inputting tokens or inputting\n",
      "placeholders based off of the scenario\n",
      "that we're you're working with\n",
      "so in this case what I'm doing here is\n",
      "I'm importing my packages again in this\n",
      "case prompt template is going to be the\n",
      "new one I'm going to do DaVinci again\n",
      "okay great and in this case I'm going to\n",
      "create a template to start so I really\n",
      "want to travel to location you'll notice\n",
      "my opened and closed brackets around\n",
      "location which means that this is going\n",
      "to be a token that I'm going to be\n",
      "replacing later what should I do there\n",
      "respond in one short sentence because\n",
      "we're also just responded too much I'm\n",
      "going to create a prompt template in\n",
      "this case I'm going to put it in this\n",
      "variable prompt my input variable is\n",
      "going to be location which matches the\n",
      "same name that we had up here and then\n",
      "the template is this this whole thing\n",
      "that I had here the final prompt is\n",
      "going to be prompt.format which means\n",
      "going to insert the values I tell you go\n",
      "and insert the value Rome into where it\n",
      "says location right here let's go ahead\n",
      "and run this\n",
      "so final prompt I really want to travel\n",
      "to Rome which replace location up above\n",
      "and here we have our prompt template\n",
      "that's finally filled out and then in\n",
      "terms of the output it tells me what I\n",
      "should do so it took that information in\n",
      "with Rome and responded one short\n",
      "sentence it gives me this which is cool\n",
      "all right the next cool part that we're\n",
      "going to look at is the example\n",
      "selectors so often when you're\n",
      "constructing your prompts you're going\n",
      "to do something called in context\n",
      "learning this means that you're going to\n",
      "show you're going to show the language\n",
      "model what you want it to do and one of\n",
      "the main ways that people do this is\n",
      "through examples this could be about how\n",
      "to answer a customer service request or\n",
      "it could be how to respond to some\n",
      "nuanced question and in this case I'm\n",
      "going to pick examples however we have\n",
      "example selectors because say you had 10\n",
      "000 different examples you don't want to\n",
      "throw all those into your uh into your\n",
      "prompt they may not fit and they may not\n",
      "be as relevant so you want to select\n",
      "which ones you want and in this case\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video provides an overview of Lang chain, a framework for building applications using language models. It covers components such as schema models, prompts, indexes, memory chains, and agents, with code samples included. Lang chain simplifies working with AI models by integrating external data and decision-making capabilities. The video highlights the flexibility of Lang chain in swapping out components and customizing chains easily, as well as the use of documents for storing text and associated metadata. It also explores different model types, including language models, chat models, and text embedding models. The video introduces the concept of text embeddings, which represent the meaning of text through numerical values. It also explains the use of prompts and prompt templates for instructing language models, as well as example selectors for providing relevant examples to the model.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "what I'm going to do is I'm going to\n",
      "import a lot of things here but the one\n",
      "I'm the main star of the show is going\n",
      "to be the semantic similarity example\n",
      "selector that's a long name for a\n",
      "functionality that's going to select\n",
      "similar examples so I'm going to get my\n",
      "language model going again I'm going to\n",
      "get my example prompt and this is just a\n",
      "prompt template like we saw up above and\n",
      "then I'm going to define a list of\n",
      "different examples so in this case I\n",
      "want to name a noun and then I want the\n",
      "language model to tell me where this\n",
      "noun is usually found so in this case a\n",
      "pirate on a ship a pilot on a plane\n",
      "driver in a car a tree\n",
      "oh that's not true a tree in the ground\n",
      "or a bird in a nest so I'll go ahead and\n",
      "run that one\n",
      "and then what we're going to do is we're\n",
      "going to get our example selector ready\n",
      "so we have our similar example selector\n",
      "we're going to pass it the list of\n",
      "examples that I just defined above but\n",
      "then we're also going to pass it our\n",
      "embedding engine and the reason why we\n",
      "do this is because we're actually going\n",
      "to match examples on their semantic\n",
      "meaning so not just matching them off of\n",
      "similar strings but off of what they\n",
      "actually mean so in this case we're\n",
      "going to use the open AI embeddings\n",
      "which is one of the models that has been\n",
      "shared by Facebook which is really cool\n",
      "and this is going to help store our\n",
      "embeddings and then we're going to tell\n",
      "it\n",
      "um how many we want how many examples we\n",
      "want back in this case I want k equals\n",
      "two let me go ahead and run that and\n",
      "then we're going to have a new prompt\n",
      "template here and this is going to be\n",
      "the few shot prompt template meaning the\n",
      "few shot part means that there's going\n",
      "to be a few examples in there for us so\n",
      "we give it our example selector we give\n",
      "our example prompt which we made up\n",
      "above and then we're going to add on\n",
      "just some little strings before and\n",
      "after to make it easier for the model so\n",
      "give the location that an item is\n",
      "usually found in cool and then the\n",
      "suffix will be the input and the output\n",
      "that we have from here based off of what\n",
      "the user inputs then the input variable\n",
      "go ahead and do that so here I'm going\n",
      "to say my noun is student So based off\n",
      "of the noun of student it's going to go\n",
      "and find me the examples up above that\n",
      "are most closely related to student and\n",
      "we're going to use those examples so if\n",
      "I would go ahead and do this I'm going\n",
      "to say print and it's going to print me\n",
      "the prompt that we're actually going to\n",
      "use within or give to our language model\n",
      "in this case it found the driver and it\n",
      "found the pilot one being most similar\n",
      "to student which is cool now if I were\n",
      "to do a different one say flower it's\n",
      "going to give me the the tree and the\n",
      "bird examples okay but I'm going to\n",
      "stick this with student and what I'm\n",
      "going to do is I'm going to take this\n",
      "prompter that we just made and I'm going\n",
      "to pass that into the language model and\n",
      "all of a sudden you get classroom the\n",
      "next thing we're going to look at is\n",
      "output parsers now that's kind of a\n",
      "complicated way to say\n",
      "um we need some structured output like\n",
      "we want the language model to return a\n",
      "Json object back to us why well because\n",
      "it makes it a heck of a lot easier to go\n",
      "deal with and work with on the other\n",
      "side\n",
      "there's two big Concepts when we talk\n",
      "about output parsers first it's going to\n",
      "be the formatting instructions piece so\n",
      "this is the prompt template that is\n",
      "going to tell your language model how to\n",
      "respond back to you and Lang chain\n",
      "provides us some conventions to do this\n",
      "automatically which is cool and then the\n",
      "second thing we're going to have is\n",
      "going to be the parser and so this is\n",
      "going to be the tool that is going to\n",
      "parse the output of your language model\n",
      "so the language model can only return\n",
      "back a string but if we want a Json\n",
      "object well we need to go and parse that\n",
      "string and extract the Json Json from\n",
      "that okay so we're going to get a\n",
      "structured output parsing and we're\n",
      "going to get the response schema from\n",
      "there let's import our language model\n",
      "again and we're going to have a response\n",
      "schema so in this case I just want it to\n",
      "be a two field Json object it I'm going\n",
      "to have a bad string which is a poorly\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video introduces the concept of semantic similarity example selection, which uses language models to match examples based on their semantic meaning. The process involves defining a list of examples and using an embedding engine to store their embeddings. The video also discusses the use of prompt templates for instructing language models and the importance of output parsers for obtaining structured output, such as JSON objects. The Lang chain framework provides conventions for automating prompt formatting and includes tools for parsing language model outputs.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "formatted user input string and then a\n",
      "good string this is your response a\n",
      "formatted response and so the really\n",
      "nice response from the um from the\n",
      "language model there and in this case\n",
      "I'm going to go ahead and create my\n",
      "output parser which is going to read the\n",
      "response schema and it's going to be\n",
      "able to parse it for us but we won't use\n",
      "that until just a second here\n",
      "so first thing we're going to have is\n",
      "our format instructions so on the output\n",
      "and parser we're going to say get format\n",
      "instructions and then let's print those\n",
      "out\n",
      "in fact I don't need to do that I could\n",
      "just print this out directly right here\n",
      "cool and so this is a piece of text that\n",
      "is going to be input or insert put into\n",
      "the prompt the output should be a\n",
      "markdown code snippet format it in the\n",
      "following schema Json and then the two\n",
      "fields that I input up above but it did\n",
      "the formatting for me or at least Lane\n",
      "chain did for him to put it into here\n",
      "so let's go ahead and create a prompt\n",
      "template we're going to do a placeholder\n",
      "variable for our format instructions and\n",
      "then we're also going to do a\n",
      "placeholder for user input this will be\n",
      "the poorly formatted string that the\n",
      "user is going to input and then finally\n",
      "I put your response here just to tell it\n",
      "it's like hey I'm done telling you\n",
      "instructions give me a response we go\n",
      "ahead and we get the prompt template we\n",
      "have the user input we have a partial\n",
      "variable of format instructions and this\n",
      "will be the format instructions we had\n",
      "up above we have our template which is\n",
      "the string up above here and then we\n",
      "have our prompt value so this will be\n",
      "the actual value that is filled out with\n",
      "the variables I tell it and I'm going to\n",
      "say welcome to California with an\n",
      "exclamation point let's go ahead and do\n",
      "that one and here I print out the final\n",
      "prompt that is going to be sent to the\n",
      "llm we have user input Welcome to\n",
      "California with everything we had up\n",
      "above let's go ahead and run this\n",
      "let's see what it responds back to us so\n",
      "we get a string here it kind of looks\n",
      "like gobbledygook but if we were to\n",
      "print this out it'd make more sense but\n",
      "before printing out let's just go ahead\n",
      "and parse this and now we can actually\n",
      "parse this and we get a nice uh Json\n",
      "object back well in this case it's going\n",
      "to be addict but um you can see here\n",
      "it's typed\n",
      "the next thing we're going to look at is\n",
      "different indexes so in this case we're\n",
      "going to be structuring documents in a\n",
      "way that language models have a better\n",
      "time working with them and one of the\n",
      "main ways that lanechain does this it's\n",
      "going to be through document loaders now\n",
      "this is very similar to the open AI\n",
      "plugins that just were released however\n",
      "there's a lot of support for a lot of\n",
      "really cool data sources in langchain\n",
      "that aren't yet supported within the\n",
      "plug-in World in this case I'm going to\n",
      "be doing a Hacker News data loader so\n",
      "all I'm doing is just passing a simple\n",
      "URL to this data loader I'm going to say\n",
      "hey go get me that data and so I'm\n",
      "asking hey how many pieces of data did\n",
      "you find\n",
      "uh and in this case it found 76\n",
      "different comments within this Hacker\n",
      "News Post and I asked it to print me out\n",
      "a sample and here we see uh one of the\n",
      "responses by the moderator dang within\n",
      "uh Hacker News and we see the response\n",
      "there we see different comments uh you\n",
      "can go and work with these within your\n",
      "language model now which is pretty cool\n",
      "another big piece of what we do a ton of\n",
      "is text splitting so oftentimes your\n",
      "document like your book or your essay or\n",
      "whatever is going to be too long for\n",
      "your language model you need to split it\n",
      "up into chunks and text Splitters will\n",
      "help with this now the reason why you do\n",
      "this is because if you want a single\n",
      "answer out of a book it wouldn't behoove\n",
      "you too much to input that entire book\n",
      "Into The Prompt one because it's too\n",
      "long but two is because the signal to\n",
      "noise ratio is too much or it's too\n",
      "little for your language model to\n",
      "effectively do its job it'd be a lot\n",
      "better if you just put in a few pieces\n",
      "of text into there and in order to get\n",
      "those few pieces of text we need to do\n",
      "splitting or chunking of those so in\n",
      "this case I'm going to do text splitting\n",
      "and the one that I use most often is\n",
      "going to be the recursive character text\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video discusses the concept of semantic similarity example selection using language models. It explains the process of defining examples, storing their embeddings using an embedding engine, and utilizing prompt templates to instruct language models. The importance of output parsers for obtaining structured output, such as JSON objects, is also highlighted. The Lang chain framework is introduced as a tool for automating prompt formatting and parsing language model outputs. The video further explores the use of output parsing and document loaders, such as Hacker News data loader, within the Lang chain framework. It also emphasizes the significance of text splitting or chunking for inputting smaller pieces of text into language models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "splitter there's a lot of different\n",
      "types of text Splitters depending on\n",
      "your use case I encourage you to go\n",
      "check those out and in this case I'm\n",
      "going to pull in a Paul Graham essay his\n",
      "worked essay this one is quite long it\n",
      "may be as long as actually so if I were\n",
      "to read his document\n",
      "I just have one big long document right\n",
      "now which means it's a really long piece\n",
      "of text but in this case what I want to\n",
      "do is I want to have the recursive\n",
      "character text splitter and I'm going to\n",
      "say chunk size equals 150. this means\n",
      "that I'm going to have a size of 150\n",
      "when I end up splitting my star document\n",
      "there and if you want chunk overlap that\n",
      "means that the Venn diagram of your docs\n",
      "is going to overlap just a little bit I\n",
      "encourage you to play with these\n",
      "variables to see which one works best\n",
      "for your use case normally I wouldn't do\n",
      "150 I'd probably do a thousand or two\n",
      "thousand but for demonstration purposes\n",
      "I'm doing 150. go ahead and run that and\n",
      "so we had one document up above but\n",
      "after I split it I now have 606\n",
      "documents all right and if I wanted to\n",
      "preview those I can go ahead and preview\n",
      "these and see how they're nice and small\n",
      "they're super small and if I wanted to\n",
      "make this 50 for example well then my\n",
      "chunks will be a whole lot smaller but\n",
      "let me go ahead and make that bigger the\n",
      "next thing we're going to look at is\n",
      "going to be retrievers now retrievers\n",
      "are easy ways to combine your documents\n",
      "with your language models there's going\n",
      "to be a lot of different types of\n",
      "retrievers and the most widely supported\n",
      "one is going to be the vector store\n",
      "Retriever and it's most widely supported\n",
      "because we're doing so much similarity\n",
      "search within embeddings let's look at\n",
      "an example here we're going to load up a\n",
      "hologram essay just like how we had\n",
      "beforehand I'm going to do some\n",
      "splitting of it and so we're going to\n",
      "get a whole bunch of documents\n",
      "we're going to split the documents and\n",
      "then I'm going to create embeddings out\n",
      "of those documents and so all those\n",
      "little chunks we're going to create\n",
      "vectors out of them which is the\n",
      "semantic meaning of them and then I'm\n",
      "going to store those vectors within a\n",
      "document store here okay and I'm going\n",
      "to call that within a my DB there and\n",
      "then I'm going to say hey this retriever\n",
      "is going to be the DB but we're going to\n",
      "set it as the retriever okay so it knows\n",
      "to go get stuff and if I were to look at\n",
      "this you can see here that we have our\n",
      "Vector store retriever that's output\n",
      "right here\n",
      "okay we're going to take our Retriever\n",
      "and I'm going to say hey go get me the\n",
      "relevant documents what types of things\n",
      "did the author want to build now in the\n",
      "background what it's doing here is it's\n",
      "taking the string and it's converting it\n",
      "to a vector it's taking that vector and\n",
      "it's going to go compare it to the\n",
      "vector store that you have and find the\n",
      "similar documents that come from there\n",
      "so what I'm going to do here is I'm just\n",
      "going to print out this is a one-liner\n",
      "kind of complicated one just to print\n",
      "out the preview of the documents that we\n",
      "have here I'm just going to have it\n",
      "print out the first two\n",
      "docs is not defined great let's go ahead\n",
      "and run those so all of a sudden these\n",
      "are the previews of the docs that it\n",
      "found there\n",
      "um what I wanted was to not just build\n",
      "things but build things that would last\n",
      "so you can see here that out of all\n",
      "those documents that I found it found\n",
      "the two that were most similar to what I\n",
      "was looking for which is really cool I\n",
      "wanted to build things nice next let's\n",
      "look at Vector stores so we briefly just\n",
      "talked about Vector stores right before\n",
      "this but to go into it a little bit\n",
      "further think of a vector store really\n",
      "the way that I think about it is a table\n",
      "with rows with your embeddings and\n",
      "Associated metadata that comes with it\n",
      "an example of it is right here two main\n",
      "players in the space are now are going\n",
      "to be pine cone and weeviate however if\n",
      "you want to you can go check out open\n",
      "ai's retriever documentation and they\n",
      "list a whole bunch of other ones that\n",
      "you may find awesome for you\n",
      "okay so let's go ahead and look at these\n",
      "again we're going to import our models\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video discusses the concept of semantic similarity example selection using language models and highlights the importance of output parsers. It introduces the Lang chain framework for automating prompt formatting and parsing language model outputs. The video also explores the use of output parsing and document loaders within the Lang chain framework. Additionally, text splitting or chunking for inputting smaller pieces of text into language models is emphasized. The video further introduces the concept of retrievers, specifically the vector store retriever, for combining documents with language models. It also mentions the significance of vector stores and provides examples of popular ones.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "we got our embeddings okay cool now with\n",
      "these embeddings I'm gonna look at that\n",
      "and based off of how I split my document\n",
      "up above with a thousand chunks or a\n",
      "thousand as a chunk size we get 78\n",
      "documents Auto programs worked essay\n",
      "okay what I'm going to do is I'm going\n",
      "to create those as embeddings and I'm\n",
      "going to get my embeddings list from\n",
      "there and I'm going to let's look at the\n",
      "length of the embedding list I have 78\n",
      "embeddings reason why is because I have\n",
      "one vector for each one of my documents\n",
      "so all right makes sense and here's a\n",
      "sample of one so here's an example of\n",
      "what the embedding would look like it's\n",
      "a numerical representation of the\n",
      "semantic meaning of your document there\n",
      "so your vector store is going to be\n",
      "storing your embeddings and it makes\n",
      "them easily searchable so in this case\n",
      "it is going to take my embedding and\n",
      "it's going to store it like a database\n",
      "the next topic I want to look at is\n",
      "going to be Memory so this is going to\n",
      "be how you help your language models\n",
      "remember things the most common use case\n",
      "for this is going to be your chat\n",
      "history so if you're making a chat bot\n",
      "then you can tell up the history\n",
      "messages that you've had beforehand\n",
      "which makes it a whole lot better at\n",
      "helping your user do whatever it needs\n",
      "to do so in this case I'm going to\n",
      "import chat message history and I'm\n",
      "going to import my chat open AI again\n",
      "and so I'm going to create my chat model\n",
      "and then I'm going to create my history\n",
      "model and to my history model I'm going\n",
      "to add an AI message Hi and then I'm\n",
      "going to add a user message what is the\n",
      "capital of France so let me go ahead and\n",
      "run that and if I were to take a look at\n",
      "my history messages I get my two that\n",
      "are input right there they're in the\n",
      "right order as we would expect there to\n",
      "be\n",
      "so what's cool is that I can pass my\n",
      "history of messages to the language\n",
      "model and so in this case it is going to\n",
      "read oh I said hi to start and then the\n",
      "human message was what's the capital of\n",
      "France and let's see what it responds\n",
      "back to us the capital France is Paris\n",
      "and it gives us an AI message which is\n",
      "cool and so what I want to do here is I\n",
      "want to add an AI message to my history\n",
      "which uh I shouldn't repeat this but I\n",
      "am actually no I'm not repeating it I'm\n",
      "taking the AI response and I'm just\n",
      "putting out the content let me print out\n",
      "those messages again and you can see\n",
      "here that it adds uh the capital Francis\n",
      "Paris to the end of my chat history\n",
      "which makes it easy for me to work with\n",
      "and another cool functionality of this\n",
      "too is link chain makes it extremely\n",
      "simple to save this chat history so you\n",
      "can go ahead and load it later a lot of\n",
      "really cool functionality I encourage\n",
      "you to go check out the next concept\n",
      "we're going to look at is chains so in\n",
      "this case we're going to be combining\n",
      "different llm calls and actions\n",
      "automatically so say you have one input\n",
      "but then the output of that language\n",
      "model you want to use as the input to\n",
      "another call and then another call and\n",
      "then another call well in that case\n",
      "you're going to be using chains which is\n",
      "where the chain and Lang chain comes\n",
      "from so in this case we're going to\n",
      "cover two of them there's a lot of\n",
      "really complicated examples here I\n",
      "encourage you again to go check out the\n",
      "documentation to see if one of them\n",
      "would cover your use case better than\n",
      "what you're seeing here the first one is\n",
      "going to be a simple sequential chain\n",
      "and in this case I'm going to go ahead\n",
      "and tell it hey I want you to do X and\n",
      "then Y and then Z now the reason why\n",
      "this is important\n",
      "or why I like to do it is because it\n",
      "helps break up the tasks Now language\n",
      "models can get distracted sometimes and\n",
      "if you ask it to do too many things in a\n",
      "row it could get it could get confused\n",
      "it could start to hallucinate and that's\n",
      "not good for anybody Plus\n",
      "I want to make sure that my thinking is\n",
      "sound and that way I can kind of check\n",
      "out the different outputs of each one of\n",
      "my different actions here so in this\n",
      "case I'm going to import the simple\n",
      "sequential chain let me go ahead and run\n",
      "this and I'm going to put two different\n",
      "things to here I'm going to use two\n",
      "different prompt templates so your job\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video explores the concept of semantic similarity example selection using language models and emphasizes the importance of output parsers. It introduces the Lang chain framework for automating prompt formatting and parsing language model outputs. The video also discusses the use of output parsing and document loaders within the Lang chain framework. Additionally, it highlights the significance of text splitting or chunking for inputting smaller pieces of text into language models. The video introduces the concept of retrievers, specifically the vector store retriever, for combining documents with language models. It also mentions the significance of vector stores and provides examples of popular ones. The new context provided discusses the use of embeddings and memory in language models, as well as the functionality of chains in combining different language model calls and actions automatically.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "is to come up with a classic dish from\n",
      "the area that the user suggests I'm\n",
      "going to input the user location and I'm\n",
      "going to give it the user location which\n",
      "we'll we'll do in a second here\n",
      "and I'm going to create a llm chain with\n",
      "this and I'm going to call it location\n",
      "chain which basically is going to take\n",
      "my language model it's going to take a\n",
      "prompt template okay\n",
      "and then the next one we're going to\n",
      "look at given a meal give a short and\n",
      "simple recipe on how to make that dish\n",
      "at home so in this case we have the user\n",
      "location which that's not actually what\n",
      "we want we want user meal output this\n",
      "wouldn't have mattered because I had the\n",
      "variables the same but it just to make\n",
      "it more clear\n",
      "given a meal okay cool your response I'm\n",
      "going to do the same thing I'm going to\n",
      "put that into a meal chain so what it's\n",
      "going to do is it's going to Output a\n",
      "meal a classic dish and then it's going\n",
      "to Output a simple recipe for that\n",
      "classic dish\n",
      "okay I'm gonna create my simple\n",
      "sequential chain and in this case I'm\n",
      "going to specify My Chains as my\n",
      "location chain and then the meal chain\n",
      "order matters be careful on that I'm\n",
      "going to set verbose equals true which\n",
      "means that it's going to tell us what\n",
      "it's thinking and it's actually going to\n",
      "print those statements out so it's\n",
      "easier to debug what's going on\n",
      "let's go ahead and create that and then\n",
      "I'm going to say My overall chain I want\n",
      "you to run and in this case I only have\n",
      "one input variable which is going to be\n",
      "Rome which is going to be the user\n",
      "location that I start in the first place\n",
      "let me go ahead and run this so you can\n",
      "see here that it's entering the new\n",
      "sequential chain and it ran Rome against\n",
      "the first prompt template and got me a\n",
      "classic dish which is really cool\n",
      "and then it gave me a recipe to on how\n",
      "to make that classic dish which is\n",
      "really cool so all of a sudden it just\n",
      "did two different runs for me all in one\n",
      "go and I didn't have to run any\n",
      "complicated code I could just use\n",
      "langtain for that it's pretty sweet\n",
      "now the next one that I want to show is\n",
      "one that I use quite often which is\n",
      "going to be the summarization chain the\n",
      "reason why this one was so cool is\n",
      "because if you have a long piece of text\n",
      "and you want it summarized or say you\n",
      "have an article you want summarized or a\n",
      "tweet thread or a Hacker News Post or\n",
      "whatever it may be you're going to want\n",
      "to Chunk Up Your longer piece of text\n",
      "and you're going to want to find you're\n",
      "going to want to find summaries of those\n",
      "different chunks and then get a final\n",
      "summary and in that case what we're\n",
      "going to do is we're going to load in\n",
      "load summarize chain and we're going to\n",
      "do Paul Graham's essay disk not even\n",
      "sure what that one's about then we're\n",
      "going to split it up into different\n",
      "texts right here the chunk size is going\n",
      "to be 700 and then I'm going to load\n",
      "summarize chain and the chain type that\n",
      "I'm going to do is going to be that one\n",
      "that I mentioned beforehand which is\n",
      "where you get the small summaries of the\n",
      "individual sections and then you get a\n",
      "summary of the small summaries I have a\n",
      "whole video on different chain types and\n",
      "so if you're curious go check out the\n",
      "video up above and you can go see it\n",
      "let me go ahead and run this and so as\n",
      "you can see here the language model is\n",
      "asking I'm sorry the chain or Lang chain\n",
      "is asking the language model to\n",
      "summarize this piece of text right here\n",
      "and then this piece of text right here\n",
      "because we only had two chunks that we\n",
      "wanted to summarize and then it's asking\n",
      "for a final concise summary so here's\n",
      "the summary of the chunk number one\n",
      "here's the summary of Chunk number two\n",
      "and it's asking for a summary of the\n",
      "summaries and we finally get a summary\n",
      "of the summaries which is really cool\n",
      "because all built into this one liner\n",
      "right here was all the different calls\n",
      "back and forth to figure out how to do\n",
      "the summary of the summaries which is\n",
      "one of the powers of Lane chain which is\n",
      "really sweet the last thing we're going\n",
      "to look at is agents and this is one of\n",
      "the most complicated Concepts within\n",
      "link chain which is why we're talking\n",
      "about it last here but I thought that\n",
      "the official link chain documentation\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video explores the concept of semantic similarity example selection using language models and emphasizes the importance of output parsers. It introduces the Lang chain framework for automating prompt formatting and parsing language model outputs. The video also discusses the use of output parsing and document loaders within the Lang chain framework. Additionally, it highlights the significance of text splitting or chunking for inputting smaller pieces of text into language models. The video introduces the concept of retrievers, specifically the vector store retriever, for combining documents with language models. It also mentions the significance of vector stores and provides examples of popular ones. The new context discusses the use of embeddings and memory in language models, as well as the functionality of chains in combining different language model calls and actions automatically. The video also demonstrates the use of the Lang chain framework for creating classic dishes based on user location and providing simple recipes. It showcases the summarization chain, which allows for the summarization of long pieces of text by chunking them and generating summaries of each chunk before producing a final concise summary. The video concludes by briefly mentioning the concept of agents within the Lang chain framework.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "did a great job describing what agents\n",
      "are\n",
      "some applications will not require just\n",
      "a predetermined chain of calls to llms\n",
      "and other tools what we did up above was\n",
      "a predetermined chain here but\n",
      "potentially an unknown chain that\n",
      "depends on the user input an unknown\n",
      "chain emphasis mine means that we're not\n",
      "really sure what route we want to take\n",
      "but we want the language model to tell\n",
      "us which route it thinks that it should\n",
      "take\n",
      "in these types of chains there is an\n",
      "agent which has an access to a suite of\n",
      "tools depending on the user input the\n",
      "agent can then decide which if any of\n",
      "these tools to call\n",
      "so for example hey you have two\n",
      "databases you could pick information\n",
      "from they're on completely different\n",
      "topics the user just asked you a\n",
      "question about uh trees\n",
      "which database should you go looking to\n",
      "find your tree information well an agent\n",
      "can decide that which is really sweet so\n",
      "I'm going to go over the vocabulary\n",
      "first and then we're going to look at an\n",
      "example so an agent is the language\n",
      "model that is going to be driving the\n",
      "decision making cool\n",
      "tools or tool is going to be a\n",
      "capability of the agent so you can think\n",
      "of this as similar to the open AI\n",
      "plugins that just came out you can also\n",
      "think of this as the ability to go\n",
      "search Google the ability to go lick\n",
      "your email whatever it may be\n",
      "a tool kit is going to be a collection\n",
      "of tools so an agent will have a toolkit\n",
      "of tools uh\n",
      "an agent will have a toolkit of tools\n",
      "and that's what that's what it's going\n",
      "to do there I'm going to import load\n",
      "tools I'm going to initialize the agent\n",
      "I'm going to import openai as well\n",
      "with that I'm going to create my\n",
      "language model now I've made I've insert\n",
      "my serp API key because that's the\n",
      "example that we're going to be running\n",
      "through here which is an easy way to\n",
      "search Google\n",
      "and then with the toolkit I'm going to\n",
      "go ahead and load the tools now in this\n",
      "case I'm only loading one tool and it's\n",
      "the server API however you could load in\n",
      "a lot of tools here and you may\n",
      "naturally think well let me just load it\n",
      "up with all the tools in the world you\n",
      "could it's just going to get difficult\n",
      "for the model or the agent to know which\n",
      "tool to use at which time so you kind of\n",
      "only want to use the ones that you know\n",
      "you're going to um\n",
      "be needing at that at that point so I'm\n",
      "going to pass in my language model and\n",
      "I'm going to pass in my serve API here\n",
      "API key then I'm going to create my\n",
      "agent so I'm going to pass in the\n",
      "toolkit that I just made I'm going to\n",
      "pass in the language model again I'm\n",
      "going to say what type of agent are you\n",
      "now there's different agent types for\n",
      "different types of tasks and I encourage\n",
      "you to go check out the language or the\n",
      "documentation to see which would be best\n",
      "for you I'm going to say verbose equals\n",
      "true so we can see it thinking I'm also\n",
      "going to return the intermediate steps\n",
      "which just means that we get more\n",
      "granularity into what it's actually\n",
      "doing\n",
      "with this I'm going to say response uh\n",
      "oh agent is not defined then what I'm\n",
      "going to do here is I'm going to pass in\n",
      "my query to the agent itself so what was\n",
      "the first album of the band that Natalie\n",
      "Bergman is a part of the reason why I\n",
      "asked this question specifically is\n",
      "because keep in mind I haven't uploaded\n",
      "any documents here so there's no\n",
      "information pre-loaded and it's kind of\n",
      "a complicated question that has multiple\n",
      "steps that need to be answered for it\n",
      "this is a perfect question for an agent\n",
      "here so let's go ahead and run this and\n",
      "let's see how the agent is thinking\n",
      "about it entering the new agent executor\n",
      "class and it said I should try to find\n",
      "out what band Natalie Bergman is a part\n",
      "of so it needs to it knows that it needs\n",
      "to go search which it has a Search tool\n",
      "up above which I gave it and it's saying\n",
      "Natalie Bergman banded so it's searching\n",
      "for that one and it says observation\n",
      "which is what it observed from its\n",
      "action Natalie Bergman is an American\n",
      "singer-songwriter she has one half the\n",
      "duo of wild Belle okay cool I should\n",
      "search for the debut album of wild Belle\n",
      "it understood the band that she's a part\n",
      "of and now it now it knows and needs to\n",
      "go search for that band so it's going to\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The video explores the concept of semantic similarity example selection using language models and emphasizes the importance of output parsers. It introduces the Lang chain framework for automating prompt formatting and parsing language model outputs. The video also discusses the use of output parsing and document loaders within the Lang chain framework. Additionally, it highlights the significance of text splitting or chunking for inputting smaller pieces of text into language models. The video introduces the concept of retrievers, specifically the vector store retriever, for combining documents with language models. It also mentions the significance of vector stores and provides examples of popular ones. The new context discusses the use of embeddings and memory in language models, as well as the functionality of chains in combining different language model calls and actions automatically. The video demonstrates the use of the Lang chain framework for creating classic dishes based on user location and providing simple recipes. It showcases the summarization chain, which allows for the summarization of long pieces of text by chunking them and generating summaries of each chunk before producing a final concise summary. The video concludes by discussing the concept of agents within the Lang chain framework and their role in determining the tools to be used based on user input.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "search again it's going to say wild\n",
      "Belle debut album and it observes that\n",
      "the debut album is Isles I know the\n",
      "final answer which is good we want it to\n",
      "know the finance\n",
      "uh Isles is the debut album of wild\n",
      "Belle the band that Natalie Bergman is a\n",
      "part of that is really cool because that\n",
      "is a multi-step question and uh the\n",
      "agent knew what it needed to go find out\n",
      "without me telling it the chain so this\n",
      "chain could have been a whole lot longer\n",
      "if it needed more steps with it but uh\n",
      "it dynamically figured that out along\n",
      "the way which is really really cool and\n",
      "so if we were to print out the\n",
      "intermediate steps you get more\n",
      "information about what it actually did\n",
      "and how it searched and all that good\n",
      "information from there\n",
      "um and if we were to confirm this let's\n",
      "go ahead and run this WOW yep wild Belle\n",
      "there's Natalie Bergman brother and\n",
      "sister Duo band\n",
      "um beautiful I would play their song If\n",
      "it wasn't going to give me copyright\n",
      "trouble but I encourage you to go look\n",
      "it up link to my favorite song of theirs\n",
      "is in the description well my friends\n",
      "that was a very broad overview of all of\n",
      "the nuts and bolts of lion chain the\n",
      "Tactical nuts and bolts I congratulate\n",
      "you for making it to the end of this\n",
      "video and if you have any questions\n",
      "please let me know I encourage you to\n",
      "subscribe to check out for part two when\n",
      "we go through actual use cases for these\n",
      "nuts and bolts and again I share a lot\n",
      "of tools on Twitter so I encourage you\n",
      "to follow me there like always please\n",
      "leave comments let me know what you\n",
      "think of the video and let me know if\n",
      "you have any questions we'll see you\n",
      "later\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video explores the concept of semantic similarity example selection using language models and emphasizes the importance of output parsers. It introduces the Lang chain framework for automating prompt formatting and parsing language model outputs. The video also discusses the use of output parsing and document loaders within the Lang chain framework. Additionally, it highlights the significance of text splitting or chunking for inputting smaller pieces of text into language models. The video introduces the concept of retrievers, specifically the vector store retriever, for combining documents with language models. It also mentions the significance of vector stores and provides examples of popular ones. The new context discusses the use of embeddings and memory in language models, as well as the functionality of chains in combining different language model calls and actions automatically. The video demonstrates the use of the Lang chain framework for creating classic dishes based on user location and providing simple recipes. It showcases the summarization chain, which allows for the summarization of long pieces of text by chunking them and generating summaries of each chunk before producing a final concise summary. The video concludes by discussing the concept of agents within the Lang chain framework and their role in determining the tools to be used based on user input. The video also mentions the debut album \"Isles\" by the band Wild Belle, which is part of the multi-step question asked to the agent. The agent dynamically figured out the necessary steps to find the answer without explicit instructions. The video encourages viewers to explore the Lang chain framework and stay tuned for part two, which will cover actual use cases.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll implement my own text splitter and summarizer using the refine method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_text_splitter(text,chunk_size=3000):\n",
    "    # Split text into chunks based on space or newline\n",
    "    chunks = text.split()\n",
    "\n",
    "    # Initialize variables\n",
    "    result = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Concatenate chunks until the total length is less than 4096 tokens\n",
    "    for chunk in chunks:\n",
    "        # if len(current_chunk) + len(chunk) < 4096:\n",
    "        if num_tokens(current_chunk+chunk) < chunk_size:\n",
    "            current_chunk += \" \" + chunk if current_chunk else chunk\n",
    "        else:\n",
    "            result.append(current_chunk.strip())\n",
    "            current_chunk = chunk\n",
    "    if current_chunk:\n",
    "        result.append(current_chunk.strip())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello good people have you ever wondered what Lang chain was or maybe you've heard about it and you've played around with a few sections but you're not quite sure where to look next well in this video we're going to be covering all of the lane chain Basics with the goal of getting you building and having fun as quick as possible my name is Greg and I've been having a ton of fun building out apps in langchain now I share most of my work on Twitter so if you want to go check it out links in the description you can go follow along with me now this video is going to be based off of the new conceptual docs from lanechain and the reason why I'm doing a video here is because it takes all the technical pieces and abstracts them up into more theoretical qualitative aspects of Lane chain which I think is extremely helpful for it and in order to understand this a little bit better I've created a companion for this video and that is the Lang chain cookbook links in the description if you want to go check that out please go and check out the GitHub and you can follow along here I'm gonna put a lot of time stamps in the description as well there's gonna be a fair amount of content in this one so you can watch it all the way through or if you want to skip to a certain section feel free to jump to that time stamp all right without further Ado let's jump into it all right here are the new conceptual docs from Lang chain now the reason why these are different is because there are the python docs which are going to be the more technical focused one or the JavaScript docs as well which is also more technical documentation however these concepts are more qualitative so you can understand what is going on in the background of these different sections here now we're going to focus on these components of Lang chain there's an entire section on use cases which is when you actually put these into practice and that is going to be a part two of this video so we won't jump into this today that would be a too long for us we're going to run through schema models prompts indexes indexes memory chains and agents with a working code sample for each one of those well without further Ado let's jump into some code here we are with the Lang chain cookbook now my goal is to make this a dense document with a ton of links so you can go and self-service right into their links in the description and if you want to follow along I encourage you to get this on your computer and go from it go for it from there so the goal of this dock is to provide an introductory understanding of the components and use cases of Lang chain in a explain like M5 way with examples and code Snippets for use cases check out part two which is not made yet that is coming soon hopefully by the time you see this it will be a bunch of links here we go into what is Lang chain so Lang chain is going to be a framework for developing applications powered by language models well Greg openai just came out with plugins yes but there is a whole lot of other things that you can do with language models outside of those and Lang chain helps abstract a ton of that so that you're able to work with it more easily and intermix different pieces and customize really how you need to so Lane chain makes the complicated parts of working and building with AI models easier it does this in two main ways the first big way is going to be through integration so you can bring external data such as your files other applications API data to your language models which is cool the other big way that it helps do this is through agency so it allows your language models to interact with its environment via decision making basically you're using the language model to help decide which action to take next and you do this when the path isn't so clear or it may be unknown and we'll get into more more of that later so why link chain specifically there are four big reasons why I like Lane chain the first one is going to be for the components Lang chain makes it easy to swap out abstractions and components necessary to work with language models basically they've created a ton of tools that make it super simple to work with language models like chat GPT or anything on how you face how you may want also because it allows you to customize chains really easily so there's a ton of out of the box of support for using and customizing chains basically combining series of actions together on the qualitative side of why LinkedIn is awesome is because the speed is great almost every day I need to go and make sure that I'm on the latest branch of Lang Chan and I go and I update it every time so the speed is awesome the other really cool part is the community so there's a ton of meetups there's a Discord Channel and there's a ton of events like webinars that go on throughout the week that are really awesome learning resources for us cool now again to summarize all this why do we need Lang chain well because language models can be pretty straightforward it is text in text app and you may have experienced this yourself however once you just start developing applications there's a ton of friction points that Lang chain is going to help you develop uh there's a ton of friction points that they're going to help you with basically now the last thing that I'll say about this before we jump into it is that this cookbook isn't going to cover all of the aspects of Lang chain this isn't meant to be a replacement for the documentation online this is meant to show you a very broad overview about the capabilities that there are with my interpretation of them and my voice over with it and with that I'm hoping that you can get to building and impact as quick as possible I'm super curious to see what you build so please let me know and uh I will hopefully uh I would love to see it first thing we're going to do is we're going to import our openai API key now I have a hidden cell here but you're going to replace your API key API key right here just throw that in there the first aspect of link chain components that we're going to look at is the schema now I almost didn't even include this one but the first one is going to be text now what's really cool about these language models is that text is the new programming language not verbatim not per se but we're using a lot more English language to tell language models what to do in this case what day comes after Friday is an example of something I may go tell a language model and it is going to respond back to me with a natural language response very cool next up is going to be chat messages so like text chat messages are similar but they have different types the first type is going to be system and this is helpful background context that tell the AI what to do all right like your helpful teacher assistant bot or something then we have human messages and these are messages that are intended to represent the user and so literally user input or something that I may text from it then we have ai messages and these are messages that show what the AI responded with and the cool part about this is the AI may or may not have actually responded with it but you can tell it that it did so that it has additional context on how to answer you okay so what I'm going to do here is I'm going to import chat open Ai and my three message types and then I'm going to create my chat model I'm going to do that and then I'm going to type in two messages the first system message is you are a nice AI bot that helps a user figure out what to eat in a short sentence and then a human message I like tomatoes what should I eat let me go ahead and run this and you get an AI message back because this is what it responds with you could try making a tomato salad with fresh basil and mozzarella cheese thanks AI That's cool what you can also do is you can also pass more chat history and get responses from the AI so in this case you're a nice AI bot that helps a user figure out where to travel to in one short sentence I'm saying I like the beaches where should I go I'm telling it that it responded to me it didn't actually do this but I'm telling it that it did you should go to Nice France cool what else should I do when when I'm there and so the reason why I did this one is because you'll notice that I didn't say where I went it's going to have to infer from the history on what where I went and it says wow and nice and so it picked up where I was because it gets the history of the chat messages now if you're making a chatbot you could see how you could append different messages that have been back and forth uh I'm not sure if that's a verb but back and forth through the user okay the next model that we're the next model that we're going to look at is going to be documents so documents are important because this represents a piece of text along with Associated metadata now metadata is just a fancy word for things about that document and in this case this document or the text is held within a field called page content so this is my document it's full text that I've gathered from other places awesome and then I'm going to pass in some metadata and this metadata is a dictionary of key value pairs my document ID which is my key here and then some random document ID here that happens to be an INT it could be whatever you want it to be my document Source this is the Lang chain papers and then my document create time is going to be some timestamp whatever you want it to be and this is going to be can be whatever format you want this is extremely helpful for when you're making a large repositories of information and you want to be able to filter by it so instead of just going and asking link chain to look at all your documents in your database you can go ahead and filter these by a certain metadata go ahead and run this and you can see here I get a document object with a bunch of metadata on it from there cool if those are the schemas that we work with the next thing we're going to look at is the different models now these are the ways of interacting with well different models um but the reason why this is important is because they're different model types let me just show an example here the normal one that we're looking at is going to be the language model and this is when text goes in and text comes out okay now the first thing I'll do is I'll import open Ai and I'll make my model and you'll notice here that I changed my model in case you ever want to change your model as well and so I'm going to pass in a regular string into this one into my language model what day comes after Friday go ahead and run this and I get Saturday comes out the other end but not all models are like this you actually have chat models as well and we looked at this in the previous example but I didn't call it out specifically so for this one I'm going to import chat open AI I'm going to import my messages again I'm going to put temperature equals one which means the model is going to get a little spicy on me no but really it just means it's going to have more creativity and it's it's a little bit more exaggerated and so in this case I'm going to say you are an unhelpful AI bot that makes jokes at whatever the user says and in this case the user says I would like to go to New York how should I do this I'm going to go ahead and run this model you could try walking but I don't recommend it unless you have a lot of time on your hands maybe try flapping your arms really hard to see if it can fly there so as you can see it took that system message and it understood those directions and it uh it wasn't very helpful for me well because I told her not to be very helpful the last type of model that we're going to look at is going to be your text and betting model the reason why this one is important is because we do a lot of similarity searches and a lot of comparing texts when working with language models now in this case openai also has an AI embeddings model that we're going to use there's a lot of embedding models out there you can use whatever you want I just use open AI because it feels like it's a standard and it's very simple right now so I'm going to pass in my API key I'm going to get my embeddings engine ready and then I'm going to define a piece of text hi it's time for the beach let me go ahead and do that text and what I'm going to do is I'm going to pass that text and I'm going to embed that text so what that means is is it's going to take this string which is just a series of letters and it's going to convert it into a vector and in this case a vector is just simply a one-dimensional array meaning a list of numbers and that'll be a semantic representation of that text that's a fancy way of saying is that meaning of that text is going to be embedded in those numbers right there which makes it really easy to compare across other as others as well so I'm going to put that in a variable called text embeddings I'm going to see how long my text embeddings is and I'm going to get a preview of it so you'll notice here that my text embedding length is 1536 this means that there are 1536 different numbers within that list that represent the meaning of my text that's a lot of numbers and I'm glad I don't have to deal with them I'm glad the computer can so here's a sample of what those look like in case you're curious I only show the first uh five here but I put a dot dot dots you know that there are 1531 other numbers out there next let's look at prompts so prompts are going to be the text that you send over to your language model we've already sent some prompts over to the language model but they've been pretty simple in this case we're going to start doing more instructional prompts and passing those to our model so again a prompt is what we pass to our language model I'm going to import open AI in this case I'm using DaVinci as my model and I'm going to say prompt equals this string now I use three three double quotes because um well I think it looks fancier no but really it's just easier to use which is why I like it in this case I'm not doing anything fancy and I could have passed this string right within my language model but in this case I made a variable for it because it's a little bit easier to understand so today's Monday tomorrow's Wednesday what is wrong with the statement the statement is incorrect tomorrow's Tuesday not Wednesday so you can see how it picked\",\n",
       " \"it up from there now why prompts are cool is because we start to get into the prompt template world the reason why prompt templates are important is because most of the time you're going to be dynamically generating your prompts meaning they won't just be static strings that you type out but you're actually going to be inputting tokens or inputting placeholders based off of the scenario that we're you're working with so in this case what I'm doing here is I'm importing my packages again in this case prompt template is going to be the new one I'm going to do DaVinci again okay great and in this case I'm going to create a template to start so I really want to travel to location you'll notice my opened and closed brackets around location which means that this is going to be a token that I'm going to be replacing later what should I do there respond in one short sentence because we're also just responded too much I'm going to create a prompt template in this case I'm going to put it in this variable prompt my input variable is going to be location which matches the same name that we had up here and then the template is this this whole thing that I had here the final prompt is going to be prompt.format which means going to insert the values I tell you go and insert the value Rome into where it says location right here let's go ahead and run this so final prompt I really want to travel to Rome which replace location up above and here we have our prompt template that's finally filled out and then in terms of the output it tells me what I should do so it took that information in with Rome and responded one short sentence it gives me this which is cool all right the next cool part that we're going to look at is the example selectors so often when you're constructing your prompts you're going to do something called in context learning this means that you're going to show you're going to show the language model what you want it to do and one of the main ways that people do this is through examples this could be about how to answer a customer service request or it could be how to respond to some nuanced question and in this case I'm going to pick examples however we have example selectors because say you had 10 000 different examples you don't want to throw all those into your uh into your prompt they may not fit and they may not be as relevant so you want to select which ones you want and in this case what I'm going to do is I'm going to import a lot of things here but the one I'm the main star of the show is going to be the semantic similarity example selector that's a long name for a functionality that's going to select similar examples so I'm going to get my language model going again I'm going to get my example prompt and this is just a prompt template like we saw up above and then I'm going to define a list of different examples so in this case I want to name a noun and then I want the language model to tell me where this noun is usually found so in this case a pirate on a ship a pilot on a plane driver in a car a tree oh that's not true a tree in the ground or a bird in a nest so I'll go ahead and run that one and then what we're going to do is we're going to get our example selector ready so we have our similar example selector we're going to pass it the list of examples that I just defined above but then we're also going to pass it our embedding engine and the reason why we do this is because we're actually going to match examples on their semantic meaning so not just matching them off of similar strings but off of what they actually mean so in this case we're going to use the open AI embeddings which is one of the models that has been shared by Facebook which is really cool and this is going to help store our embeddings and then we're going to tell it um how many we want how many examples we want back in this case I want k equals two let me go ahead and run that and then we're going to have a new prompt template here and this is going to be the few shot prompt template meaning the few shot part means that there's going to be a few examples in there for us so we give it our example selector we give our example prompt which we made up above and then we're going to add on just some little strings before and after to make it easier for the model so give the location that an item is usually found in cool and then the suffix will be the input and the output that we have from here based off of what the user inputs then the input variable go ahead and do that so here I'm going to say my noun is student So based off of the noun of student it's going to go and find me the examples up above that are most closely related to student and we're going to use those examples so if I would go ahead and do this I'm going to say print and it's going to print me the prompt that we're actually going to use within or give to our language model in this case it found the driver and it found the pilot one being most similar to student which is cool now if I were to do a different one say flower it's going to give me the the tree and the bird examples okay but I'm going to stick this with student and what I'm going to do is I'm going to take this prompter that we just made and I'm going to pass that into the language model and all of a sudden you get classroom the next thing we're going to look at is output parsers now that's kind of a complicated way to say um we need some structured output like we want the language model to return a Json object back to us why well because it makes it a heck of a lot easier to go deal with and work with on the other side there's two big Concepts when we talk about output parsers first it's going to be the formatting instructions piece so this is the prompt template that is going to tell your language model how to respond back to you and Lang chain provides us some conventions to do this automatically which is cool and then the second thing we're going to have is going to be the parser and so this is going to be the tool that is going to parse the output of your language model so the language model can only return back a string but if we want a Json object well we need to go and parse that string and extract the Json Json from that okay so we're going to get a structured output parsing and we're going to get the response schema from there let's import our language model again and we're going to have a response schema so in this case I just want it to be a two field Json object it I'm going to have a bad string which is a poorly formatted user input string and then a good string this is your response a formatted response and so the really nice response from the um from the language model there and in this case I'm going to go ahead and create my output parser which is going to read the response schema and it's going to be able to parse it for us but we won't use that until just a second here so first thing we're going to have is our format instructions so on the output and parser we're going to say get format instructions and then let's print those out in fact I don't need to do that I could just print this out directly right here cool and so this is a piece of text that is going to be input or insert put into the prompt the output should be a markdown code snippet format it in the following schema Json and then the two fields that I input up above but it did the formatting for me or at least Lane chain did for him to put it into here so let's go ahead and create a prompt template we're going to do a placeholder variable for our format instructions and then we're also going to do a placeholder for user input this will be the poorly formatted string that the user is going to input and then finally I put your response here just to tell it it's like hey I'm done telling you instructions give me a response we go ahead and we get the prompt template we have the user input we have a partial variable of format instructions and this will be the format instructions we had up above we have our template which is the string up above here and then we have our prompt value so this will be the actual value that is filled out with the variables I tell it and I'm going to say welcome to California with an exclamation point let's go ahead and do that one and here I print out the final prompt that is going to be sent to the llm we have user input Welcome to California with everything we had up above let's go ahead and run this let's see what it responds back to us so we get a string here it kind of looks like gobbledygook but if we were to print this out it'd make more sense but before printing out let's just go ahead and parse this and now we can actually parse this and we get a nice uh Json object back well in this case it's going to be addict but um you can see here it's typed the next thing we're going to look at is different indexes so in this case we're going to be structuring documents in a way that language models have a better time working with them and one of the main ways that lanechain does this it's going to be through document loaders now this is very similar to the open AI plugins that just were released however there's a lot of support for a lot of really cool data sources in langchain that aren't yet supported within the plug-in World in this case I'm going to be doing a Hacker News data loader so all I'm doing is just passing a simple URL to this data loader I'm going to say hey go get me that data and so I'm asking hey how many pieces of data did you find uh and in this case it found 76 different comments within this Hacker News Post and I asked it to print me out a sample and here we see uh one of the responses by the moderator dang within uh Hacker News and we see the response there we see different comments uh you can go and work with these within your language model now which is pretty cool another big piece of what we do a ton of is text splitting so oftentimes your document like your book or your essay or whatever is going to be too long for your language model you need to split it up into chunks and text Splitters will help with this now the reason why you do this is because if you want a single answer out of a book it wouldn't behoove you too much to input that entire book Into The Prompt one because it's too long but two is because the signal to noise ratio is too much or it's too little for your language model to effectively do its job it'd be a lot better if you just put in a few pieces of text into there and in order to get those few pieces of text we need to do splitting or chunking of those so in this case I'm going to do text splitting and the one that I use most often is going to be the recursive character text splitter there's a lot of different types of text Splitters depending on your use case I encourage you to go check those out and in this case I'm going to pull in a Paul Graham essay his worked essay this one is quite long it may be as long as actually so if I were to read his document I just have one big long document right now which means it's a really long piece of text but in this case what I want to do is I want to have the recursive character text splitter and I'm going to say chunk size equals 150. this means that I'm going to have a size of 150 when I end up splitting my star document there and if you want chunk overlap that means that the Venn diagram of your docs is going to overlap just a little bit I encourage you to play with these variables to see which one works best for your use case normally I wouldn't do 150 I'd probably do a thousand or two thousand but for demonstration purposes I'm doing 150. go ahead and run that and so we had one document up above but after I split it I now have 606 documents all right and if I wanted to preview those I can go ahead and preview these and see how they're nice and small they're super small and if I wanted to make this 50 for example well then my chunks will be a whole lot smaller but let me go ahead and make that bigger the next thing we're going to look at is going to be retrievers now retrievers are easy ways to combine your documents with your language models there's going to be a lot of different types of retrievers and the most widely supported one is going to be the vector store Retriever and it's most widely supported because we're doing so much similarity search within embeddings let's look at an example here we're going to load up a hologram essay just like how we had beforehand I'm going to do some splitting of it and so we're going to get a whole bunch of documents we're going to split the documents and then I'm going to create embeddings out of those documents and so all those little chunks we're going to create vectors out of them which is the semantic meaning of them and then I'm going to store those vectors within a document store here okay and I'm going to call that within a my DB there and then I'm going to say hey this retriever is going to be the DB but we're going to set it as the retriever okay so it knows to go get stuff and if I were to look at this you can see here that we have our Vector store retriever that's output right here okay we're going to take our Retriever and I'm going to say hey go get me the relevant documents what types of things did the author want to build now in the background what it's doing here is it's taking the string and it's converting it to a vector it's taking that vector and it's going to go compare it to the vector store that you have and find the similar documents that come from there so what I'm going to do here is I'm just going to print out this is a one-liner kind of complicated one just to print out the preview of the documents that we have here I'm just going to have it print out the first two docs is not defined great let's go ahead and run those so all of a sudden these are the previews of the docs that it found there um what I wanted was to not just build things but build things that would last so you can see here that out of all those documents that I found it found the two that were most similar to what\",\n",
       " \"I was looking for which is really cool I wanted to build things nice next let's look at Vector stores so we briefly just talked about Vector stores right before this but to go into it a little bit further think of a vector store really the way that I think about it is a table with rows with your embeddings and Associated metadata that comes with it an example of it is right here two main players in the space are now are going to be pine cone and weeviate however if you want to you can go check out open ai's retriever documentation and they list a whole bunch of other ones that you may find awesome for you okay so let's go ahead and look at these again we're going to import our models we got our embeddings okay cool now with these embeddings I'm gonna look at that and based off of how I split my document up above with a thousand chunks or a thousand as a chunk size we get 78 documents Auto programs worked essay okay what I'm going to do is I'm going to create those as embeddings and I'm going to get my embeddings list from there and I'm going to let's look at the length of the embedding list I have 78 embeddings reason why is because I have one vector for each one of my documents so all right makes sense and here's a sample of one so here's an example of what the embedding would look like it's a numerical representation of the semantic meaning of your document there so your vector store is going to be storing your embeddings and it makes them easily searchable so in this case it is going to take my embedding and it's going to store it like a database the next topic I want to look at is going to be Memory so this is going to be how you help your language models remember things the most common use case for this is going to be your chat history so if you're making a chat bot then you can tell up the history messages that you've had beforehand which makes it a whole lot better at helping your user do whatever it needs to do so in this case I'm going to import chat message history and I'm going to import my chat open AI again and so I'm going to create my chat model and then I'm going to create my history model and to my history model I'm going to add an AI message Hi and then I'm going to add a user message what is the capital of France so let me go ahead and run that and if I were to take a look at my history messages I get my two that are input right there they're in the right order as we would expect there to be so what's cool is that I can pass my history of messages to the language model and so in this case it is going to read oh I said hi to start and then the human message was what's the capital of France and let's see what it responds back to us the capital France is Paris and it gives us an AI message which is cool and so what I want to do here is I want to add an AI message to my history which uh I shouldn't repeat this but I am actually no I'm not repeating it I'm taking the AI response and I'm just putting out the content let me print out those messages again and you can see here that it adds uh the capital Francis Paris to the end of my chat history which makes it easy for me to work with and another cool functionality of this too is link chain makes it extremely simple to save this chat history so you can go ahead and load it later a lot of really cool functionality I encourage you to go check out the next concept we're going to look at is chains so in this case we're going to be combining different llm calls and actions automatically so say you have one input but then the output of that language model you want to use as the input to another call and then another call and then another call well in that case you're going to be using chains which is where the chain and Lang chain comes from so in this case we're going to cover two of them there's a lot of really complicated examples here I encourage you again to go check out the documentation to see if one of them would cover your use case better than what you're seeing here the first one is going to be a simple sequential chain and in this case I'm going to go ahead and tell it hey I want you to do X and then Y and then Z now the reason why this is important or why I like to do it is because it helps break up the tasks Now language models can get distracted sometimes and if you ask it to do too many things in a row it could get it could get confused it could start to hallucinate and that's not good for anybody Plus I want to make sure that my thinking is sound and that way I can kind of check out the different outputs of each one of my different actions here so in this case I'm going to import the simple sequential chain let me go ahead and run this and I'm going to put two different things to here I'm going to use two different prompt templates so your job is to come up with a classic dish from the area that the user suggests I'm going to input the user location and I'm going to give it the user location which we'll we'll do in a second here and I'm going to create a llm chain with this and I'm going to call it location chain which basically is going to take my language model it's going to take a prompt template okay and then the next one we're going to look at given a meal give a short and simple recipe on how to make that dish at home so in this case we have the user location which that's not actually what we want we want user meal output this wouldn't have mattered because I had the variables the same but it just to make it more clear given a meal okay cool your response I'm going to do the same thing I'm going to put that into a meal chain so what it's going to do is it's going to Output a meal a classic dish and then it's going to Output a simple recipe for that classic dish okay I'm gonna create my simple sequential chain and in this case I'm going to specify My Chains as my location chain and then the meal chain order matters be careful on that I'm going to set verbose equals true which means that it's going to tell us what it's thinking and it's actually going to print those statements out so it's easier to debug what's going on let's go ahead and create that and then I'm going to say My overall chain I want you to run and in this case I only have one input variable which is going to be Rome which is going to be the user location that I start in the first place let me go ahead and run this so you can see here that it's entering the new sequential chain and it ran Rome against the first prompt template and got me a classic dish which is really cool and then it gave me a recipe to on how to make that classic dish which is really cool so all of a sudden it just did two different runs for me all in one go and I didn't have to run any complicated code I could just use langtain for that it's pretty sweet now the next one that I want to show is one that I use quite often which is going to be the summarization chain the reason why this one was so cool is because if you have a long piece of text and you want it summarized or say you have an article you want summarized or a tweet thread or a Hacker News Post or whatever it may be you're going to want to Chunk Up Your longer piece of text and you're going to want to find you're going to want to find summaries of those different chunks and then get a final summary and in that case what we're going to do is we're going to load in load summarize chain and we're going to do Paul Graham's essay disk not even sure what that one's about then we're going to split it up into different texts right here the chunk size is going to be 700 and then I'm going to load summarize chain and the chain type that I'm going to do is going to be that one that I mentioned beforehand which is where you get the small summaries of the individual sections and then you get a summary of the small summaries I have a whole video on different chain types and so if you're curious go check out the video up above and you can go see it let me go ahead and run this and so as you can see here the language model is asking I'm sorry the chain or Lang chain is asking the language model to summarize this piece of text right here and then this piece of text right here because we only had two chunks that we wanted to summarize and then it's asking for a final concise summary so here's the summary of the chunk number one here's the summary of Chunk number two and it's asking for a summary of the summaries and we finally get a summary of the summaries which is really cool because all built into this one liner right here was all the different calls back and forth to figure out how to do the summary of the summaries which is one of the powers of Lane chain which is really sweet the last thing we're going to look at is agents and this is one of the most complicated Concepts within link chain which is why we're talking about it last here but I thought that the official link chain documentation did a great job describing what agents are some applications will not require just a predetermined chain of calls to llms and other tools what we did up above was a predetermined chain here but potentially an unknown chain that depends on the user input an unknown chain emphasis mine means that we're not really sure what route we want to take but we want the language model to tell us which route it thinks that it should take in these types of chains there is an agent which has an access to a suite of tools depending on the user input the agent can then decide which if any of these tools to call so for example hey you have two databases you could pick information from they're on completely different topics the user just asked you a question about uh trees which database should you go looking to find your tree information well an agent can decide that which is really sweet so I'm going to go over the vocabulary first and then we're going to look at an example so an agent is the language model that is going to be driving the decision making cool tools or tool is going to be a capability of the agent so you can think of this as similar to the open AI plugins that just came out you can also think of this as the ability to go search Google the ability to go lick your email whatever it may be a tool kit is going to be a collection of tools so an agent will have a toolkit of tools uh an agent will have a toolkit of tools and that's what that's what it's going to do there I'm going to import load tools I'm going to initialize the agent I'm going to import openai as well with that I'm going to create my language model now I've made I've insert my serp API key because that's the example that we're going to be running through here which is an easy way to search Google and then with the toolkit I'm going to go ahead and load the tools now in this case I'm only loading one tool and it's the server API however you could load in a lot of tools here and you may naturally think well let me just load it up with all the tools in the world you could it's just going to get difficult for the model or the agent to know which tool to use at which time so you kind of only want to use the ones that you know you're going to um be needing at that at that point so I'm going to pass in my language model and I'm going to pass in my serve API here API key then I'm going to create my agent so I'm going to pass in the toolkit that I just made I'm going to pass in the language model again I'm going to say what type of agent are you now there's different agent types for different types of tasks and I encourage you to go check out the language or the documentation to see which would be best for you I'm going to say verbose equals true so we can see it thinking I'm also going to return the intermediate steps which just means that we get more granularity into what it's actually doing with this I'm going to say response uh oh agent is not defined then what I'm going to do here is I'm going to pass in my query to the agent itself so what was the first album of the band that Natalie Bergman is a part of the reason why I asked this question specifically is because keep in mind I haven't uploaded any documents here so there's no information pre-loaded and it's kind of a complicated question that has multiple steps that need to be answered for it this is a perfect question for an agent here so let's go ahead and run this and let's see how the agent is thinking about it entering the new agent executor class and it said I should try to find out what band Natalie Bergman is a part of so it needs to it knows that it needs to go search which it has a Search tool up above which I gave it and it's saying Natalie Bergman banded so it's searching for that one and it says observation which is what it observed from its action Natalie Bergman is an American singer-songwriter she has one half the duo of wild Belle okay cool I should search for the debut album of wild Belle it understood the band that she's a part of and now it now it knows and needs to go search for that band so it's going to search again it's going to say wild Belle debut album and it observes that the debut album is Isles I know the final answer which is good we want it to know the finance uh Isles is the debut album of wild Belle the band that Natalie Bergman is a part of that is really cool because that is a multi-step question and uh the agent knew what it needed to go find out without me telling it the chain so this chain could have been a whole lot longer if it needed more steps with it but uh it dynamically figured that out along the way which is really really cool and so if we were to print out the intermediate steps you get more information about what it actually did and how it searched and all that good information from there um and if we were to confirm this let's go ahead and run this WOW yep wild Belle there's Natalie Bergman brother and sister Duo band um beautiful I would play their song If it wasn't going to give me copyright trouble but I encourage you to go look it up link to my favorite song of theirs is in the description well my friends that was a very broad overview of all of the nuts and bolts\",\n",
       " \"of lion chain the Tactical nuts and bolts I congratulate you for making it to the end of this video and if you have any questions please let me know I encourage you to subscribe to check out for part two when we go through actual use cases for these nuts and bolts and again I share a lot of tools on Twitter so I encourage you to follow me there like always please leave comments let me know what you think of the video and let me know if you have any questions we'll see you later\"]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = my_text_splitter(caption)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def summarize(text, context = 'summarize the following text:', model = 'gpt-3.5-turbo'):\n",
    "    \"\"\"Returns the summary of a text.\"\"\"\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[\n",
    "        {'role': 'system','content': context},\n",
    "        {'role': 'user', 'content': text}\n",
    "            ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(summary, chunk,  model = 'gpt-3.5-turbo'):\n",
    "    \"\"\"Refine the summary with each new chunk of text\"\"\"\n",
    "    context = \"Refine the summary with the following context: \" + summary\n",
    "    summary = summarize(chunk, context, model)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The video provides an introduction to Lang chain, a framework for building applications powered by language models. It explains the various components of Lang chain and highlights its benefits. The author also provides a companion document and code samples for further exploration. The video concludes with an overview of chains and agents, which allow for combining different language model calls and enable decision-making. The author encourages viewers to subscribe and stay tuned for part two, where they will explore actual use cases. They also invite comments and questions.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires initialization with summary of first chunk \n",
    "summary = summarize(chunks[0])\n",
    "for chunk in chunks[1:]:\n",
    "    summary = refine(summary, chunk)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
