WEBVTT

00:00:00.539 --> 00:00:04.620
hi I am Jeremy Howard from fast.ai and

00:00:04.620 --> 00:00:07.200
this is a hacker's guide to language

00:00:07.200 --> 00:00:09.720
models

00:00:09.720 --> 00:00:12.000
when I say a hacker's guide what we're

00:00:12.000 --> 00:00:14.759
going to be looking at is a code first

00:00:14.759 --> 00:00:16.980
approach to understanding how to use

00:00:16.980 --> 00:00:19.980
language models in practice

00:00:19.980 --> 00:00:21.840
so before we get started we should

00:00:21.840 --> 00:00:23.400
probably talk about what is a language

00:00:23.400 --> 00:00:25.460
model

00:00:25.460 --> 00:00:28.519
I would say that

00:00:28.519 --> 00:00:31.859
this is going to make more sense if you

00:00:31.859 --> 00:00:34.820
know the kind of basics of deep learning

00:00:34.820 --> 00:00:37.620
if you don't I think you'll still get

00:00:37.620 --> 00:00:39.059
plenty out of it and there'll be plenty

00:00:39.059 --> 00:00:42.059
of things you can do but if you do have

00:00:42.059 --> 00:00:44.360
a chance I would recommend checking out

00:00:44.360 --> 00:00:46.760
course.fast.ai which is a free course

00:00:46.760 --> 00:00:49.980
and specifically

00:00:49.980 --> 00:00:50.760
um

00:00:50.760 --> 00:00:53.700
if you could at least kind of watch if

00:00:53.700 --> 00:00:57.059
not work through the first five lessons

00:00:57.059 --> 00:00:59.699
that would get you to a point where you

00:00:59.699 --> 00:01:02.219
understand all the basic fundamentals of

00:01:02.219 --> 00:01:05.600
deep learning that will make this this

00:01:05.600 --> 00:01:08.780
lesson tutorial make even more sense

00:01:08.780 --> 00:01:11.159
maybe I shouldn't call this a tutorial

00:01:11.159 --> 00:01:13.799
it's more of a quick run through so I've

00:01:13.799 --> 00:01:15.600
got to try to run through all the basic

00:01:15.600 --> 00:01:18.960
ideas of language models how to use them

00:01:18.960 --> 00:01:22.020
both open source ones and open AI based

00:01:22.020 --> 00:01:23.759
ones and it's all going to be based

00:01:23.759 --> 00:01:27.720
using Code as much as possible

00:01:27.720 --> 00:01:30.659
um so let's start by talking about what

00:01:30.659 --> 00:01:33.659
a language model is and so as you might

00:01:33.659 --> 00:01:35.400
have heard before a language model is

00:01:35.400 --> 00:01:36.960
something that knows how to predict the

00:01:36.960 --> 00:01:39.180
next word of a sentence or knows how to

00:01:39.180 --> 00:01:41.299
fill in the missing words of a sentence

00:01:41.299 --> 00:01:43.860
and we can look at an example of one

00:01:43.860 --> 00:01:46.560
open AI has a language model text

00:01:46.560 --> 00:01:50.340
DaVinci 003 and we can play with it by

00:01:50.340 --> 00:01:52.259
passing in

00:01:52.259 --> 00:01:55.259
some words and ask it to predict what

00:01:55.259 --> 00:01:58.500
the next words might be so if we pass in

00:01:58.500 --> 00:02:00.479
when I arrived back at the panda

00:02:00.479 --> 00:02:02.040
breeding facility after the

00:02:02.040 --> 00:02:04.020
extraordinary reign of live frogs I

00:02:04.020 --> 00:02:06.659
couldn't believe what I saw I just came

00:02:06.659 --> 00:02:07.979
up with that yesterday and I thought

00:02:07.979 --> 00:02:09.840
what might happen next so kind of fun

00:02:09.840 --> 00:02:12.959
for Creative brainstorming uh there's a

00:02:12.959 --> 00:02:15.660
nice site called nat.dev

00:02:15.660 --> 00:02:18.300
Nat dot let Dev lets us play with a

00:02:18.300 --> 00:02:20.040
variety of language models and here I've

00:02:20.040 --> 00:02:23.400
selected text DaVinci 003 and I'll hit

00:02:23.400 --> 00:02:27.239
submit and it starts printing stuff out

00:02:27.239 --> 00:02:29.580
the pandas were happily playing and

00:02:29.580 --> 00:02:31.140
eating the frogs that had fallen from

00:02:31.140 --> 00:02:33.120
the sky there's an amazing sight to see

00:02:33.120 --> 00:02:34.739
these animals taking advantage of such a

00:02:34.739 --> 00:02:36.660
unique opportunity

00:02:36.660 --> 00:02:39.000
first after quick measures to ensure the

00:02:39.000 --> 00:02:41.099
safety of the pandas and the frogs so

00:02:41.099 --> 00:02:42.180
there you go that's what happened after

00:02:42.180 --> 00:02:44.099
the extraordinary reign of live frogs at

00:02:44.099 --> 00:02:46.620
the panda breeding facility uh you'll

00:02:46.620 --> 00:02:49.019
see here that I've enabled show

00:02:49.019 --> 00:02:50.519
probabilities which is a thing in

00:02:50.519 --> 00:02:53.160
that.dev where it shows

00:02:53.160 --> 00:02:55.319
um well let's take a look it's pretty

00:02:55.319 --> 00:02:56.940
likely the next word here is going to be

00:02:56.940 --> 00:02:59.220
the and after this since we're talking

00:02:59.220 --> 00:03:00.660
about a panda breeding facility it's

00:03:00.660 --> 00:03:02.519
going to be Panda's were and what were

00:03:02.519 --> 00:03:04.200
they doing well they could have been

00:03:04.200 --> 00:03:05.580
doing a few things they could have been

00:03:05.580 --> 00:03:07.140
doing something happily

00:03:07.140 --> 00:03:09.660
or the pandas were having the pandas

00:03:09.660 --> 00:03:12.300
were out the pandas were playing so it

00:03:12.300 --> 00:03:14.819
picked the most likely uh it thought it

00:03:14.819 --> 00:03:16.560
was 20 likely it's going to be happily

00:03:16.560 --> 00:03:19.260
and what were they happily doing

00:03:19.260 --> 00:03:21.000
could have been playing

00:03:21.000 --> 00:03:22.319
hopping

00:03:22.319 --> 00:03:23.819
eating

00:03:23.819 --> 00:03:26.159
and so forth

00:03:26.159 --> 00:03:28.980
so they're eating the frogs that and

00:03:28.980 --> 00:03:31.680
then had almost certainly so you can see

00:03:31.680 --> 00:03:33.780
what it's doing at each point is it's

00:03:33.780 --> 00:03:35.940
predicting the probability of a variety

00:03:35.940 --> 00:03:38.459
of possible next words and depending on

00:03:38.459 --> 00:03:40.920
how you set it up it will either pick

00:03:40.920 --> 00:03:43.400
the most likely one every time

00:03:43.400 --> 00:03:46.500
or you can change muck around with

00:03:46.500 --> 00:03:49.580
things like P values and temperatures

00:03:49.580 --> 00:03:55.319
to change what comes up so at each time

00:03:55.319 --> 00:03:58.700
then it'll give us a different result

00:03:58.700 --> 00:04:02.480
and this is kind of fun

00:04:03.120 --> 00:04:05.340
frogs perched on the heads of some of

00:04:05.340 --> 00:04:07.739
the pandas it was an amazing sight

00:04:07.739 --> 00:04:09.360
etc etc

00:04:09.360 --> 00:04:10.680
okay

00:04:10.680 --> 00:04:15.540
so that's what a language model does

00:04:15.540 --> 00:04:17.659
um

00:04:18.299 --> 00:04:21.000
now you might notice

00:04:21.000 --> 00:04:24.120
here it hasn't predicted pandas it's

00:04:24.120 --> 00:04:27.060
predicted panned

00:04:27.060 --> 00:04:28.919
and then separately

00:04:28.919 --> 00:04:30.120
us

00:04:30.120 --> 00:04:32.460
okay after Panda it's going to be us so

00:04:32.460 --> 00:04:34.440
it's not always a whole word

00:04:34.440 --> 00:04:36.600
here it's an

00:04:36.600 --> 00:04:38.340
and then harmed

00:04:38.340 --> 00:04:42.540
oh actually it's unha mood so you can

00:04:42.540 --> 00:04:43.979
see that it's not always predicting

00:04:43.979 --> 00:04:45.540
words specifically what it's doing is

00:04:45.540 --> 00:04:48.960
predicting tokens uh tokens are either

00:04:48.960 --> 00:04:53.699
whole words or sub word units pieces of

00:04:53.699 --> 00:04:55.500
a word or it could even be punctuation

00:04:55.500 --> 00:04:59.000
or numbers or so forth

00:05:00.780 --> 00:05:02.100
um so let's have a look at how that

00:05:02.100 --> 00:05:05.040
works so for example we can use the

00:05:05.040 --> 00:05:06.000
actual

00:05:06.000 --> 00:05:07.919
um it's called tokenization to create

00:05:07.919 --> 00:05:10.620
tokens from us from a uh from a string

00:05:10.620 --> 00:05:14.160
we can use the same tokenizer that GPT

00:05:14.160 --> 00:05:16.740
uses by using tick token and we can

00:05:16.740 --> 00:05:19.040
specifically say we want to use the same

00:05:19.040 --> 00:05:21.120
tokenizer that that model text

00:05:21.120 --> 00:05:23.580
eventually double O three uses and so

00:05:23.580 --> 00:05:26.699
for example when I earlier tried this it

00:05:26.699 --> 00:05:29.280
talked about the Frog splashing and so I

00:05:29.280 --> 00:05:31.020
thought I'll include data we'll encode

00:05:31.020 --> 00:05:33.600
they are splashing and the result is a

00:05:33.600 --> 00:05:35.400
bunch of numbers

00:05:35.400 --> 00:05:37.320
and what those numbers are they'd

00:05:37.320 --> 00:05:39.840
basically just lookups into a vocabulary

00:05:39.840 --> 00:05:42.240
that openai in this case created and if

00:05:42.240 --> 00:05:44.280
you train your own models you'll be

00:05:44.280 --> 00:05:45.840
automatically creating or your code will

00:05:45.840 --> 00:05:46.620
create

00:05:46.620 --> 00:05:50.460
and if I then decode those it says oh

00:05:50.460 --> 00:05:52.440
these numbers are they

00:05:52.440 --> 00:05:56.460
space r space spool

00:05:56.460 --> 00:05:57.660
hashing

00:05:57.660 --> 00:05:59.520
and so put that all together they are

00:05:59.520 --> 00:06:02.160
splashing so you can see that

00:06:02.160 --> 00:06:05.880
the start of a word is give me the space

00:06:05.880 --> 00:06:11.360
before it is also being encoded here

00:06:11.720 --> 00:06:16.880
so these um language models are quite

00:06:16.880 --> 00:06:21.120
neat that they can work at all but

00:06:21.120 --> 00:06:23.639
they're not of themselves really

00:06:23.639 --> 00:06:27.300
designed to do anything

00:06:27.300 --> 00:06:29.639
um uh let me explain

00:06:29.639 --> 00:06:32.880
um the basic idea

00:06:32.880 --> 00:06:38.900
of what chat GPT gpt4 Bard Etc are doing

00:06:38.900 --> 00:06:41.479
comes from a paper

00:06:41.479 --> 00:06:44.280
which describes an algorithm that I

00:06:44.280 --> 00:06:47.940
created back in 2017 called ULM fit and

00:06:47.940 --> 00:06:50.039
Sebastian Rooter and I wrote a paper up

00:06:50.039 --> 00:06:52.020
describing the ULM fit approach which

00:06:52.020 --> 00:06:54.180
was the one that basically laid out

00:06:54.180 --> 00:06:56.039
what everybody's doing how this system

00:06:56.039 --> 00:06:59.699
works and the system has three steps

00:06:59.699 --> 00:07:01.440
step one is

00:07:01.440 --> 00:07:03.479
language model training but you'll see

00:07:03.479 --> 00:07:05.340
this is actually from the paper we

00:07:05.340 --> 00:07:07.680
actually described it as pre-training

00:07:07.680 --> 00:07:09.479
now what language model pre-training

00:07:09.479 --> 00:07:12.060
does is this is the thing which predicts

00:07:12.060 --> 00:07:14.699
the next word of a sentence and so in

00:07:14.699 --> 00:07:17.039
the original ULM fit paper so the

00:07:17.039 --> 00:07:19.380
algorithm I developed in 2017 then

00:07:19.380 --> 00:07:21.479
Sebastian Rooter and I wrote it up in

00:07:21.479 --> 00:07:24.020
2018 early 2018

00:07:24.020 --> 00:07:27.240
what I originally did was I trained this

00:07:27.240 --> 00:07:29.340
language model on Wikipedia now what

00:07:29.340 --> 00:07:33.840
that meant is I took a neural network

00:07:33.840 --> 00:07:35.699
um and a neural network is just a

00:07:35.699 --> 00:07:36.840
function if you don't know what it is

00:07:36.840 --> 00:07:38.280
it's just a mathematical function that's

00:07:38.280 --> 00:07:40.319
extremely flexible and it's got lots and

00:07:40.319 --> 00:07:41.819
lots of parameters and initially it

00:07:41.819 --> 00:07:45.000
can't do anything but using stochastic

00:07:45.000 --> 00:07:47.699
gradient descent or SGD you can teach it

00:07:47.699 --> 00:07:49.740
to do almost anything if you give it

00:07:49.740 --> 00:07:51.780
examples and so I gave it lots of

00:07:51.780 --> 00:07:54.419
examples of sentences from Wikipedia so

00:07:54.419 --> 00:07:56.340
for example from the Wikipedia article

00:07:56.340 --> 00:07:59.280
for the birds the birds is a 1963

00:07:59.280 --> 00:08:02.220
American Natural horror natural horror

00:08:02.220 --> 00:08:04.020
Thriller film produced and directed by

00:08:04.020 --> 00:08:06.180
Alfred and then it would stop

00:08:06.180 --> 00:08:07.979
and so then the model would have to

00:08:07.979 --> 00:08:10.080
guess what the next word is

00:08:10.080 --> 00:08:12.780
and if it guest Hitchcock it would be

00:08:12.780 --> 00:08:15.300
rewarded and if it gets guessed

00:08:15.300 --> 00:08:17.639
something else it would be penalized and

00:08:17.639 --> 00:08:19.199
effectively basically it's trying to

00:08:19.199 --> 00:08:21.360
maximize those rewards it's trying to

00:08:21.360 --> 00:08:23.879
find a set of weights for this function

00:08:23.879 --> 00:08:25.740
that makes it more likely that it would

00:08:25.740 --> 00:08:27.900
predict Hitchcock and then later on in

00:08:27.900 --> 00:08:30.660
this article it reads from Wikipedia at

00:08:30.660 --> 00:08:33.240
a previously dated Mitch but ended it

00:08:33.240 --> 00:08:35.159
due to Mitch's cold overbearing mother

00:08:35.159 --> 00:08:38.820
Lydia who dislikes any woman in mitches

00:08:38.820 --> 00:08:41.219
now you can see that filling this in

00:08:41.219 --> 00:08:43.620
actually requires being pretty

00:08:43.620 --> 00:08:45.000
thoughtful because there's a bunch of

00:08:45.000 --> 00:08:47.640
things that like kind of logically could

00:08:47.640 --> 00:08:49.560
go there like a woman could be in

00:08:49.560 --> 00:08:51.060
Mitch's

00:08:51.060 --> 00:08:56.420
closet could be in which is house

00:08:56.420 --> 00:08:58.920
and so you know you could probably guess

00:08:58.920 --> 00:09:01.140
in the Wikipedia article describing the

00:09:01.140 --> 00:09:02.459
plot of the birds it's actually any

00:09:02.459 --> 00:09:05.279
woman in Mitch's life

00:09:05.279 --> 00:09:06.839
now

00:09:06.839 --> 00:09:09.120
to do a good job

00:09:09.120 --> 00:09:11.880
of solving this problem as well as

00:09:11.880 --> 00:09:14.279
possible of guessing the next word of

00:09:14.279 --> 00:09:15.320
sentences

00:09:15.320 --> 00:09:17.640
the neural network

00:09:17.640 --> 00:09:21.480
is gonna have to learn a lot of stuff

00:09:21.480 --> 00:09:25.380
about the world it's going to learn

00:09:25.380 --> 00:09:27.480
that there are things called objects

00:09:27.480 --> 00:09:29.220
that there's a thing called time that

00:09:29.220 --> 00:09:32.640
objects react to each other over time

00:09:32.640 --> 00:09:35.100
that there are things called movies that

00:09:35.100 --> 00:09:37.560
movies have directors that there are

00:09:37.560 --> 00:09:39.120
people that people have names and so

00:09:39.120 --> 00:09:41.220
forth and that a movie director is

00:09:41.220 --> 00:09:43.140
Alfred Hitchcock and he directed horror

00:09:43.140 --> 00:09:45.120
films and

00:09:45.120 --> 00:09:45.959
um

00:09:45.959 --> 00:09:49.140
so on and so forth it's going to have to

00:09:49.140 --> 00:09:51.180
learn extraordinary amount if it's going

00:09:51.180 --> 00:09:52.680
to do a really good job of predicting

00:09:52.680 --> 00:09:54.660
the next word of sentences

00:09:54.660 --> 00:09:57.779
now these neural networks specifically

00:09:57.779 --> 00:09:59.760
are deep neural networks so this is deep

00:09:59.760 --> 00:10:01.920
learning and in these deep neural

00:10:01.920 --> 00:10:04.380
networks which have

00:10:04.380 --> 00:10:06.060
um when when I created this I think it

00:10:06.060 --> 00:10:08.399
had like 100 million parameters nowadays

00:10:08.399 --> 00:10:10.920
they have billions of parameters

00:10:10.920 --> 00:10:12.920
um

00:10:13.080 --> 00:10:16.940
it's got the ability to create a rich

00:10:16.940 --> 00:10:19.260
hierarchy of abstractions and

00:10:19.260 --> 00:10:23.000
representations which it can build on

00:10:23.000 --> 00:10:27.779
and so this is really the the key idea

00:10:27.779 --> 00:10:29.459
behind

00:10:29.459 --> 00:10:31.740
neural networks and language models is

00:10:31.740 --> 00:10:33.720
that if it's going to do a good job of

00:10:33.720 --> 00:10:35.399
being able to predict the next word of

00:10:35.399 --> 00:10:38.700
any sentence in any situation it's going

00:10:38.700 --> 00:10:40.140
to have to know an awful lot about the

00:10:40.140 --> 00:10:41.880
world it's going to have to know about

00:10:41.880 --> 00:10:44.940
how to solve math questions or figure

00:10:44.940 --> 00:10:49.399
out the next move in a chess game or

00:10:49.399 --> 00:10:52.860
recognize poetry and so on and so forth

00:10:52.860 --> 00:10:53.880
now

00:10:53.880 --> 00:10:55.680
nobody said it's going to do a good job

00:10:55.680 --> 00:10:56.720
of that

00:10:56.720 --> 00:11:00.000
so it's a lot of work to find to create

00:11:00.000 --> 00:11:02.160
and train a model that is good at that

00:11:02.160 --> 00:11:03.899
but if you can create one that's good at

00:11:03.899 --> 00:11:07.100
that it's going to have a lot of

00:11:07.100 --> 00:11:09.839
capabilities internally that it would

00:11:09.839 --> 00:11:11.820
have to be a drawing on to be able to do

00:11:11.820 --> 00:11:14.640
this effectively so the key idea here

00:11:14.640 --> 00:11:18.180
for me is that this is a form of

00:11:18.180 --> 00:11:20.519
compression and this idea of the

00:11:20.519 --> 00:11:22.380
relationship between compression and

00:11:22.380 --> 00:11:25.519
intelligence goes back many many decades

00:11:25.519 --> 00:11:28.620
and the basic idea is that yeah if you

00:11:28.620 --> 00:11:30.140
can

00:11:30.140 --> 00:11:33.839
guess what words are coming up next then

00:11:33.839 --> 00:11:35.820
effectively you're compressing all that

00:11:35.820 --> 00:11:40.700
information down into a neural network

00:11:41.100 --> 00:11:42.779
um now I said this is not useful of

00:11:42.779 --> 00:11:46.079
itself well why do we do it well we do

00:11:46.079 --> 00:11:48.420
it because we want to pull out those

00:11:48.420 --> 00:11:50.579
capabilities and the way we pull out

00:11:50.579 --> 00:11:52.440
those capabilities is we take two more

00:11:52.440 --> 00:11:55.440
steps the second step is we do something

00:11:55.440 --> 00:11:57.839
called language model fine tuning

00:11:57.839 --> 00:12:01.320
a language model fine tuning we are no

00:12:01.320 --> 00:12:03.420
longer just giving it all of Wikipedia

00:12:03.420 --> 00:12:05.160
or nowadays we don't just give it all of

00:12:05.160 --> 00:12:06.540
Wikipedia

00:12:06.540 --> 00:12:08.040
but in fact

00:12:08.040 --> 00:12:10.800
a large chunk of the internet is fed to

00:12:10.800 --> 00:12:12.779
pre-training these models in the fine

00:12:12.779 --> 00:12:14.399
tuning stage

00:12:14.399 --> 00:12:17.339
we feed it a set of documents a lot

00:12:17.339 --> 00:12:20.220
closer to the final task that we want

00:12:20.220 --> 00:12:21.839
the model to do

00:12:21.839 --> 00:12:23.940
but it's still the same basic idea it's

00:12:23.940 --> 00:12:26.700
still trying to predict the next word of

00:12:26.700 --> 00:12:29.220
a sentence

00:12:29.220 --> 00:12:32.459
after that we then do a final classifier

00:12:32.459 --> 00:12:33.959
fine tuning and then the classifier

00:12:33.959 --> 00:12:37.200
fine-tuning this is this is the kind of

00:12:37.200 --> 00:12:39.000
end task we're trying to get it to do

00:12:39.000 --> 00:12:43.560
now nowadays these two steps are very

00:12:43.560 --> 00:12:45.720
specific approaches are taken for the

00:12:45.720 --> 00:12:48.000
step two the step B the language model

00:12:48.000 --> 00:12:49.320
fine tuning

00:12:49.320 --> 00:12:51.240
people nowadays do a particular kind

00:12:51.240 --> 00:12:54.060
called instruction tuning the idea is

00:12:54.060 --> 00:12:56.279
that the task we want most of the time

00:12:56.279 --> 00:12:58.260
to achieve is

00:12:58.260 --> 00:13:01.560
solve problems answer questions and so

00:13:01.560 --> 00:13:04.139
in the instruction tuning phase we use

00:13:04.139 --> 00:13:07.440
data sets like this one this is a great

00:13:07.440 --> 00:13:09.779
data set called openalker created by a

00:13:09.779 --> 00:13:14.579
fantastic open source group and and it's

00:13:14.579 --> 00:13:16.260
built on top of something called the

00:13:16.260 --> 00:13:17.779
flan collection

00:13:17.779 --> 00:13:21.899
and you can see that basically

00:13:21.899 --> 00:13:23.820
there's all kinds of different questions

00:13:23.820 --> 00:13:27.779
in here so this four gigabytes of

00:13:27.779 --> 00:13:30.839
of questions and context and so forth

00:13:30.839 --> 00:13:34.500
and each one generally has a question or

00:13:34.500 --> 00:13:37.139
an instruction or a request and then a

00:13:37.139 --> 00:13:39.680
response

00:13:40.139 --> 00:13:44.459
here are some examples of instructions I

00:13:44.459 --> 00:13:46.200
think this is from the flan data set if

00:13:46.200 --> 00:13:48.540
I remember correctly so for instance it

00:13:48.540 --> 00:13:50.399
could be does the sentence in the Iron

00:13:50.399 --> 00:13:53.160
Age answer the question the period of

00:13:53.160 --> 00:13:55.740
time from 1200 to 1000 BCE is known as

00:13:55.740 --> 00:13:59.579
what choice is one yes or no and then

00:13:59.579 --> 00:14:02.220
the language model is meant to write one

00:14:02.220 --> 00:14:07.079
or two as appropriate for yes or no

00:14:07.079 --> 00:14:09.060
or it could be uh things about I think

00:14:09.060 --> 00:14:11.700
this is from a music video who is the

00:14:11.700 --> 00:14:14.459
girl in more than you know answer and

00:14:14.459 --> 00:14:15.959
then it would have to write the correct

00:14:15.959 --> 00:14:18.779
name of the remember model or dancer or

00:14:18.779 --> 00:14:21.300
whatever from um from that music video

00:14:21.300 --> 00:14:24.660
and so forth so it's still doing

00:14:24.660 --> 00:14:27.540
language modeling so fine-tuning and

00:14:27.540 --> 00:14:29.000
pre-training are kind of the same thing

00:14:29.000 --> 00:14:32.820
but this is more targeted now not just

00:14:32.820 --> 00:14:35.100
to be able to fill in the missing parts

00:14:35.100 --> 00:14:38.700
of any document from the internet

00:14:38.700 --> 00:14:42.839
um but to fill in the words necessary to

00:14:42.839 --> 00:14:45.839
to answer questions to do useful things

00:14:45.839 --> 00:14:49.440
okay so that's instruction tuning

00:14:49.440 --> 00:14:51.660
and then step three which is the

00:14:51.660 --> 00:14:54.120
classifier fine tuning nowadays there's

00:14:54.120 --> 00:14:56.639
generally various approaches such as

00:14:56.639 --> 00:14:58.260
reinforcement learning from Human

00:14:58.260 --> 00:15:03.660
feedback and others which are basically

00:15:03.779 --> 00:15:08.040
giving humans or sometimes more advanced

00:15:08.040 --> 00:15:09.320
models

00:15:09.320 --> 00:15:12.360
multiple answers to a question such as

00:15:12.360 --> 00:15:14.579
here are some from a reinforcement

00:15:14.579 --> 00:15:16.079
lighting from Human feedback paper I

00:15:16.079 --> 00:15:17.160
can't remember which one I got it from

00:15:17.160 --> 00:15:19.560
list five ideas for how to regain

00:15:19.560 --> 00:15:22.440
enthusiasm for my career and so the

00:15:22.440 --> 00:15:25.380
model will spit out two possible answers

00:15:25.380 --> 00:15:27.540
or it'll have a less good model and a

00:15:27.540 --> 00:15:30.300
more good model and then a human or a

00:15:30.300 --> 00:15:33.199
better model will pick which is best

00:15:33.199 --> 00:15:35.880
and so that's used for the the final

00:15:35.880 --> 00:15:37.920
fine tuning Stitch

00:15:37.920 --> 00:15:40.079
so all of that is to say

00:15:40.079 --> 00:15:40.740
um

00:15:40.740 --> 00:15:44.639
although you can download pure language

00:15:44.639 --> 00:15:47.760
models from the internet

00:15:47.760 --> 00:15:51.240
um they're not generally that useful of

00:15:51.240 --> 00:15:53.699
their on their own until you've

00:15:53.699 --> 00:15:55.019
fine-tuned them now you don't

00:15:55.019 --> 00:15:56.940
necessarily need step C nowadays

00:15:56.940 --> 00:15:58.199
actually people are discovering that

00:15:58.199 --> 00:16:00.360
maybe just step B might be enough it's

00:16:00.360 --> 00:16:02.760
still a bit controversial

00:16:02.760 --> 00:16:04.680
Okay so

00:16:04.680 --> 00:16:07.380
when we talk about a language bottle

00:16:07.380 --> 00:16:09.120
um where we could be talking about

00:16:09.120 --> 00:16:10.620
something that's just been pre-trained

00:16:10.620 --> 00:16:13.320
something that's been fine-tuned or

00:16:13.320 --> 00:16:14.339
something that's gone through something

00:16:14.339 --> 00:16:16.980
like rlhf all of those things are

00:16:16.980 --> 00:16:19.500
generally described nowadays as language

00:16:19.500 --> 00:16:22.040
models

00:16:22.980 --> 00:16:25.620
so my view my view is that if you are

00:16:25.620 --> 00:16:27.060
going to be

00:16:27.060 --> 00:16:30.320
good at language modeling in any way

00:16:30.320 --> 00:16:33.660
then you need to start by being a really

00:16:33.660 --> 00:16:35.940
effective user of language models and to

00:16:35.940 --> 00:16:37.259
be a really effective user of language

00:16:37.259 --> 00:16:38.940
models you've got to use the best one

00:16:38.940 --> 00:16:41.820
that there is and currently so what are

00:16:41.820 --> 00:16:45.899
we up to September 2023 the best one is

00:16:45.899 --> 00:16:50.699
by far gpt4 this might change sometime

00:16:50.699 --> 00:16:52.199
in the not too distant future but this

00:16:52.199 --> 00:16:54.660
is right now gpt4 is the recommendation

00:16:54.660 --> 00:16:58.019
strong strong recommendation now you can

00:16:58.019 --> 00:17:00.180
use GPT for

00:17:00.180 --> 00:17:02.820
by paying 20 bucks a month to open Ai

00:17:02.820 --> 00:17:05.220
and then you can use it a whole lot it's

00:17:05.220 --> 00:17:06.540
very hard to

00:17:06.540 --> 00:17:10.459
to run out of credits I find

00:17:10.459 --> 00:17:13.799
now what can GPT do

00:17:13.799 --> 00:17:16.260
it's interesting and instructive in my

00:17:16.260 --> 00:17:19.439
opinion to start with the very common

00:17:19.439 --> 00:17:21.540
views you see on the internet or even in

00:17:21.540 --> 00:17:24.000
Academia about what it can't do so for

00:17:24.000 --> 00:17:25.439
example there was this paper you might

00:17:25.439 --> 00:17:29.179
have seen GPT for can't reason

00:17:29.179 --> 00:17:33.500
which describes a number of uh empirical

00:17:33.500 --> 00:17:36.960
analysis done of 25 diverse reasoning

00:17:36.960 --> 00:17:40.559
problems and found it that it

00:17:40.559 --> 00:17:42.480
was not able to solve them and it's

00:17:42.480 --> 00:17:46.799
utterly incapable of reasoning so

00:17:46.799 --> 00:17:48.660
I always find you've got to be a bit

00:17:48.660 --> 00:17:50.460
careful about reading stuff like this

00:17:50.460 --> 00:17:51.900
because I just talked the first three

00:17:51.900 --> 00:17:55.580
that I came across in that paper and I

00:17:55.580 --> 00:17:59.640
gave them to gpt4

00:17:59.640 --> 00:18:01.860
um and by the way something very useful

00:18:01.860 --> 00:18:06.960
in gpt4 is you can click on the the

00:18:06.960 --> 00:18:10.080
share button and you'll get something

00:18:10.080 --> 00:18:12.000
that looks like this and this is really

00:18:12.000 --> 00:18:14.520
handy so here's an example of something

00:18:14.520 --> 00:18:17.640
from the paper that said gpt4 can't do

00:18:17.640 --> 00:18:20.760
this Mabel's heart rate at 9 00 am was

00:18:20.760 --> 00:18:22.799
75 beats per minute her blood pressure

00:18:22.799 --> 00:18:26.340
at 7 pm was 120 over 80. she died 11 p.m

00:18:26.340 --> 00:18:28.380
while she arrive at noon so of course

00:18:28.380 --> 00:18:30.600
you're human we know obviously she must

00:18:30.600 --> 00:18:31.940
be

00:18:31.940 --> 00:18:35.580
and GPT forces Hmm this appears to be a

00:18:35.580 --> 00:18:37.620
riddle not a real inquiry into medical

00:18:37.620 --> 00:18:40.440
conditions uh here's a summary of the

00:18:40.440 --> 00:18:43.320
information and yeah

00:18:43.320 --> 00:18:46.919
it sounds like Mabel was alive at noon

00:18:46.919 --> 00:18:49.380
so that's correct uh this was the second

00:18:49.380 --> 00:18:51.480
one I tried from the paper that says

00:18:51.480 --> 00:18:54.299
gpt4 can't do this and I found actually

00:18:54.299 --> 00:18:57.419
gpt4 can do this

00:18:57.419 --> 00:18:59.820
um and it said that gpt4 can't do this

00:18:59.820 --> 00:19:03.600
and I found gpt4 can do this now

00:19:03.600 --> 00:19:06.660
um I mentioned this to say gpt4 is

00:19:06.660 --> 00:19:08.940
probably a lot better than you would

00:19:08.940 --> 00:19:11.880
expect if you've read all this um stuff

00:19:11.880 --> 00:19:13.799
on the internet about all the dumb

00:19:13.799 --> 00:19:16.440
things that it does

00:19:16.440 --> 00:19:17.039
um

00:19:17.039 --> 00:19:18.720
almost every time I see on the internet

00:19:18.720 --> 00:19:21.120
saying something something that GPT 4

00:19:21.120 --> 00:19:23.820
can't do I check it and it turns out it

00:19:23.820 --> 00:19:25.860
does this one was just last week

00:19:25.860 --> 00:19:29.039
Sally a girl has three brothers each

00:19:29.039 --> 00:19:31.440
brother has two sisters how many sisters

00:19:31.440 --> 00:19:33.419
does Sally have

00:19:33.419 --> 00:19:36.620
so have a think about it

00:19:36.840 --> 00:19:40.140
and so gpt4 says okay

00:19:40.140 --> 00:19:42.120
Sally's counted as one system each of

00:19:42.120 --> 00:19:43.440
her brothers

00:19:43.440 --> 00:19:45.780
if each brother has two sisters

00:19:45.780 --> 00:19:47.460
that means there's another sister in the

00:19:47.460 --> 00:19:49.799
picture apart from salary so Sally has

00:19:49.799 --> 00:19:53.160
one sister okay correct

00:19:53.160 --> 00:19:55.280
um

00:19:56.880 --> 00:19:58.799
and then this one I got sort of like

00:19:58.799 --> 00:20:00.740
three or four days ago

00:20:00.740 --> 00:20:03.660
this is a common view that language

00:20:03.660 --> 00:20:05.820
models can't

00:20:05.820 --> 00:20:08.039
track things like this see is the riddle

00:20:08.039 --> 00:20:09.780
I'm in my house on top of my chair in

00:20:09.780 --> 00:20:11.640
the living room is a coffee cup inside

00:20:11.640 --> 00:20:13.799
the coffee cup is a thimble inside the

00:20:13.799 --> 00:20:15.419
thimble is a diamond

00:20:15.419 --> 00:20:17.460
I moved the chair to the bedroom I put

00:20:17.460 --> 00:20:18.960
the coffee cup on the bed I turned the

00:20:18.960 --> 00:20:20.880
cup upside down then I return it upside

00:20:20.880 --> 00:20:22.740
up Place The Coffee Cup on the counter

00:20:22.740 --> 00:20:24.780
in the kitchen where's my diamond

00:20:24.780 --> 00:20:28.919
and so gpt4 says yeah okay you turned it

00:20:28.919 --> 00:20:31.320
upside down so probably the diamond fell

00:20:31.320 --> 00:20:32.340
out

00:20:32.340 --> 00:20:34.260
so therefore the diamond is in the

00:20:34.260 --> 00:20:37.260
bedroom where it fell out okay correct

00:20:37.260 --> 00:20:38.039
um

00:20:38.039 --> 00:20:43.200
why is it that people are claiming that

00:20:43.200 --> 00:20:45.660
gpt4 can't do these things we can well

00:20:45.660 --> 00:20:47.460
the reason is because I think on the

00:20:47.460 --> 00:20:50.520
whole they are not aware of how gpt4 was

00:20:50.520 --> 00:20:52.940
trained

00:20:53.000 --> 00:20:54.720
gpt4

00:20:54.720 --> 00:20:57.840
was not trained at any point to give

00:20:57.840 --> 00:20:59.780
correct answers

00:20:59.780 --> 00:21:04.440
gpt4 was trained initially to give most

00:21:04.440 --> 00:21:06.900
likely next words and there's an awful

00:21:06.900 --> 00:21:08.880
lot of stuff on the internet where the

00:21:08.880 --> 00:21:10.860
most rare documents are not describing

00:21:10.860 --> 00:21:12.780
things that are true there could be

00:21:12.780 --> 00:21:15.360
fiction there could be jokes there could

00:21:15.360 --> 00:21:17.220
be just stupid people don't saying dumb

00:21:17.220 --> 00:21:18.080
stuff

00:21:18.080 --> 00:21:21.240
so this first stage does not necessarily

00:21:21.240 --> 00:21:23.700
give you correct answers the second

00:21:23.700 --> 00:21:26.640
stage with the instruction tuning uh

00:21:26.640 --> 00:21:29.159
also like it's it's

00:21:29.159 --> 00:21:31.919
it's trying to give correct answers but

00:21:31.919 --> 00:21:33.720
part of the problem is that then in the

00:21:33.720 --> 00:21:35.419
stage where you start asking people

00:21:35.419 --> 00:21:38.419
which answer do they like better

00:21:38.419 --> 00:21:42.720
people tended to say in these uh in

00:21:42.720 --> 00:21:45.120
these things that they prefer more

00:21:45.120 --> 00:21:48.840
confident answers and they often were

00:21:48.840 --> 00:21:50.340
not people who were trained well enough

00:21:50.340 --> 00:21:52.520
to recognize wrong answers

00:21:52.520 --> 00:21:56.100
so there's lots of reasons that the that

00:21:56.100 --> 00:21:58.440
the you know SGD weight updates from

00:21:58.440 --> 00:22:01.799
this process for stuff like gpt4 don't

00:22:01.799 --> 00:22:04.980
particularly or don't entirely reward

00:22:04.980 --> 00:22:06.720
correct answers

00:22:06.720 --> 00:22:10.380
but you can help it want to give you

00:22:10.380 --> 00:22:12.780
correct answers if you think about

00:22:12.780 --> 00:22:15.480
the LM pre-training what are the kinds

00:22:15.480 --> 00:22:18.299
of things in a document that would

00:22:18.299 --> 00:22:21.000
suggest oh this is going to be high

00:22:21.000 --> 00:22:23.580
quality information and so you can

00:22:23.580 --> 00:22:26.299
actually Prime

00:22:26.299 --> 00:22:29.100
gpt4 to give you high quality

00:22:29.100 --> 00:22:30.659
information by giving it custom

00:22:30.659 --> 00:22:32.820
instructions

00:22:32.820 --> 00:22:36.539
and what this does is this is basically

00:22:36.539 --> 00:22:39.120
text that is prepended to all of your

00:22:39.120 --> 00:22:40.260
queries

00:22:40.260 --> 00:22:43.380
and so you say like oh you're brilliant

00:22:43.380 --> 00:22:46.440
at reasoning so like okay that's

00:22:46.440 --> 00:22:48.419
obviously or to prime it to give good

00:22:48.419 --> 00:22:49.799
answers

00:22:49.799 --> 00:22:50.640
um

00:22:50.640 --> 00:22:52.860
and then try to work against the fact

00:22:52.860 --> 00:22:53.940
that

00:22:53.940 --> 00:22:58.799
um the the rlhf uh folks uh preferred

00:22:58.799 --> 00:23:02.280
confidence just tell it no tell me if

00:23:02.280 --> 00:23:05.120
there might not be a correct answer

00:23:05.120 --> 00:23:08.940
also the way that the text is generated

00:23:08.940 --> 00:23:12.419
is it literally generates the next word

00:23:12.419 --> 00:23:15.240
and then it puts all that whole lot back

00:23:15.240 --> 00:23:16.799
into the bottle and generates the next

00:23:16.799 --> 00:23:18.419
next word puts that all back in the

00:23:18.419 --> 00:23:20.460
model generates the next next word and

00:23:20.460 --> 00:23:21.900
so forth

00:23:21.900 --> 00:23:23.640
that means the more words it generates

00:23:23.640 --> 00:23:26.280
the more computation it can do and so I

00:23:26.280 --> 00:23:28.080
literally I tell it that

00:23:28.080 --> 00:23:30.780
right and so I say first spend a few

00:23:30.780 --> 00:23:33.240
sentences explaining background context

00:23:33.240 --> 00:23:34.740
Etc

00:23:34.740 --> 00:23:39.559
so this uh custom instruction

00:23:40.320 --> 00:23:43.559
um allows it to solve more challenging

00:23:43.559 --> 00:23:46.039
problems

00:23:48.000 --> 00:23:51.140
and you can see the difference

00:23:51.900 --> 00:23:54.419
here's what it looks like for example if

00:23:54.419 --> 00:23:56.159
I say how do I get a count of rows

00:23:56.159 --> 00:23:58.380
grouped by value in pandas

00:23:58.380 --> 00:24:01.020
and it just gives me a whole lot of

00:24:01.020 --> 00:24:02.520
information which is actually it

00:24:02.520 --> 00:24:05.039
thinking so I just skip over it and then

00:24:05.039 --> 00:24:06.360
it gives me the answer

00:24:06.360 --> 00:24:09.960
and actually in my uh

00:24:09.960 --> 00:24:11.580
um

00:24:11.580 --> 00:24:13.620
custom instructions I actually say if

00:24:13.620 --> 00:24:16.020
the request begins with VV

00:24:16.020 --> 00:24:18.900
actually make it as concise as possible

00:24:18.900 --> 00:24:21.780
and so it kind of goes into brief mode

00:24:21.780 --> 00:24:24.000
and here's brief mode how do I get the

00:24:24.000 --> 00:24:25.500
group this is the same thing but with VV

00:24:25.500 --> 00:24:26.520
at the start

00:24:26.520 --> 00:24:29.100
and it just spits it out now in this

00:24:29.100 --> 00:24:30.900
case it's a really simple question so I

00:24:30.900 --> 00:24:32.820
didn't need time to think

00:24:32.820 --> 00:24:35.940
so hopefully that gives you a sense of

00:24:35.940 --> 00:24:39.299
how to get language models to give

00:24:39.299 --> 00:24:40.679
good answers

00:24:40.679 --> 00:24:44.460
you have to help them and if you if it's

00:24:44.460 --> 00:24:46.799
not working it might be user error

00:24:46.799 --> 00:24:49.500
basically but having said that there's

00:24:49.500 --> 00:24:51.539
plenty of stuff that language models

00:24:51.539 --> 00:24:53.720
like gpt4 can't do

00:24:53.720 --> 00:24:57.480
one thing to think carefully about is

00:24:57.480 --> 00:25:00.299
does it know about itself can you ask it

00:25:00.299 --> 00:25:03.480
what is your context length how were you

00:25:03.480 --> 00:25:06.179
trained what Transformer architecture

00:25:06.179 --> 00:25:08.780
are you based on

00:25:09.600 --> 00:25:11.880
any one of these stages

00:25:11.880 --> 00:25:14.039
did it have the opportunity to learn any

00:25:14.039 --> 00:25:15.419
of those things

00:25:15.419 --> 00:25:17.159
well obviously not at the pre-training

00:25:17.159 --> 00:25:19.140
stage nothing on the internet

00:25:19.140 --> 00:25:21.840
existed during GPT 4's training saying

00:25:21.840 --> 00:25:23.820
how gpt4 was trained

00:25:23.820 --> 00:25:26.760
right uh probably Ditto in the

00:25:26.760 --> 00:25:28.559
instruction tuning probably Ditto in the

00:25:28.559 --> 00:25:31.559
rlhf so in general you can't ask for

00:25:31.559 --> 00:25:35.279
example a language model about itself

00:25:35.279 --> 00:25:38.220
now again because of the rlhf it'll want

00:25:38.220 --> 00:25:40.320
to make you happy by giving your

00:25:40.320 --> 00:25:43.380
opinionated answers so it'll just spit

00:25:43.380 --> 00:25:46.140
out the most likely thing it thinks with

00:25:46.140 --> 00:25:48.120
great confidence

00:25:48.120 --> 00:25:49.559
this is just a general kind of

00:25:49.559 --> 00:25:51.659
hallucination right so hallucinations is

00:25:51.659 --> 00:25:54.059
just this idea that the language model

00:25:54.059 --> 00:25:56.640
wants to complete the sentence and it

00:25:56.640 --> 00:25:58.740
wants to do it in an opinionated way

00:25:58.740 --> 00:26:01.679
that's likely to make people happy

00:26:01.679 --> 00:26:02.400
um

00:26:02.400 --> 00:26:05.159
it doesn't know anything about URLs it

00:26:05.159 --> 00:26:08.100
really hasn't seen many at all I think a

00:26:08.100 --> 00:26:09.659
lot of them if not all of them pretty

00:26:09.659 --> 00:26:12.779
much were stripped out so if you ask it

00:26:12.779 --> 00:26:14.520
anything about like what's at this

00:26:14.520 --> 00:26:17.039
webpage again it'll generally just make

00:26:17.039 --> 00:26:18.900
it up

00:26:18.900 --> 00:26:21.000
um and it doesn't know at least gpt4

00:26:21.000 --> 00:26:22.620
doesn't know anything after September

00:26:22.620 --> 00:26:24.120
2021

00:26:24.120 --> 00:26:26.580
um because the

00:26:26.580 --> 00:26:29.760
um information it was pre-trained on was

00:26:29.760 --> 00:26:30.720
from

00:26:30.720 --> 00:26:32.700
that time period September 2021 and

00:26:32.700 --> 00:26:35.520
before called the knowledge cut off

00:26:35.520 --> 00:26:37.620
so here's some things it can't do

00:26:37.620 --> 00:26:38.460
um

00:26:38.460 --> 00:26:40.860
Steve Newman sent me this good example

00:26:40.860 --> 00:26:43.860
of something that it can't do

00:26:43.860 --> 00:26:45.299
um

00:26:45.299 --> 00:26:48.419
here is a logic puzzle I need to carry a

00:26:48.419 --> 00:26:50.880
cabbage a goat and a wolf across a river

00:26:50.880 --> 00:26:54.000
I can only carry one item at a time I

00:26:54.000 --> 00:26:56.279
can't leave the goat with a cabbage I

00:26:56.279 --> 00:26:58.020
can't leave the cabbage with the wolf

00:26:58.020 --> 00:26:59.880
how do I get everything across to the

00:26:59.880 --> 00:27:01.020
other side

00:27:01.020 --> 00:27:04.380
now the problem is this looks a lot like

00:27:04.380 --> 00:27:06.419
something called the classic River

00:27:06.419 --> 00:27:08.940
Crossing puzzle

00:27:08.940 --> 00:27:12.419
so classic in fact that it has a whole

00:27:12.419 --> 00:27:14.400
Wikipedia page

00:27:14.400 --> 00:27:15.900
about it

00:27:15.900 --> 00:27:19.500
and in the classic puzzle

00:27:19.500 --> 00:27:22.080
the wolf would eat the goat

00:27:22.080 --> 00:27:24.900
or the goat would eat the cabbage

00:27:24.900 --> 00:27:26.640
now

00:27:26.640 --> 00:27:29.100
in

00:27:29.100 --> 00:27:33.559
in Steve's version he changed it

00:27:34.559 --> 00:27:37.080
the goat would eat the cabbage and the

00:27:37.080 --> 00:27:38.880
Wolf would eat the cabbage but the wolf

00:27:38.880 --> 00:27:41.720
won't eat the goat

00:27:41.820 --> 00:27:45.080
so what happens well very interestingly

00:27:45.080 --> 00:27:48.480
gpt4 here is entirely overwhelmed by the

00:27:48.480 --> 00:27:50.159
language model training it's seen this

00:27:50.159 --> 00:27:52.320
puzzle so many times it knows what word

00:27:52.320 --> 00:27:53.460
comes next

00:27:53.460 --> 00:27:55.140
so it says oh yeah I take the goat

00:27:55.140 --> 00:27:57.360
across the road across the river and

00:27:57.360 --> 00:27:59.400
leave it on the other side leaving the

00:27:59.400 --> 00:28:01.080
wolf with a cabbage but we're just told

00:28:01.080 --> 00:28:04.320
you can't leave the wolf with a cabbage

00:28:04.320 --> 00:28:07.020
so it gets it wrong

00:28:07.020 --> 00:28:09.240
now the thing is though you can

00:28:09.240 --> 00:28:12.299
encourage gpt4 or any of these language

00:28:12.299 --> 00:28:14.460
models to try again so during the

00:28:14.460 --> 00:28:16.320
instruction tuning an R lhf they're

00:28:16.320 --> 00:28:18.720
actually fine-tuned with multi-stage

00:28:18.720 --> 00:28:20.940
conversations so you can give it a

00:28:20.940 --> 00:28:23.159
multi-stage conversation repeat back to

00:28:23.159 --> 00:28:25.020
me the constraints I listed what

00:28:25.020 --> 00:28:27.179
happened after Step One is a constraint

00:28:27.179 --> 00:28:28.260
violated

00:28:28.260 --> 00:28:32.720
oh yeah yeah yeah I made a mistake okay

00:28:32.720 --> 00:28:35.580
my new attempt instead of taking the

00:28:35.580 --> 00:28:37.020
goat across the river and leaving it on

00:28:37.020 --> 00:28:38.400
the other side is I'll take the code

00:28:38.400 --> 00:28:39.659
across the river and leave from the

00:28:39.659 --> 00:28:41.220
other side

00:28:41.220 --> 00:28:44.640
it's done the same thing

00:28:44.640 --> 00:28:47.820
um oh yeah I did do the same thing okay

00:28:47.820 --> 00:28:50.340
I'll take the wolf across well now the

00:28:50.340 --> 00:28:51.720
goats with the Cabbage that still

00:28:51.720 --> 00:28:53.940
doesn't work oh yeah that didn't work

00:28:53.940 --> 00:28:56.059
out

00:28:56.059 --> 00:28:59.159
uh sorry about that instead of taking

00:28:59.159 --> 00:29:00.539
the goat across the other side I'll take

00:29:00.539 --> 00:29:02.279
the goat across the other side okay

00:29:02.279 --> 00:29:04.320
what's going on here right this is

00:29:04.320 --> 00:29:05.760
terrible

00:29:05.760 --> 00:29:09.059
well one of the problems here is that

00:29:09.059 --> 00:29:11.520
not only is

00:29:11.520 --> 00:29:14.700
on the Internet it's so common to see

00:29:14.700 --> 00:29:17.760
this particular goat puzzle that it's so

00:29:17.760 --> 00:29:19.320
confident it knows what the next word is

00:29:19.320 --> 00:29:22.279
also on the internet when you see stuff

00:29:22.279 --> 00:29:25.860
which is stupid on a web page it's

00:29:25.860 --> 00:29:27.720
really likely to be followed up with

00:29:27.720 --> 00:29:30.539
more stuff that is stupid

00:29:30.539 --> 00:29:33.120
once gpt4

00:29:33.120 --> 00:29:35.820
starts being wrong

00:29:35.820 --> 00:29:39.059
it tends to be more and more wrong it's

00:29:39.059 --> 00:29:41.399
very hard to turn it around to start it

00:29:41.399 --> 00:29:43.820
making it be right

00:29:43.820 --> 00:29:46.919
so you actually have to go back and

00:29:46.919 --> 00:29:51.080
there's actually a an edit button

00:29:51.080 --> 00:29:55.260
on these chats

00:29:55.260 --> 00:29:57.000
um

00:29:57.000 --> 00:29:58.980
and so what you generally want to do is

00:29:58.980 --> 00:30:01.320
if it's made a mistake is don't say oh

00:30:01.320 --> 00:30:03.059
here's more information to help you fix

00:30:03.059 --> 00:30:05.279
it but instead go back and click the

00:30:05.279 --> 00:30:07.620
edit

00:30:07.620 --> 00:30:10.760
and change it here

00:30:16.020 --> 00:30:18.600
and so this time it's not going to get

00:30:18.600 --> 00:30:20.820
confused

00:30:20.820 --> 00:30:23.880
so in this case actually fixing Steve's

00:30:23.880 --> 00:30:25.140
example

00:30:25.140 --> 00:30:27.179
takes quite a lot of effort but I think

00:30:27.179 --> 00:30:28.500
I've managed to get it to work

00:30:28.500 --> 00:30:30.419
eventually and I actually said oh

00:30:30.419 --> 00:30:32.100
sometimes people read things too quickly

00:30:32.100 --> 00:30:34.260
they don't notice things it can trick

00:30:34.260 --> 00:30:37.260
them up then they apply some pattern get

00:30:37.260 --> 00:30:39.360
the wrong answer you do the same thing

00:30:39.360 --> 00:30:41.940
by the way so I'm going to trick you

00:30:41.940 --> 00:30:45.600
so before you about to get tricked make

00:30:45.600 --> 00:30:47.039
sure you don't get tricked here's the

00:30:47.039 --> 00:30:48.419
tricky puzzle

00:30:48.419 --> 00:30:50.279
and then also with my custom

00:30:50.279 --> 00:30:52.799
instructions it takes time

00:30:52.799 --> 00:30:54.539
discussing it

00:30:54.539 --> 00:30:56.520
and this time it gets it correct it

00:30:56.520 --> 00:30:59.340
takes the Cabbage across first so it

00:30:59.340 --> 00:31:02.039
took a lot of effort to get to a point

00:31:02.039 --> 00:31:03.120
where it could actually solve this

00:31:03.120 --> 00:31:05.399
because yeah when it's you know for

00:31:05.399 --> 00:31:07.140
things where

00:31:07.140 --> 00:31:11.100
it's been primed to answer a certain way

00:31:11.100 --> 00:31:13.140
again and again and again it's very hard

00:31:13.140 --> 00:31:16.020
for it to not do that

00:31:16.020 --> 00:31:21.440
okay now uh something else super helpful

00:31:21.440 --> 00:31:23.880
that you can use is what they call

00:31:23.880 --> 00:31:26.279
Advanced Data analysis

00:31:26.279 --> 00:31:29.279
in Advanced Data analysis you can ask it

00:31:29.279 --> 00:31:31.500
to basically write code for you

00:31:31.500 --> 00:31:32.820
and we're going to look at how to

00:31:32.820 --> 00:31:34.140
implement this from scratch ourself

00:31:34.140 --> 00:31:36.240
quite soon but first of all let's learn

00:31:36.240 --> 00:31:39.779
how to use it so I was trying to build

00:31:39.779 --> 00:31:42.480
something that split uh into markdown

00:31:42.480 --> 00:31:44.640
headings a document on third level

00:31:44.640 --> 00:31:47.159
markdown headings so that's uh three

00:31:47.159 --> 00:31:49.140
hashes at the start of a line

00:31:49.140 --> 00:31:51.480
and I was doing it on the whole of

00:31:51.480 --> 00:31:54.000
Wikipedia so using regular Expressions

00:31:54.000 --> 00:31:55.740
was really slow so I said oh I want to

00:31:55.740 --> 00:31:57.419
speed this up

00:31:57.419 --> 00:32:00.419
and it said okay here's some code

00:32:00.419 --> 00:32:01.980
which is great because then I can say

00:32:01.980 --> 00:32:05.580
Okay test it and include edge cases

00:32:05.580 --> 00:32:08.279
and so it then

00:32:08.279 --> 00:32:11.480
puts in the code creates extra cases

00:32:11.480 --> 00:32:14.659
tests it

00:32:15.140 --> 00:32:19.460
says yep it's working

00:32:19.760 --> 00:32:22.260
it's not I notice it's actually removing

00:32:22.260 --> 00:32:23.880
the carriage return at the end of each

00:32:23.880 --> 00:32:26.580
sentence so I said I'll fix that

00:32:26.580 --> 00:32:28.799
and update your tests

00:32:28.799 --> 00:32:30.919
so it said okay

00:32:30.919 --> 00:32:33.419
so now it's changed the test update the

00:32:33.419 --> 00:32:37.679
test cases surround them and oh it's not

00:32:37.679 --> 00:32:38.700
working

00:32:38.700 --> 00:32:40.919
so it says oh yeah

00:32:40.919 --> 00:32:43.860
fix the issue in the test cases

00:32:43.860 --> 00:32:45.539
nope they didn't work

00:32:45.539 --> 00:32:48.539
and you can see it's quite clever the

00:32:48.539 --> 00:32:51.120
way it's trying to fix it by looking at

00:32:51.120 --> 00:32:55.080
the results and but as you can see it's

00:32:55.080 --> 00:32:56.100
not

00:32:56.100 --> 00:32:58.500
every one of these is another attempt

00:32:58.500 --> 00:33:00.000
another attempt another attempt until

00:33:00.000 --> 00:33:02.399
eventually I gave up waiting and it's so

00:33:02.399 --> 00:33:04.760
funny each time it's like debating again

00:33:04.760 --> 00:33:07.740
okay this time I gotta handle it

00:33:07.740 --> 00:33:09.960
properly and I gave up at the point

00:33:09.960 --> 00:33:11.820
where it's like oh one more attempt

00:33:11.820 --> 00:33:14.100
so I didn't solve it

00:33:14.100 --> 00:33:16.740
um interestingly enough and

00:33:16.740 --> 00:33:20.279
you know I I again it's it it's there's

00:33:20.279 --> 00:33:23.700
some limits to the amount of kind of

00:33:23.700 --> 00:33:25.620
logic that it can do this is really a

00:33:25.620 --> 00:33:28.200
very simple question I asked it to do

00:33:28.200 --> 00:33:30.480
for me and so hopefully you can see you

00:33:30.480 --> 00:33:32.399
can't expect

00:33:32.399 --> 00:33:35.220
even GPT for code interpreter or

00:33:35.220 --> 00:33:39.559
Advanced Data analysis is now called to

00:33:39.659 --> 00:33:41.159
make it so you don't have to write code

00:33:41.159 --> 00:33:43.019
anymore you know it's not a substitute

00:33:43.019 --> 00:33:45.360
for having programmers

00:33:45.360 --> 00:33:46.860
um

00:33:46.860 --> 00:33:49.860
um so

00:33:50.159 --> 00:33:52.559
but again you know it it can often do a

00:33:52.559 --> 00:33:54.419
lot as I'll show you in a moment so for

00:33:54.419 --> 00:33:56.399
example actually

00:33:56.399 --> 00:33:59.460
um OCR uh like this is something I

00:33:59.460 --> 00:34:00.779
thought was really cool

00:34:00.779 --> 00:34:02.700
um you can just paste and um sorry

00:34:02.700 --> 00:34:06.360
pastry upload so jpt4 you can upload

00:34:06.360 --> 00:34:08.159
um an image

00:34:08.159 --> 00:34:10.020
um

00:34:10.020 --> 00:34:11.760
Advanced Data analysis yeah you can

00:34:11.760 --> 00:34:14.760
upload an image here

00:34:14.760 --> 00:34:15.720
um

00:34:15.720 --> 00:34:19.320
and then um I wanted to basically grab

00:34:19.320 --> 00:34:20.940
some text out of an image somebody had

00:34:20.940 --> 00:34:22.859
got a screenshot of their screen and I

00:34:22.859 --> 00:34:24.599
wanted to edit which is something saying

00:34:24.599 --> 00:34:26.820
oh uh this language model can't do this

00:34:26.820 --> 00:34:28.859
and I wanted to try it as well so rather

00:34:28.859 --> 00:34:30.839
than retyping it I just uploaded that

00:34:30.839 --> 00:34:32.460
image my screenshot and said can you

00:34:32.460 --> 00:34:34.560
extract the text from this image

00:34:34.560 --> 00:34:36.599
and it said oh yeah I could do that I

00:34:36.599 --> 00:34:38.339
could use OCR

00:34:38.339 --> 00:34:40.980
um and like so it literally wrote at OCR

00:34:40.980 --> 00:34:42.599
script

00:34:42.599 --> 00:34:44.099
and

00:34:44.099 --> 00:34:48.659
there it is just took a few seconds so

00:34:48.659 --> 00:34:50.820
the difference here is it didn't really

00:34:50.820 --> 00:34:54.000
require it to think of much logic it

00:34:54.000 --> 00:34:56.520
could just use a very very familiar

00:34:56.520 --> 00:34:58.020
pattern that it would have seen many

00:34:58.020 --> 00:34:59.339
times

00:34:59.339 --> 00:35:01.200
so this is generally where I find

00:35:01.200 --> 00:35:04.020
language models Excel is where it

00:35:04.020 --> 00:35:05.520
doesn't have to think too far outside

00:35:05.520 --> 00:35:08.099
the box I mean it's great on kind of

00:35:08.099 --> 00:35:10.740
creativity tasks but for like reasoning

00:35:10.740 --> 00:35:12.480
and logic tasks that are outside the box

00:35:12.480 --> 00:35:15.540
I find it not great but yeah it's great

00:35:15.540 --> 00:35:18.300
at doing code for a whole wide variety

00:35:18.300 --> 00:35:20.660
of different libraries and languages

00:35:20.660 --> 00:35:23.480
having said that by the way

00:35:23.480 --> 00:35:25.880
Google also has

00:35:25.880 --> 00:35:29.040
a language model called bad it's way

00:35:29.040 --> 00:35:31.920
less good than gpd4 most of the time but

00:35:31.920 --> 00:35:32.820
there is a nice thing that you can

00:35:32.820 --> 00:35:34.760
literally paste

00:35:34.760 --> 00:35:37.619
an image straight into the prompt and I

00:35:37.619 --> 00:35:39.960
just typed OCR this and it didn't even

00:35:39.960 --> 00:35:41.400
have to go through code interpreter or

00:35:41.400 --> 00:35:43.020
whatever it just said oh sure I've done

00:35:43.020 --> 00:35:44.520
it and

00:35:44.520 --> 00:35:46.800
there's the result of the OCR

00:35:46.800 --> 00:35:48.960
and then it even commented I thought it

00:35:48.960 --> 00:35:50.460
just does yard which I thought was cute

00:35:50.460 --> 00:35:54.119
and oh even more interestingly it even

00:35:54.119 --> 00:35:57.180
figured out where the OCR text came from

00:35:57.180 --> 00:35:59.700
and gave me a link to it

00:35:59.700 --> 00:36:02.579
um that I thought that was pretty cool

00:36:02.579 --> 00:36:05.400
okay so

00:36:05.400 --> 00:36:08.040
there's an example of it doing well I'll

00:36:08.040 --> 00:36:09.960
show you one for this talk I found

00:36:09.960 --> 00:36:12.359
really helpful I wanted to show you guys

00:36:12.359 --> 00:36:18.240
how much it cost to use the open AI API

00:36:18.240 --> 00:36:19.440
um but unfortunately when I went to the

00:36:19.440 --> 00:36:21.000
open AI webpage

00:36:21.000 --> 00:36:24.180
it was like all over the place the

00:36:24.180 --> 00:36:26.339
pricing information was on all Separate

00:36:26.339 --> 00:36:28.200
Tables and it was kind of a bit of a

00:36:28.200 --> 00:36:29.180
mess

00:36:29.180 --> 00:36:32.520
so I wanted to create a table with all

00:36:32.520 --> 00:36:37.020
of the information combined like this

00:36:37.020 --> 00:36:40.820
um and here's how I did it

00:36:42.119 --> 00:36:45.420
I went to the open AI page

00:36:45.420 --> 00:36:48.300
I hit Apple a to select all

00:36:48.300 --> 00:36:51.599
and then I said in chat jpt create a

00:36:51.599 --> 00:36:53.579
table with the pricing information Rose

00:36:53.579 --> 00:36:56.520
no summarization no information not in

00:36:56.520 --> 00:36:58.320
this page every row should appear as a

00:36:58.320 --> 00:36:59.880
separate Row in your output and I hit

00:36:59.880 --> 00:37:00.900
paste

00:37:00.900 --> 00:37:02.520
now that was not very helpful to it

00:37:02.520 --> 00:37:04.079
because hitting paste it's got the nav

00:37:04.079 --> 00:37:05.880
bar it's got

00:37:05.880 --> 00:37:08.280
uh

00:37:08.280 --> 00:37:10.800
lots of extra information at the bottom

00:37:10.800 --> 00:37:14.460
it's got all of its uh footer

00:37:14.460 --> 00:37:16.619
Etc

00:37:16.619 --> 00:37:19.079
um but it's really good at this stuff it

00:37:19.079 --> 00:37:21.480
did it first time so there was the

00:37:21.480 --> 00:37:23.640
markdown table so I copied and pasted

00:37:23.640 --> 00:37:27.000
that into Jupiter and I got my markdown

00:37:27.000 --> 00:37:29.820
table and so now you can see at a glance

00:37:29.820 --> 00:37:34.859
the cost of gpt4 3.5 Etc but then what I

00:37:34.859 --> 00:37:36.599
really wanted to do was show you that is

00:37:36.599 --> 00:37:38.280
a picture

00:37:38.280 --> 00:37:41.339
so I just said oh chart the input Row

00:37:41.339 --> 00:37:42.900
from this table and just paste to the

00:37:42.900 --> 00:37:44.460
table back

00:37:44.460 --> 00:37:46.260
um

00:37:46.260 --> 00:37:47.940
and it did

00:37:47.940 --> 00:37:50.280
so that's pretty amazing now so let's

00:37:50.280 --> 00:37:54.240
talk about this um pricing so so far

00:37:54.240 --> 00:37:56.400
we've used chat GPT which costs 20 bucks

00:37:56.400 --> 00:37:59.160
a month and there's no like per token

00:37:59.160 --> 00:38:00.960
cost or anything but if you want to use

00:38:00.960 --> 00:38:03.119
the API from python or whatever you have

00:38:03.119 --> 00:38:06.000
to pay per token which is approximately

00:38:06.000 --> 00:38:09.000
per word maybe it's about uh

00:38:09.000 --> 00:38:11.280
one and a third tokens per word on

00:38:11.280 --> 00:38:12.660
average

00:38:12.660 --> 00:38:14.760
unfortunately in the chart it did not

00:38:14.760 --> 00:38:17.579
include these headers gpt4 GPT 3.5 so

00:38:17.579 --> 00:38:19.619
these first two ones are gpt4

00:38:19.619 --> 00:38:22.440
and these two are GPT 3.5 so you can see

00:38:22.440 --> 00:38:27.140
the GPT 3.5 is way way cheaper

00:38:27.540 --> 00:38:30.060
um and you can see it here it's 0.03

00:38:30.060 --> 00:38:33.480
versus 0.0015

00:38:34.280 --> 00:38:36.119
so

00:38:36.119 --> 00:38:38.820
it's so cheap you can really play around

00:38:38.820 --> 00:38:41.220
with it and not worry and I want to give

00:38:41.220 --> 00:38:43.020
you a sense of what that looks like

00:38:43.020 --> 00:38:45.660
Okay so

00:38:45.660 --> 00:38:49.380
why would you use the open AI API rather

00:38:49.380 --> 00:38:50.880
than chat GPT

00:38:50.880 --> 00:38:52.920
because you can do it programmatically

00:38:52.920 --> 00:38:56.099
so you can

00:38:56.099 --> 00:38:59.160
you know you can analyze data sets you

00:38:59.160 --> 00:39:01.820
can do repetitive stuff

00:39:01.820 --> 00:39:03.839
it's kind of like a different way of

00:39:03.839 --> 00:39:06.000
programming you know it's it's things

00:39:06.000 --> 00:39:09.000
that you can think of describing

00:39:09.000 --> 00:39:10.800
but let's just look at the most simple

00:39:10.800 --> 00:39:12.540
example of what that looks like so if

00:39:12.540 --> 00:39:14.339
your pip install open AI

00:39:14.339 --> 00:39:17.640
then you can import check and chat

00:39:17.640 --> 00:39:19.800
completion and then you can say Okay

00:39:19.800 --> 00:39:23.220
chat completion.create using GPT 3.5

00:39:23.220 --> 00:39:24.720
Turbo

00:39:24.720 --> 00:39:27.119
and then you can pass in a system

00:39:27.119 --> 00:39:29.099
message this is basically the same as

00:39:29.099 --> 00:39:31.500
custom instructions so okay you're an

00:39:31.500 --> 00:39:33.359
Aussie llm that uses Aussie slang and

00:39:33.359 --> 00:39:35.339
analogies wherever possible

00:39:35.339 --> 00:39:37.440
okay and so you can see I'm passing in

00:39:37.440 --> 00:39:39.540
an array here of messages so the first

00:39:39.540 --> 00:39:42.000
is the system message and then the user

00:39:42.000 --> 00:39:42.780
message

00:39:42.780 --> 00:39:45.180
which is what is money

00:39:45.180 --> 00:39:49.680
okay so GPT 3.5 returns a big embedded

00:39:49.680 --> 00:39:52.560
dictionary

00:39:52.560 --> 00:39:56.579
um and the message content is well my

00:39:56.579 --> 00:39:58.440
money is like the oil that keeps the

00:39:58.440 --> 00:40:00.000
Machinery of our economy running

00:40:00.000 --> 00:40:02.520
smoothly

00:40:02.520 --> 00:40:04.859
there you go just like a koala loves its

00:40:04.859 --> 00:40:06.480
eucalyptus leaves we humans can't

00:40:06.480 --> 00:40:08.640
survive without this stuff

00:40:08.640 --> 00:40:10.800
so there's the Aussie llm's view of what

00:40:10.800 --> 00:40:12.960
is money

00:40:12.960 --> 00:40:14.640
so

00:40:14.640 --> 00:40:17.460
the really uh the main ones I pretty

00:40:17.460 --> 00:40:22.400
much always use are gpt4 and GPT 3.5

00:40:22.400 --> 00:40:26.040
gpd4 is just so so much better at

00:40:26.040 --> 00:40:28.020
anything remotely

00:40:28.020 --> 00:40:30.300
challenging but obviously it's much more

00:40:30.300 --> 00:40:32.760
expensive so rule of thumb you know

00:40:32.760 --> 00:40:35.579
maybe try 3.5 turbo first

00:40:35.579 --> 00:40:37.680
see how it goes if you're happy with the

00:40:37.680 --> 00:40:39.300
results then great if you're not

00:40:39.300 --> 00:40:42.720
planning out for the more expensive one

00:40:42.720 --> 00:40:45.060
okay so I just created a little function

00:40:45.060 --> 00:40:47.940
here called response that will print out

00:40:47.940 --> 00:40:48.480
um

00:40:48.480 --> 00:40:51.540
this nested thing

00:40:51.540 --> 00:40:52.200
um

00:40:52.200 --> 00:40:54.599
and so now oh and so then the other

00:40:54.599 --> 00:40:56.940
thing to point out here is that the

00:40:56.940 --> 00:41:00.480
result of this also has a usage field

00:41:00.480 --> 00:41:04.200
which contains how many tokens was it

00:41:04.200 --> 00:41:06.420
so it's about 150 tokens

00:41:06.420 --> 00:41:10.740
so at point zero zero two

00:41:10.740 --> 00:41:14.460
dollars per thousand tokens

00:41:14.460 --> 00:41:16.859
for 150 tokens

00:41:16.859 --> 00:41:20.720
means we just paid

00:41:20.720 --> 00:41:24.240
.03 cents point zero zero zero three

00:41:24.240 --> 00:41:26.880
dollars uh to get that done so as you

00:41:26.880 --> 00:41:29.700
can see the cost is insignificant if we

00:41:29.700 --> 00:41:34.200
were using gpt4 it would be 0.03 per

00:41:34.200 --> 00:41:35.220
thousand

00:41:35.220 --> 00:41:36.660
so it would be

00:41:36.660 --> 00:41:38.700
half a cent

00:41:38.700 --> 00:41:39.599
um

00:41:39.599 --> 00:41:42.480
so unless you're doing

00:41:42.480 --> 00:41:45.180
many thousands of gpt4 you're not going

00:41:45.180 --> 00:41:48.300
to be even up into the dollars and GPT

00:41:48.300 --> 00:41:50.820
3.5 even more than that

00:41:50.820 --> 00:41:53.220
but you know keep an eye on it open AI

00:41:53.220 --> 00:41:55.440
has a usage page and you can track your

00:41:55.440 --> 00:41:56.520
usage

00:41:56.520 --> 00:41:57.900
now

00:41:57.900 --> 00:42:00.599
happens when we are this is really

00:42:00.599 --> 00:42:02.280
important to understand

00:42:02.280 --> 00:42:05.640
when we have a follow-up

00:42:05.640 --> 00:42:07.619
in the same conversation

00:42:07.619 --> 00:42:10.520
how does that work

00:42:11.460 --> 00:42:15.000
so we just asked what goat means so for

00:42:15.000 --> 00:42:17.640
example Michael Jordan is often referred

00:42:17.640 --> 00:42:19.079
to as

00:42:19.079 --> 00:42:22.260
the goat for his exceptional skills and

00:42:22.260 --> 00:42:25.020
accomplishments

00:42:25.020 --> 00:42:27.119
and Elvis and The Beatles referred to as

00:42:27.119 --> 00:42:28.560
goat due to their profound influence and

00:42:28.560 --> 00:42:29.760
achievement

00:42:29.760 --> 00:42:32.960
so I could say

00:42:34.980 --> 00:42:36.720
what

00:42:36.720 --> 00:42:38.579
profound influence and achievements are

00:42:38.579 --> 00:42:42.619
you referring to

00:42:44.880 --> 00:42:47.220
okay well I meant Elvis Presley and the

00:42:47.220 --> 00:42:49.260
Beatles did all these things now how

00:42:49.260 --> 00:42:50.760
does that work how does this follow-up

00:42:50.760 --> 00:42:53.880
work well what happens is the entire

00:42:53.880 --> 00:42:57.359
conversation is passed back

00:42:57.359 --> 00:43:00.240
and so we can actually do that here so

00:43:00.240 --> 00:43:02.520
here is the same

00:43:02.520 --> 00:43:06.359
system prompt here is the same question

00:43:06.359 --> 00:43:08.760
right and then the answer comes back

00:43:08.760 --> 00:43:10.560
with role assistant and I'm going to do

00:43:10.560 --> 00:43:12.119
something pretty cheeky

00:43:12.119 --> 00:43:13.859
I'm going to pretend

00:43:13.859 --> 00:43:15.660
that it didn't say

00:43:15.660 --> 00:43:18.660
money is like oil I'm going to say oh

00:43:18.660 --> 00:43:20.760
you actually said money is like

00:43:20.760 --> 00:43:21.900
kangaroos

00:43:21.900 --> 00:43:24.900
I thought what it's going to do okay so

00:43:24.900 --> 00:43:27.420
you can like literally invent a

00:43:27.420 --> 00:43:29.099
conversation in which the language model

00:43:29.099 --> 00:43:31.440
said something different because this is

00:43:31.440 --> 00:43:33.180
actually how it's done in a multi-stage

00:43:33.180 --> 00:43:36.060
conversation there's no state

00:43:36.060 --> 00:43:37.980
right there's nothing stored on the

00:43:37.980 --> 00:43:40.200
server you're passing back the entire

00:43:40.200 --> 00:43:43.319
conversation again and telling it what

00:43:43.319 --> 00:43:44.640
it told you

00:43:44.640 --> 00:43:47.040
right so I'm going to tell it it's it

00:43:47.040 --> 00:43:49.020
told me that money is like kangaroos and

00:43:49.020 --> 00:43:51.359
then I'll ask the user oh really in what

00:43:51.359 --> 00:43:53.280
way and this is kind of cool because you

00:43:53.280 --> 00:43:57.000
can like see how it convinces you of of

00:43:57.000 --> 00:43:59.579
something I just invented oh let me

00:43:59.579 --> 00:44:01.140
break it down for you cover it just like

00:44:01.140 --> 00:44:02.760
kangaroos hop around and carry their

00:44:02.760 --> 00:44:04.800
Joeys in their pouch money is a means of

00:44:04.800 --> 00:44:07.020
carrying value around so there you go

00:44:07.020 --> 00:44:10.380
it's uh make your own analogy

00:44:10.380 --> 00:44:13.440
cool so I'll create a little function

00:44:13.440 --> 00:44:15.660
here that just puts these things

00:44:15.660 --> 00:44:18.420
together for us just a message if there

00:44:18.420 --> 00:44:20.280
is one the user message and returns

00:44:20.280 --> 00:44:22.380
they're completion

00:44:22.380 --> 00:44:24.180
and so now we can ask it what's the

00:44:24.180 --> 00:44:25.619
meaning of life

00:44:25.619 --> 00:44:28.859
passing in the Aussie system prompt

00:44:28.859 --> 00:44:30.720
the meaning of life is like trying to

00:44:30.720 --> 00:44:32.460
catch a wave on a sunny day at Bondi

00:44:32.460 --> 00:44:35.460
Beach okay there you go so

00:44:35.460 --> 00:44:37.500
um what do you need to be aware of

00:44:37.500 --> 00:44:39.119
um well as I said one thing is keep an

00:44:39.119 --> 00:44:41.460
eye on your usage if you're doing it you

00:44:41.460 --> 00:44:43.440
know hundreds or thousands of times in a

00:44:43.440 --> 00:44:46.319
loop keep an eye on not spending too

00:44:46.319 --> 00:44:49.079
much money but also if you're doing it

00:44:49.079 --> 00:44:50.940
too fast particularly the first day or

00:44:50.940 --> 00:44:53.579
two you've got an account you're likely

00:44:53.579 --> 00:44:58.560
to hit the limits for the API and so the

00:44:58.560 --> 00:45:02.520
limits initially are pretty low

00:45:02.520 --> 00:45:06.599
as you can see three requests per minute

00:45:06.599 --> 00:45:08.660
um

00:45:09.720 --> 00:45:11.940
so that's for free users page users

00:45:11.940 --> 00:45:15.000
First 48 hours and after that it starts

00:45:15.000 --> 00:45:16.319
going up and you can always ask for more

00:45:16.319 --> 00:45:19.560
I just mentioned this because you're

00:45:19.560 --> 00:45:21.180
going to want to have a function that

00:45:21.180 --> 00:45:24.300
keeps an eye on that and so what I did

00:45:24.300 --> 00:45:26.579
is I actually just went to Bing which

00:45:26.579 --> 00:45:29.940
has a somewhat crappy version of gpt4

00:45:29.940 --> 00:45:31.859
nowadays but it can still do basic stuff

00:45:31.859 --> 00:45:34.760
for free and I said please show me

00:45:34.760 --> 00:45:39.300
python code to call the open AI API

00:45:39.300 --> 00:45:41.460
and handle rate limits

00:45:41.460 --> 00:45:44.040
and it wrote this code

00:45:44.040 --> 00:45:45.780
it's got to try

00:45:45.780 --> 00:45:47.760
checks for rate limit errors

00:45:47.760 --> 00:45:51.300
grabs the retry after

00:45:51.300 --> 00:45:54.900
sleeps for that long and calls itself

00:45:54.900 --> 00:45:57.240
and so now we can use that to ask for

00:45:57.240 --> 00:46:00.420
example what's the world's funniest joke

00:46:00.420 --> 00:46:03.140
and

00:46:03.140 --> 00:46:05.640
there we go is the world's funniest

00:46:05.640 --> 00:46:07.920
trick

00:46:07.920 --> 00:46:10.800
so there's like the basic stuff you need

00:46:10.800 --> 00:46:15.420
to get started using the open AI

00:46:15.420 --> 00:46:17.700
llms

00:46:17.700 --> 00:46:20.460
um and uh

00:46:20.460 --> 00:46:22.380
and yeah I'd definitely suggest spending

00:46:22.380 --> 00:46:23.760
plenty of time

00:46:23.760 --> 00:46:26.160
with that so that you feel like you're

00:46:26.160 --> 00:46:28.380
really a llm

00:46:28.380 --> 00:46:29.819
using

00:46:29.819 --> 00:46:32.480
expert

00:46:33.599 --> 00:46:35.400
so what else can we do

00:46:35.400 --> 00:46:37.440
well let's create our own code

00:46:37.440 --> 00:46:40.579
interpreter that runs inside Jupiter

00:46:40.579 --> 00:46:43.859
and so to do this we're going to take

00:46:43.859 --> 00:46:46.099
advantage of a really Nifty thing

00:46:46.099 --> 00:46:49.500
called function calling which is

00:46:49.500 --> 00:46:52.319
provided by the open AI API and in

00:46:52.319 --> 00:46:54.599
function calling when we call our ask

00:46:54.599 --> 00:46:56.640
GPT function

00:46:56.640 --> 00:46:59.099
which is this little one here

00:46:59.099 --> 00:47:01.319
we had room to pass in some keyword

00:47:01.319 --> 00:47:03.480
arguments that will be just passed along

00:47:03.480 --> 00:47:05.579
to chat completion.create

00:47:05.579 --> 00:47:07.980
and one of those keyword arguments you

00:47:07.980 --> 00:47:10.859
can pass is

00:47:10.859 --> 00:47:12.420
functions

00:47:12.420 --> 00:47:16.560
what on Earth is that functions tells

00:47:16.560 --> 00:47:18.300
open AI

00:47:18.300 --> 00:47:19.560
about

00:47:19.560 --> 00:47:22.440
tools that you have about functions that

00:47:22.440 --> 00:47:24.660
you have so for example I created a

00:47:24.660 --> 00:47:28.619
really simple function called sums

00:47:28.619 --> 00:47:30.839
and it adds

00:47:30.839 --> 00:47:33.420
two things

00:47:33.420 --> 00:47:36.680
in fact it adds two it's

00:47:37.380 --> 00:47:40.140
um and I'm going to pass that function

00:47:40.140 --> 00:47:43.140
to

00:47:44.240 --> 00:47:45.960
chatcompletion.create

00:47:45.960 --> 00:47:48.960
now you can't pass a python function

00:47:48.960 --> 00:47:50.760
directly you actually have to pass

00:47:50.760 --> 00:47:54.359
What's called the Json schema so you

00:47:54.359 --> 00:47:56.540
have to pass the schema for the function

00:47:56.540 --> 00:47:59.700
so I created this Nifty little function

00:47:59.700 --> 00:48:01.400
that you're welcome to borrow

00:48:01.400 --> 00:48:05.300
which uses pedantic

00:48:05.300 --> 00:48:08.880
and also Python's inspect module to

00:48:08.880 --> 00:48:13.140
automatically take a python function and

00:48:13.140 --> 00:48:14.220
return

00:48:14.220 --> 00:48:16.440
the schema for it and so this is

00:48:16.440 --> 00:48:17.579
actually what's going to get passed to

00:48:17.579 --> 00:48:18.960
open AI so it's going to know that

00:48:18.960 --> 00:48:20.280
there's a function called sums it's

00:48:20.280 --> 00:48:21.960
going to know what it does

00:48:21.960 --> 00:48:24.240
and it's going to know what parameters

00:48:24.240 --> 00:48:25.560
it takes

00:48:25.560 --> 00:48:28.140
what the defaults are and what's

00:48:28.140 --> 00:48:29.640
required

00:48:29.640 --> 00:48:32.220
so this is like when I first heard about

00:48:32.220 --> 00:48:34.140
this I found this a bit mind-bending

00:48:34.140 --> 00:48:35.880
because this is so different to how we

00:48:35.880 --> 00:48:38.700
normally program computers where the key

00:48:38.700 --> 00:48:40.560
thing for programming the computer

00:48:40.560 --> 00:48:42.900
here actually is the doc string this is

00:48:42.900 --> 00:48:45.960
the thing that gpt4 will look at and say

00:48:45.960 --> 00:48:48.420
oh what does this function do so it's

00:48:48.420 --> 00:48:50.160
critical that this describes exactly

00:48:50.160 --> 00:48:52.020
what the function does

00:48:52.020 --> 00:48:55.920
and so if I then say

00:48:55.920 --> 00:48:57.240
um

00:48:57.240 --> 00:49:00.060
what is six plus three

00:49:00.060 --> 00:49:02.040
right and I just I really wanted to make

00:49:02.040 --> 00:49:04.200
sure it actually did it here so I gave

00:49:04.200 --> 00:49:06.000
it lots of prompts to say because

00:49:06.000 --> 00:49:08.099
obviously it knows how to do it itself

00:49:08.099 --> 00:49:11.099
without calling sums so it'll only use

00:49:11.099 --> 00:49:14.040
your functions if it feels it needs to

00:49:14.040 --> 00:49:16.380
which is a weird concept I mean I guess

00:49:16.380 --> 00:49:19.319
feels is not a great word to use but you

00:49:19.319 --> 00:49:20.940
kind of have to anthropomorphize these

00:49:20.940 --> 00:49:21.960
things a little bit because they don't

00:49:21.960 --> 00:49:25.380
behave like normal computer programs

00:49:25.380 --> 00:49:29.640
um so if I if I ask GPT what is six plus

00:49:29.640 --> 00:49:31.319
three and tell it that there's a

00:49:31.319 --> 00:49:33.960
function called sums then it does not

00:49:33.960 --> 00:49:36.900
actually return the number nine

00:49:36.900 --> 00:49:39.060
instead it returns something saying

00:49:39.060 --> 00:49:40.980
please call a function

00:49:40.980 --> 00:49:45.060
call this function and pass it these

00:49:45.060 --> 00:49:47.040
arguments so if I print it out there's

00:49:47.040 --> 00:49:49.560
the arguments

00:49:49.560 --> 00:49:51.660
so I created a little function called

00:49:51.660 --> 00:49:54.780
core function and it goes into the

00:49:54.780 --> 00:49:58.319
result of open AI grabs the function

00:49:58.319 --> 00:49:59.839
call

00:49:59.839 --> 00:50:02.040
checks that the name is something that

00:50:02.040 --> 00:50:03.300
it's allowed to do

00:50:03.300 --> 00:50:06.300
grabs it from the global system table

00:50:06.300 --> 00:50:08.160
and calls it

00:50:08.160 --> 00:50:10.560
passing in the parameters

00:50:10.560 --> 00:50:12.839
and so if I now say okay call the

00:50:12.839 --> 00:50:15.859
function that

00:50:16.200 --> 00:50:17.760
we got back

00:50:17.760 --> 00:50:20.040
we finally get

00:50:20.040 --> 00:50:22.500
nine

00:50:22.500 --> 00:50:23.880
so

00:50:23.880 --> 00:50:25.859
this is a very simple example it's not

00:50:25.859 --> 00:50:27.480
really doing anything that useful but

00:50:27.480 --> 00:50:29.819
what we could do now is we can create a

00:50:29.819 --> 00:50:32.099
much more powerful function

00:50:32.099 --> 00:50:33.780
called

00:50:33.780 --> 00:50:35.339
python

00:50:35.339 --> 00:50:38.099
and the python function

00:50:38.099 --> 00:50:40.079
executes

00:50:40.079 --> 00:50:41.760
code

00:50:41.760 --> 00:50:43.740
using python

00:50:43.740 --> 00:50:46.500
and Returns the result

00:50:46.500 --> 00:50:49.859
now of course I didn't want my computer

00:50:49.859 --> 00:50:53.400
to run arbitrary python code that gpt4

00:50:53.400 --> 00:50:55.680
told it to without checking so I just

00:50:55.680 --> 00:50:57.660
got it to check first so say oh you're

00:50:57.660 --> 00:51:00.319
sure you want to do this

00:51:00.599 --> 00:51:01.440
um

00:51:01.440 --> 00:51:03.599
so now

00:51:03.599 --> 00:51:05.640
I can say

00:51:05.640 --> 00:51:09.300
ask GPT what is 12 factorial

00:51:09.300 --> 00:51:11.640
system prompt you can use Python for any

00:51:11.640 --> 00:51:13.440
required computations and say okay

00:51:13.440 --> 00:51:14.720
here's a function you've got available

00:51:14.720 --> 00:51:18.059
it's the python function

00:51:18.059 --> 00:51:21.480
so if I now call this

00:51:21.480 --> 00:51:24.720
it will pass me back again

00:51:24.720 --> 00:51:26.520
a completion object and here it's going

00:51:26.520 --> 00:51:28.740
to say okay I want you to call python

00:51:28.740 --> 00:51:30.420
passing in

00:51:30.420 --> 00:51:33.260
this argument

00:51:33.300 --> 00:51:35.579
and when I do it's going to go import

00:51:35.579 --> 00:51:37.859
math result equals blur and then return

00:51:37.859 --> 00:51:43.079
result do I want to do that yes I do

00:51:43.079 --> 00:51:45.599
and there it is

00:51:45.599 --> 00:51:48.359
now there's one more step which we can

00:51:48.359 --> 00:51:49.559
optionally do I mean we've got the

00:51:49.559 --> 00:51:51.900
answer we wanted but often we want the

00:51:51.900 --> 00:51:54.480
answer in more of a chat format and so

00:51:54.480 --> 00:51:57.359
the way to do that is to again repeat

00:51:57.359 --> 00:52:01.380
everything that you've passed into so

00:52:01.380 --> 00:52:02.520
far

00:52:02.520 --> 00:52:04.980
but then instead of adding in

00:52:04.980 --> 00:52:07.319
an assistant role response we have to

00:52:07.319 --> 00:52:10.619
provide a function role response and

00:52:10.619 --> 00:52:12.780
simply

00:52:12.780 --> 00:52:15.240
put in here the result we got back from

00:52:15.240 --> 00:52:16.319
the function

00:52:16.319 --> 00:52:19.700
and if we do that

00:52:21.059 --> 00:52:22.920
we now get

00:52:22.920 --> 00:52:25.380
the prose response 12 factorial is equal

00:52:25.380 --> 00:52:26.220
to

00:52:26.220 --> 00:52:30.000
470 and a million 1 600.

00:52:30.000 --> 00:52:31.319
now

00:52:31.319 --> 00:52:33.720
functions

00:52:33.720 --> 00:52:37.680
like python you can still ask it about

00:52:37.680 --> 00:52:40.440
non-python things

00:52:40.440 --> 00:52:43.260
and it just ignores it if you don't need

00:52:43.260 --> 00:52:45.000
it right so you can have a whole bunch

00:52:45.000 --> 00:52:47.700
of functions available that you've built

00:52:47.700 --> 00:52:51.359
to do whatever you need for the stuff

00:52:51.359 --> 00:52:52.920
which

00:52:52.920 --> 00:52:54.180
um

00:52:54.180 --> 00:52:56.480
the language model isn't familiar with

00:52:56.480 --> 00:52:59.000
and it'll still

00:52:59.000 --> 00:53:02.400
solve whatever it can on its own and use

00:53:02.400 --> 00:53:05.700
your tools use your functions where

00:53:05.700 --> 00:53:08.119
possible

00:53:08.520 --> 00:53:13.380
okay so we have built our own

00:53:13.380 --> 00:53:15.359
code interpreter from scratch I think

00:53:15.359 --> 00:53:18.079
that's pretty amazing

00:53:22.200 --> 00:53:24.240
so that is

00:53:24.240 --> 00:53:27.780
um what you can do with or some of the

00:53:27.780 --> 00:53:32.220
stuff you can do with open AI

00:53:32.220 --> 00:53:34.980
um what about stuff that you can do on

00:53:34.980 --> 00:53:36.720
your own computer

00:53:36.720 --> 00:53:39.540
well to use a language model on your own

00:53:39.540 --> 00:53:42.420
computer you're going to need to use

00:53:42.420 --> 00:53:45.059
a GPU

00:53:45.059 --> 00:53:47.099
um so I guess the first thing to think

00:53:47.099 --> 00:53:48.780
about is like

00:53:48.780 --> 00:53:52.020
do you want this does it make sense to

00:53:52.020 --> 00:53:55.800
do stuff on your own computer what are

00:53:55.800 --> 00:53:57.660
the benefits

00:53:57.660 --> 00:53:58.500
um

00:53:58.500 --> 00:54:01.319
there are not any open source models

00:54:01.319 --> 00:54:06.079
that are as good yet as gpt4

00:54:06.540 --> 00:54:08.099
and I would have to say also like

00:54:08.099 --> 00:54:10.260
actually open ai's pricing's really

00:54:10.260 --> 00:54:13.380
pretty good so it's it's not immediately

00:54:13.380 --> 00:54:16.260
obvious that you definitely want to kind

00:54:16.260 --> 00:54:18.059
of go in-house but there's lots of

00:54:18.059 --> 00:54:20.700
reasons you might want to and we'll look

00:54:20.700 --> 00:54:24.260
at some examples of them today

00:54:24.260 --> 00:54:26.099
one example you might want to go

00:54:26.099 --> 00:54:29.760
in-house is that you want to be able to

00:54:29.760 --> 00:54:33.180
ask questions about your proprietary

00:54:33.180 --> 00:54:35.760
documents or about information after

00:54:35.760 --> 00:54:38.540
September 2021 the the knowledge cut off

00:54:38.540 --> 00:54:40.800
or you might want to create your own

00:54:40.800 --> 00:54:42.660
model that's particularly good at

00:54:42.660 --> 00:54:44.160
solving the kinds of problems that you

00:54:44.160 --> 00:54:46.859
need to solve using fine tuning and

00:54:46.859 --> 00:54:48.240
these are all things that you absolutely

00:54:48.240 --> 00:54:50.900
can get better than GPT for performance

00:54:50.900 --> 00:54:54.020
at work or at home without too much

00:54:54.020 --> 00:54:57.480
without too much money or travel so

00:54:57.480 --> 00:54:58.980
these are the situations in which you

00:54:58.980 --> 00:55:01.380
might want to go down this path and so

00:55:01.380 --> 00:55:03.920
you don't necessarily have to buy a GPU

00:55:03.920 --> 00:55:07.079
on kaggle they will give you a notebook

00:55:07.079 --> 00:55:08.339
with two

00:55:08.339 --> 00:55:10.800
quite old gpus attached

00:55:10.800 --> 00:55:13.940
and very little Ram but it's something

00:55:13.940 --> 00:55:17.700
or you can use collab and on collab you

00:55:17.700 --> 00:55:21.059
can get much better gpus than kaggle has

00:55:21.059 --> 00:55:24.839
and more RAM particularly if you pay a

00:55:24.839 --> 00:55:27.839
monthly subscription fee

00:55:27.839 --> 00:55:28.440
um

00:55:28.440 --> 00:55:31.680
so those are some options for free or

00:55:31.680 --> 00:55:33.140
low cost

00:55:33.140 --> 00:55:37.040
you can also of course

00:55:37.380 --> 00:55:41.359
you know go to one of the many kind of

00:55:41.359 --> 00:55:45.359
GPU server providers and they change all

00:55:45.359 --> 00:55:47.880
the time is to kind of what's what's

00:55:47.880 --> 00:55:51.000
good or what's not run pod is one

00:55:51.000 --> 00:55:52.220
example

00:55:52.220 --> 00:55:56.099
and you can see you know if you want the

00:55:56.099 --> 00:55:58.920
biggest and best machine you're talking

00:55:58.920 --> 00:56:01.940
34 an hour so it gets pretty expensive

00:56:01.940 --> 00:56:04.260
but you can certainly get things a lot

00:56:04.260 --> 00:56:05.220
cheaper

00:56:05.220 --> 00:56:07.619
80 cents an hour

00:56:07.619 --> 00:56:11.839
um Lambda Labs is often pretty good

00:56:14.579 --> 00:56:15.960
um you know it's really hard at the

00:56:15.960 --> 00:56:18.660
moment to actually find

00:56:18.660 --> 00:56:20.780
um

00:56:22.020 --> 00:56:23.880
let's see pricing to actually find

00:56:23.880 --> 00:56:25.740
people that have them available so

00:56:25.740 --> 00:56:27.660
they've got lots listed here but they

00:56:27.660 --> 00:56:32.280
often have nine or very few available

00:56:32.280 --> 00:56:34.079
um there's also something pretty

00:56:34.079 --> 00:56:36.660
interesting called Fast AI

00:56:36.660 --> 00:56:40.740
which basically lets you use

00:56:40.740 --> 00:56:41.400
um

00:56:41.400 --> 00:56:43.440
other people's

00:56:43.440 --> 00:56:46.040
computers when they're not using them

00:56:46.040 --> 00:56:49.260
and as you can see

00:56:49.260 --> 00:56:53.480
you know they tend to be much cheaper

00:56:53.520 --> 00:56:55.200
than other folks

00:56:55.200 --> 00:56:56.880
and they they tend to have better

00:56:56.880 --> 00:56:58.980
availability as well but of course for

00:56:58.980 --> 00:57:00.180
sensitive stuff you don't want to be

00:57:00.180 --> 00:57:02.339
running it on some randos computer so

00:57:02.339 --> 00:57:03.900
anyway so there's a few options for

00:57:03.900 --> 00:57:05.700
renting stuff

00:57:05.700 --> 00:57:07.380
um you know I think it's if you can it's

00:57:07.380 --> 00:57:09.059
worth buying something and definitely

00:57:09.059 --> 00:57:11.400
the one to buy at the moment is the GTX

00:57:11.400 --> 00:57:13.460
3090 used

00:57:13.460 --> 00:57:16.200
you can generally get them from eBay for

00:57:16.200 --> 00:57:19.200
like 700 bucks or so

00:57:19.200 --> 00:57:22.200
um a 40 90 isn't really better for

00:57:22.200 --> 00:57:24.720
language models even though it's a newer

00:57:24.720 --> 00:57:27.359
GPU the reason for that is that language

00:57:27.359 --> 00:57:30.660
models are all about memory speed how

00:57:30.660 --> 00:57:32.160
quickly can you get in and stuff in and

00:57:32.160 --> 00:57:33.780
out of memory rather than how fast is

00:57:33.780 --> 00:57:35.400
the processor and that hasn't really

00:57:35.400 --> 00:57:37.099
improved a whole lot

00:57:37.099 --> 00:57:40.859
so the two thousand bucks hmm the other

00:57:40.859 --> 00:57:42.480
thing as well as memory speed is memory

00:57:42.480 --> 00:57:43.559
size

00:57:43.559 --> 00:57:46.680
24 gigs it doesn't quite cut it for a

00:57:46.680 --> 00:57:48.000
lot of things so you'd probably want to

00:57:48.000 --> 00:57:50.579
get two of these gpus so you're talking

00:57:50.579 --> 00:57:53.579
like fifteen hundred dollars or so

00:57:53.579 --> 00:57:57.960
um or you can get a 48 gig ram GPU it's

00:57:57.960 --> 00:58:01.140
called an a6000 but this is going to

00:58:01.140 --> 00:58:03.319
cost you more like five grand

00:58:03.319 --> 00:58:06.839
so again getting two of these is going

00:58:06.839 --> 00:58:09.480
to be a better deal and this is not

00:58:09.480 --> 00:58:12.720
going to be faster than these either

00:58:12.720 --> 00:58:13.859
um

00:58:13.859 --> 00:58:15.599
or funnily enough you could just get a

00:58:15.599 --> 00:58:17.819
Mac with a lot of ram particularly if

00:58:17.819 --> 00:58:20.059
you get an M2 Ultra

00:58:20.059 --> 00:58:22.859
Max have

00:58:22.859 --> 00:58:23.460
um

00:58:23.460 --> 00:58:25.380
particularly the M2 Ultra has pretty

00:58:25.380 --> 00:58:27.240
fast memory it's still going to be way

00:58:27.240 --> 00:58:30.180
slower than using an Nvidia card but

00:58:30.180 --> 00:58:31.440
it's going to be like you're going to be

00:58:31.440 --> 00:58:34.319
able to get you know like I think 192

00:58:34.319 --> 00:58:36.359
gig or something

00:58:36.359 --> 00:58:37.260
um

00:58:37.260 --> 00:58:39.859
so

00:58:40.020 --> 00:58:42.119
it's not a terrible option particularly

00:58:42.119 --> 00:58:43.500
if you're not training models you're

00:58:43.500 --> 00:58:46.140
just wanting to use other existing

00:58:46.140 --> 00:58:48.920
trained models

00:58:50.460 --> 00:58:51.780
um

00:58:51.780 --> 00:58:53.940
so anyway most people who do this stuff

00:58:53.940 --> 00:58:56.280
seriously almost everybody

00:58:56.280 --> 00:58:59.400
has in video cards

00:58:59.400 --> 00:59:00.119
um

00:59:00.119 --> 00:59:01.980
so then what we're going to be using is

00:59:01.980 --> 00:59:03.839
a library called Transformers from

00:59:03.839 --> 00:59:06.359
hugging face and the reason for that is

00:59:06.359 --> 00:59:08.839
that basically people upload

00:59:08.839 --> 00:59:10.980
lots of pre-trained models or

00:59:10.980 --> 00:59:13.559
firetrained models up to the hugging

00:59:13.559 --> 00:59:15.480
face Hub and in fact there's even a

00:59:15.480 --> 00:59:18.900
leaderboard where you can see which are

00:59:18.900 --> 00:59:20.760
the best models

00:59:20.760 --> 00:59:26.520
now this is a really uh

00:59:26.520 --> 00:59:29.099
fraud area so at the moment this one is

00:59:29.099 --> 00:59:30.900
meant to be the best model it has the

00:59:30.900 --> 00:59:32.400
highest average score

00:59:32.400 --> 00:59:34.680
and maybe it is good I haven't actually

00:59:34.680 --> 00:59:36.960
used this particular model

00:59:36.960 --> 00:59:39.720
um or maybe it's not I actually have no

00:59:39.720 --> 00:59:41.460
idea because the problem is

00:59:41.460 --> 00:59:46.020
these metrics are not particularly well

00:59:46.020 --> 00:59:49.260
aligned with real life usage

00:59:49.260 --> 00:59:49.980
um

00:59:49.980 --> 00:59:51.660
for all kinds of reasons and also

00:59:51.660 --> 00:59:53.160
sometimes you get something called

00:59:53.160 --> 00:59:55.500
leakage which means that sometimes some

00:59:55.500 --> 00:59:57.780
of the questions from these things

00:59:57.780 --> 00:59:59.339
actually leaks through to some of the

00:59:59.339 --> 01:00:00.920
training sets

01:00:00.920 --> 01:00:04.680
so you can get as a rule of thumb what

01:00:04.680 --> 01:00:06.119
to use from here but you should always

01:00:06.119 --> 01:00:07.859
try things

01:00:07.859 --> 01:00:08.760
um

01:00:08.760 --> 01:00:11.040
and you can also say you know these ones

01:00:11.040 --> 01:00:13.680
are all the 70b here that tells you how

01:00:13.680 --> 01:00:15.420
big it is so this is a 70 billion

01:00:15.420 --> 01:00:18.079
parameter model

01:00:18.299 --> 01:00:18.900
um

01:00:18.900 --> 01:00:22.559
so generally speaking for the kinds of

01:00:22.559 --> 01:00:24.900
gpus you we're talking about you'll be

01:00:24.900 --> 01:00:27.420
wanting no bigger than 13B and quite

01:00:27.420 --> 01:00:29.280
often 7B

01:00:29.280 --> 01:00:30.359
um

01:00:30.359 --> 01:00:32.099
so

01:00:32.099 --> 01:00:34.500
let's see if we've confined here the 13B

01:00:34.500 --> 01:00:37.520
model for example

01:00:37.559 --> 01:00:39.960
um all right so you can find models to

01:00:39.960 --> 01:00:41.819
try out from things like this

01:00:41.819 --> 01:00:43.920
leaderboard

01:00:43.920 --> 01:00:45.839
um and there's also a really great

01:00:45.839 --> 01:00:47.700
leaderboard called fast eval which I

01:00:47.700 --> 01:00:51.359
like a lot because it focuses on some

01:00:51.359 --> 01:00:55.079
more sophisticated evaluation methods

01:00:55.079 --> 01:00:57.299
such as this Chain of Thought evaluation

01:00:57.299 --> 01:01:00.780
method so I kind of trust these a little

01:01:00.780 --> 01:01:04.799
bit more and these are also GSM 8K is a

01:01:04.799 --> 01:01:06.660
difficult math benchmark

01:01:06.660 --> 01:01:09.059
uh big bench hard

01:01:09.059 --> 01:01:12.720
um so forth so yeah so you know stable

01:01:12.720 --> 01:01:16.680
Beluga 2 Wizard math 13B dolphin Lima

01:01:16.680 --> 01:01:19.260
13B et cetera these would all be good

01:01:19.260 --> 01:01:21.920
options

01:01:23.040 --> 01:01:25.559
um yeah so you need to pick a model and

01:01:25.559 --> 01:01:28.020
at the moment nearly all the good models

01:01:28.020 --> 01:01:30.599
are based on metas

01:01:30.599 --> 01:01:33.299
llama too so when I say based on what

01:01:33.299 --> 01:01:36.420
does that mean well what that means is

01:01:36.420 --> 01:01:41.640
this model here llama 2 7B so it's a

01:01:41.640 --> 01:01:43.260
llama model that's that's just the name

01:01:43.260 --> 01:01:45.119
meta called it this is their version two

01:01:45.119 --> 01:01:47.520
of llama this is their seven billion

01:01:47.520 --> 01:01:49.500
size one it's the smallest one that they

01:01:49.500 --> 01:01:51.839
make and specifically these weights have

01:01:51.839 --> 01:01:53.940
been created for hugging face so you can

01:01:53.940 --> 01:01:54.780
load it with the hugging face

01:01:54.780 --> 01:01:55.980
Transformers

01:01:55.980 --> 01:01:59.579
and this model has only got As far as

01:01:59.579 --> 01:02:01.440
here it's done the language model of

01:02:01.440 --> 01:02:03.420
pre-trading it's done none of the

01:02:03.420 --> 01:02:07.980
instruction tuning and none of the rlhf

01:02:07.980 --> 01:02:08.880
um

01:02:08.880 --> 01:02:11.280
so we would need to fine tune it to

01:02:11.280 --> 01:02:14.299
really get it to do much useful

01:02:14.299 --> 01:02:18.299
so we can just say Okay create a

01:02:18.299 --> 01:02:20.280
automatically create the appropriate

01:02:20.280 --> 01:02:21.059
model

01:02:21.059 --> 01:02:24.059
for language models so cause or LM is

01:02:24.059 --> 01:02:26.280
basically refers to that ULM fit stage

01:02:26.280 --> 01:02:27.440
one process

01:02:27.440 --> 01:02:30.720
or stage two in fact so we've got the

01:02:30.720 --> 01:02:33.000
pre-trained model from this name metal

01:02:33.000 --> 01:02:36.299
alarm element two blah blah okay

01:02:36.299 --> 01:02:38.099
now

01:02:38.099 --> 01:02:39.299
um

01:02:39.299 --> 01:02:42.619
generally speaking we use

01:02:42.619 --> 01:02:47.119
16-bit floating Point numbers nowadays

01:02:47.119 --> 01:02:49.859
but if you think about it

01:02:49.859 --> 01:02:54.480
16 bit is two bytes so 7B times two it's

01:02:54.480 --> 01:02:57.500
going to be 14 gigabytes

01:02:57.500 --> 01:03:01.380
just to load in the weights so you've

01:03:01.380 --> 01:03:03.960
got to have a decent model to be able to

01:03:03.960 --> 01:03:05.059
do that

01:03:05.059 --> 01:03:07.859
perhaps surprisingly you can actually

01:03:07.859 --> 01:03:10.799
just cast it to 8-bit and it still works

01:03:10.799 --> 01:03:12.059
pretty well thanks to something called

01:03:12.059 --> 01:03:13.980
discretization

01:03:13.980 --> 01:03:17.940
so let's try that so remember this is

01:03:17.940 --> 01:03:19.200
just a language model looking only

01:03:19.200 --> 01:03:21.000
complete sentences we can't ask it a

01:03:21.000 --> 01:03:22.980
question and expect a great answer so

01:03:22.980 --> 01:03:24.059
let's just give it the start of a

01:03:24.059 --> 01:03:26.400
sentence Jeremy how it is a

01:03:26.400 --> 01:03:28.680
and so we need the right tokenizer so

01:03:28.680 --> 01:03:29.940
this will automatically create the right

01:03:29.940 --> 01:03:31.980
kind of tokenizer for this model

01:03:31.980 --> 01:03:36.299
we can grab the tokens as Pi torch

01:03:36.299 --> 01:03:39.079
here they are

01:03:40.680 --> 01:03:42.839
and

01:03:42.839 --> 01:03:44.640
just to confirm if we decode them back

01:03:44.640 --> 01:03:46.380
again we get back the original plus a

01:03:46.380 --> 01:03:48.240
special token to say this is the start

01:03:48.240 --> 01:03:49.920
of a document

01:03:49.920 --> 01:03:53.400
and so we can now call generate so

01:03:53.400 --> 01:03:54.780
generate

01:03:54.780 --> 01:03:56.640
will

01:03:56.640 --> 01:03:59.760
um Auto regressively so call the model

01:03:59.760 --> 01:04:02.099
again and again passing its previous

01:04:02.099 --> 01:04:05.000
result back as the next

01:04:05.000 --> 01:04:07.579
as the next input

01:04:07.579 --> 01:04:10.740
and I'm just going to do that 15 times

01:04:10.740 --> 01:04:12.780
so this is you can you can write this

01:04:12.780 --> 01:04:14.220
for Loop yourself this isn't doing

01:04:14.220 --> 01:04:16.020
anything fancy in fact I would recommend

01:04:16.020 --> 01:04:18.720
writing this yourself to make sure that

01:04:18.720 --> 01:04:22.040
you know how that it all works okay

01:04:22.859 --> 01:04:23.940
um

01:04:23.940 --> 01:04:26.339
we have to put those tokens on the GPU

01:04:26.339 --> 01:04:27.960
and at the end I recommend putting them

01:04:27.960 --> 01:04:30.000
back onto the CPU the result and here

01:04:30.000 --> 01:04:31.380
are the tokens

01:04:31.380 --> 01:04:33.180
not very interesting so we have to

01:04:33.180 --> 01:04:35.940
decode them using the tokenizer and so

01:04:35.940 --> 01:04:38.460
the first 25 sorry first 15 tokens are

01:04:38.460 --> 01:04:40.920
Jeremy Howard is a 28 year old

01:04:40.920 --> 01:04:42.599
Australian AI researcher and

01:04:42.599 --> 01:04:45.000
entrepreneur okay well 28 years old is

01:04:45.000 --> 01:04:46.859
not exactly correct but we'll call it

01:04:46.859 --> 01:04:49.020
close enough I like that thank you very

01:04:49.020 --> 01:04:49.859
much

01:04:49.859 --> 01:04:53.520
llama 7B So Okay so we've got a language

01:04:53.520 --> 01:04:56.359
model completing sentences

01:04:56.359 --> 01:04:59.160
it took

01:04:59.160 --> 01:05:01.579
one in the third seconds

01:05:01.579 --> 01:05:04.020
and that's a bit slower than it could be

01:05:04.020 --> 01:05:06.059
because we used 8-bit

01:05:06.059 --> 01:05:08.400
if we use 16 bit there's a special thing

01:05:08.400 --> 01:05:11.400
called B float 16 which is a really

01:05:11.400 --> 01:05:13.980
great 16-bit floating Point format

01:05:13.980 --> 01:05:16.740
that's used usable on any somewhat

01:05:16.740 --> 01:05:19.799
recent GPS Nvidia GPU now if we use it

01:05:19.799 --> 01:05:22.319
it's going to take twice as much RAM as

01:05:22.319 --> 01:05:23.640
we discussed

01:05:23.640 --> 01:05:25.859
but look at the time

01:05:25.859 --> 01:05:29.780
it's come down to 390 milliseconds

01:05:29.819 --> 01:05:30.599
um

01:05:30.599 --> 01:05:33.059
now there is a better option still than

01:05:33.059 --> 01:05:34.559
even that

01:05:34.559 --> 01:05:36.119
there's a different kind of

01:05:36.119 --> 01:05:39.000
discretization called gptq

01:05:39.000 --> 01:05:43.619
where a model is carefully optimized to

01:05:43.619 --> 01:05:47.280
work with uh four or eight or other you

01:05:47.280 --> 01:05:51.420
know lower Precision data automatically

01:05:51.420 --> 01:05:53.700
and

01:05:53.700 --> 01:05:54.660
um

01:05:54.660 --> 01:05:56.400
this particular person known as the

01:05:56.400 --> 01:05:59.819
bloke is fantastic at taking popular

01:05:59.819 --> 01:06:02.700
models running that optimization process

01:06:02.700 --> 01:06:04.799
and then uploading the results back to

01:06:04.799 --> 01:06:06.960
hacking face

01:06:06.960 --> 01:06:11.359
so we can use this gptq version

01:06:11.359 --> 01:06:14.220
and internally this is actually going to

01:06:14.220 --> 01:06:15.780
use I'm not sure exactly how many bits

01:06:15.780 --> 01:06:16.859
this particular one is I think it's

01:06:16.859 --> 01:06:18.839
probably going to be four bits

01:06:18.839 --> 01:06:21.660
but it's going to be much more optimized

01:06:21.660 --> 01:06:24.240
um and so look at this 270 milliseconds

01:06:24.240 --> 01:06:26.579
it's actually faster

01:06:26.579 --> 01:06:28.140
than

01:06:28.140 --> 01:06:30.900
16 bit even though internally it's

01:06:30.900 --> 01:06:32.940
actually casting it up to 16 bit each

01:06:32.940 --> 01:06:34.380
layer to do it

01:06:34.380 --> 01:06:35.880
and that's because there's a lot less

01:06:35.880 --> 01:06:38.160
memory moving around

01:06:38.160 --> 01:06:40.140
and to confirm

01:06:40.140 --> 01:06:41.940
in fact what we could even do now is we

01:06:41.940 --> 01:06:45.140
go up to 13B easy and in fact it's still

01:06:45.140 --> 01:06:48.299
faster than the 7B now that we're using

01:06:48.299 --> 01:06:50.099
the gptq version so this is a really

01:06:50.099 --> 01:06:52.440
helpful tip

01:06:52.440 --> 01:06:54.180
so let's put all those things together

01:06:54.180 --> 01:06:56.220
the tokenizer the generate the batch

01:06:56.220 --> 01:06:58.260
decode we'll call this gen for Generate

01:06:58.260 --> 01:07:01.319
and so we can now use the 13B GPT key

01:07:01.319 --> 01:07:03.180
model

01:07:03.180 --> 01:07:06.000
and let's try this Jeremy Howard is a so

01:07:06.000 --> 01:07:08.940
it's got to 50 tokens so fast 16-year

01:07:08.940 --> 01:07:10.859
veteran of Silicon Valley co-founder of

01:07:10.859 --> 01:07:12.839
cargo a Marketplace or predictive model

01:07:12.839 --> 01:07:15.420
here's company kaggle.com has become the

01:07:15.420 --> 01:07:17.160
data science competitions what I don't

01:07:17.160 --> 01:07:18.480
know I was going to say but anyway it's

01:07:18.480 --> 01:07:20.460
on the right track I was actually there

01:07:20.460 --> 01:07:24.799
for 10 years not 16 but that's all right

01:07:24.900 --> 01:07:25.500
um

01:07:25.500 --> 01:07:28.319
okay so this is looking good

01:07:28.319 --> 01:07:29.760
um

01:07:29.760 --> 01:07:31.920
but probably a lot of the time we're

01:07:31.920 --> 01:07:34.380
going to be interested in you know

01:07:34.380 --> 01:07:36.240
asking questions or using instructions

01:07:36.240 --> 01:07:38.760
so stability AI has this nice series

01:07:38.760 --> 01:07:41.099
called stable Beluga including a small

01:07:41.099 --> 01:07:44.099
7B one and other bigger ones

01:07:44.099 --> 01:07:46.559
and these are all based on llama2 but

01:07:46.559 --> 01:07:49.140
these have been instruction tuned they

01:07:49.140 --> 01:07:50.760
might even have been RL hdf but I can't

01:07:50.760 --> 01:07:52.140
remember now

01:07:52.140 --> 01:07:52.799
um

01:07:52.799 --> 01:07:56.880
so we can create a stable Beluga model

01:07:56.880 --> 01:08:00.000
and now something really important

01:08:00.000 --> 01:08:02.520
that I keep forgetting everybody keeps

01:08:02.520 --> 01:08:04.319
forgetting is

01:08:04.319 --> 01:08:09.559
during the instruction tuning process

01:08:12.059 --> 01:08:15.059
during the instruction tuning process

01:08:15.059 --> 01:08:18.660
the instructions that are

01:08:18.660 --> 01:08:23.040
passed in actually uh

01:08:23.040 --> 01:08:23.660
um

01:08:23.660 --> 01:08:26.580
they don't just appear like this they

01:08:26.580 --> 01:08:28.140
actually always are in a particular

01:08:28.140 --> 01:08:30.540
format and the format Believe It or Not

01:08:30.540 --> 01:08:34.020
changes quite a bit from from fine tune

01:08:34.020 --> 01:08:36.839
to fine tune and so you have to go to

01:08:36.839 --> 01:08:39.120
the web page

01:08:39.120 --> 01:08:41.940
for the model

01:08:41.940 --> 01:08:44.880
and scroll down to found out

01:08:44.880 --> 01:08:47.219
what the prompt format is

01:08:47.219 --> 01:08:48.960
so here's the prompt format so I

01:08:48.960 --> 01:08:50.640
generally just copy it

01:08:50.640 --> 01:08:54.739
and then I paste it into python

01:08:54.900 --> 01:08:57.739
which I did here

01:08:58.199 --> 01:09:00.000
and

01:09:00.000 --> 01:09:03.420
created a function called make prompt

01:09:03.420 --> 01:09:07.259
that used the exact same format that it

01:09:07.259 --> 01:09:10.080
said to use and so now if I want to say

01:09:10.080 --> 01:09:12.960
who is Jeremy Howard I can call Jen

01:09:12.960 --> 01:09:14.640
again that was that function I created

01:09:14.640 --> 01:09:15.960
up here

01:09:15.960 --> 01:09:19.080
and make the correct prompt from that

01:09:19.080 --> 01:09:20.400
question

01:09:20.400 --> 01:09:22.739
and then it returns back okay so you can

01:09:22.739 --> 01:09:23.940
see here

01:09:23.940 --> 01:09:25.859
or this prefix this is a system

01:09:25.859 --> 01:09:27.299
instruction

01:09:27.299 --> 01:09:29.339
this is my question and then the

01:09:29.339 --> 01:09:31.500
assistant says Jeremy Howard's an

01:09:31.500 --> 01:09:32.699
Australian entrepreneur computer

01:09:32.699 --> 01:09:34.920
scientist co-founder of machine learning

01:09:34.920 --> 01:09:37.319
and deep Learning Company faster AI okay

01:09:37.319 --> 01:09:39.359
so this one's actually all correct so

01:09:39.359 --> 01:09:42.120
it's getting better by using an actual

01:09:42.120 --> 01:09:45.060
instruction tune model

01:09:45.060 --> 01:09:46.020
um

01:09:46.020 --> 01:09:48.120
and so we could then start to scale up

01:09:48.120 --> 01:09:52.500
so we could use the 13B and in fact uh

01:09:52.500 --> 01:09:54.719
we looked briefly at this open Orca data

01:09:54.719 --> 01:09:57.540
set earlier so llama2 has been

01:09:57.540 --> 01:10:00.719
fine-tuned on Oakman Orca and then also

01:10:00.719 --> 01:10:02.580
fine-tuned on another really great data

01:10:02.580 --> 01:10:05.880
set called platypus and so the whole

01:10:05.880 --> 01:10:09.000
thing together is the open Orca platypus

01:10:09.000 --> 01:10:10.560
and then this is going to be the bigger

01:10:10.560 --> 01:10:11.580
13B

01:10:11.580 --> 01:10:15.420
gptq means it's going to be quantized

01:10:15.420 --> 01:10:18.900
so that's got a different format okay a

01:10:18.900 --> 01:10:21.060
different prompt format so again we can

01:10:21.060 --> 01:10:23.340
scroll down and see what the prompt

01:10:23.340 --> 01:10:26.100
format is there it is

01:10:26.100 --> 01:10:30.060
okay and so

01:10:30.060 --> 01:10:32.159
we can create a function called make

01:10:32.159 --> 01:10:34.620
open Orca prompt that has that prompt

01:10:34.620 --> 01:10:37.040
format

01:10:37.260 --> 01:10:39.659
and so now we can say okay who is Jeremy

01:10:39.659 --> 01:10:41.760
Howard and now I've become British which

01:10:41.760 --> 01:10:42.960
is kind of true I was born in England

01:10:42.960 --> 01:10:44.760
but I moved to Australia

01:10:44.760 --> 01:10:47.040
uh professional poker player no

01:10:47.040 --> 01:10:49.320
definitely not that uh co-founding

01:10:49.320 --> 01:10:50.940
several companies

01:10:50.940 --> 01:10:54.420
including first.ai also kaggle okay so

01:10:54.420 --> 01:10:56.640
not bad yeah it was acquired by Google

01:10:56.640 --> 01:10:58.620
was it 2017 probably something around

01:10:58.620 --> 01:11:02.699
there okay so you can see we've got our

01:11:02.699 --> 01:11:04.320
own models

01:11:04.320 --> 01:11:08.659
giving us some pretty good information

01:11:09.659 --> 01:11:11.340
how do we make it even better you know

01:11:11.340 --> 01:11:12.540
because it's it's it's still

01:11:12.540 --> 01:11:14.640
hallucinating

01:11:14.640 --> 01:11:16.440
you know

01:11:16.440 --> 01:11:18.420
um

01:11:18.420 --> 01:11:19.980
and

01:11:19.980 --> 01:11:22.560
you know llama two I think has been

01:11:22.560 --> 01:11:24.060
trained with more up-to-date information

01:11:24.060 --> 01:11:26.580
than gpt4 it doesn't have the September

01:11:26.580 --> 01:11:29.280
2021 cut off

01:11:29.280 --> 01:11:32.219
um but it you know it's still

01:11:32.219 --> 01:11:33.960
got a knowledge cut off you know we

01:11:33.960 --> 01:11:35.460
would like to use the most up-to-date

01:11:35.460 --> 01:11:37.020
information we want to use the right

01:11:37.020 --> 01:11:39.179
information to answer these questions as

01:11:39.179 --> 01:11:41.159
well as possible

01:11:41.159 --> 01:11:43.020
so to do this we can use something

01:11:43.020 --> 01:11:46.320
called retrieval augmented generation

01:11:46.320 --> 01:11:49.560
so what happens with retrieval augmented

01:11:49.560 --> 01:11:52.440
generation is

01:11:52.440 --> 01:11:54.420
when we take the question we've been

01:11:54.420 --> 01:11:57.900
asked like who is Jeremy held and then

01:11:57.900 --> 01:12:00.800
we say okay

01:12:01.020 --> 01:12:03.780
let's try and search for

01:12:03.780 --> 01:12:05.940
documents that may help us answer that

01:12:05.940 --> 01:12:07.400
question

01:12:07.400 --> 01:12:10.020
so obviously we would expect for example

01:12:10.020 --> 01:12:12.780
Wikipedia to be useful and then what we

01:12:12.780 --> 01:12:17.480
do is we say okay with that information

01:12:17.480 --> 01:12:21.960
let's now see if we can tell

01:12:21.960 --> 01:12:24.900
the language model about what we found

01:12:24.900 --> 01:12:28.460
and then have it answer the question

01:12:28.460 --> 01:12:31.320
so let me show you so let's actually

01:12:31.320 --> 01:12:34.340
grab a Wikipedia

01:12:34.340 --> 01:12:36.239
python package

01:12:36.239 --> 01:12:39.780
we will scrape Wikipedia grabbing the

01:12:39.780 --> 01:12:42.179
Jeremy Howard web page

01:12:42.179 --> 01:12:44.280
and so here's the start of the Jeremy

01:12:44.280 --> 01:12:47.420
Howard Wikipedia page

01:12:48.199 --> 01:12:52.199
it has 613 words now generally speaking

01:12:52.199 --> 01:12:54.060
these open source models will have a

01:12:54.060 --> 01:12:55.860
context length of about two thousand or

01:12:55.860 --> 01:12:57.840
four thousand so the context length is

01:12:57.840 --> 01:13:00.780
how many tokens Can it handle so that's

01:13:00.780 --> 01:13:02.460
fine it'll be able to handle this web

01:13:02.460 --> 01:13:03.360
page

01:13:03.360 --> 01:13:04.800
and what we're going to do is we're

01:13:04.800 --> 01:13:07.020
going to ask it the question so we're

01:13:07.020 --> 01:13:08.460
going to have here question and with a

01:13:08.460 --> 01:13:10.380
question but before it we're going to

01:13:10.380 --> 01:13:11.820
say answer the question with the help of

01:13:11.820 --> 01:13:13.320
the context we're going to provide this

01:13:13.320 --> 01:13:14.640
to the language model

01:13:14.640 --> 01:13:16.020
and we're going to say context and

01:13:16.020 --> 01:13:17.940
they're going to have the whole web page

01:13:17.940 --> 01:13:20.100
so suddenly now our question is going to

01:13:20.100 --> 01:13:22.980
be a lot bigger our prompt

01:13:22.980 --> 01:13:25.980
right so our prompt

01:13:25.980 --> 01:13:29.520
now contains the entire web page the

01:13:29.520 --> 01:13:31.500
whole Wikipedia page

01:13:31.500 --> 01:13:33.780
followed by a question

01:13:33.780 --> 01:13:36.060
and so now

01:13:36.060 --> 01:13:37.560
it says

01:13:37.560 --> 01:13:39.420
Jeremy how does an Australian data

01:13:39.420 --> 01:13:41.460
scientist Edge entrepreneur an educator

01:13:41.460 --> 01:13:42.840
known for his work in deep learning

01:13:42.840 --> 01:13:45.179
co-founder of fast AI teaches courses

01:13:45.179 --> 01:13:47.820
develops software conducts research used

01:13:47.820 --> 01:13:52.380
to be yeah okay it's perfect right so

01:13:52.380 --> 01:13:54.420
it's actually done a really good job

01:13:54.420 --> 01:13:57.239
like if somebody asked me to send them a

01:13:57.239 --> 01:13:59.719
you know 100 word bio

01:13:59.719 --> 01:14:02.219
uh that would actually probably be

01:14:02.219 --> 01:14:03.840
better than I would have written myself

01:14:03.840 --> 01:14:05.520
and you'll see even though I asked for

01:14:05.520 --> 01:14:09.719
300 tokens it actually got sent back the

01:14:09.719 --> 01:14:12.659
end of stream token and so it knows to

01:14:12.659 --> 01:14:15.120
stop at this point

01:14:15.120 --> 01:14:16.980
um

01:14:16.980 --> 01:14:19.380
well that's all very well but how do we

01:14:19.380 --> 01:14:21.120
know to pass in the Jeremy Howard

01:14:21.120 --> 01:14:22.739
Wikipedia page

01:14:22.739 --> 01:14:25.080
well the way we know which Wikipedia

01:14:25.080 --> 01:14:26.699
page to pass in

01:14:26.699 --> 01:14:30.540
is that we can use another model to tell

01:14:30.540 --> 01:14:34.080
us which web page or which document is

01:14:34.080 --> 01:14:37.520
the most useful for answering a question

01:14:41.040 --> 01:14:44.100
and the way we do that is we we can use

01:14:44.100 --> 01:14:45.540
something called sentence Transformer

01:14:45.540 --> 01:14:48.120
and we can use a special kind of model

01:14:48.120 --> 01:14:51.480
that specifically designed to take a

01:14:51.480 --> 01:14:54.900
document and turn it into a bunch of

01:14:54.900 --> 01:14:57.420
activations where

01:14:57.420 --> 01:14:59.880
two documents that are similar will have

01:14:59.880 --> 01:15:01.440
similar activations

01:15:01.440 --> 01:15:03.120
so let me just let me show you what I

01:15:03.120 --> 01:15:05.159
mean what I'm going to do is I'm going

01:15:05.159 --> 01:15:07.679
to grab just the first paragraph of my

01:15:07.679 --> 01:15:10.679
Wikipedia page and I'm going to grab the

01:15:10.679 --> 01:15:13.140
first paragraph of Tony Blair's

01:15:13.140 --> 01:15:14.699
Wikipedia page

01:15:14.699 --> 01:15:16.620
okay so we're pretty different people

01:15:16.620 --> 01:15:18.540
right this is just like a really simple

01:15:18.540 --> 01:15:21.960
small example and I'm going to then call

01:15:21.960 --> 01:15:23.280
this model

01:15:23.280 --> 01:15:25.679
so I'm going to say encode and I'm going

01:15:25.679 --> 01:15:27.780
to encode my Wikipedia first paragraph

01:15:27.780 --> 01:15:30.300
Tony Blair's first paragraph and the

01:15:30.300 --> 01:15:31.920
question

01:15:31.920 --> 01:15:36.620
which was who is Jeremy Howard

01:15:36.840 --> 01:15:39.000
and it's going to pass back

01:15:39.000 --> 01:15:42.960
a 384 long vector

01:15:42.960 --> 01:15:44.699
of embeddings

01:15:44.699 --> 01:15:46.080
for the question

01:15:46.080 --> 01:15:49.020
for me and for Tony Blair

01:15:49.020 --> 01:15:51.540
and what I can now do is I can calculate

01:15:51.540 --> 01:15:53.880
the similarity

01:15:53.880 --> 01:15:56.580
between the question and the Jeremy

01:15:56.580 --> 01:15:58.380
Howard Wikipedia page

01:15:58.380 --> 01:16:00.000
and I can also do it for the question

01:16:00.000 --> 01:16:03.239
versus the Tony Blair Wikipedia page and

01:16:03.239 --> 01:16:06.840
as you can see it's higher for me and so

01:16:06.840 --> 01:16:09.600
that tells you that if you're trying to

01:16:09.600 --> 01:16:11.460
figure out what document to use to help

01:16:11.460 --> 01:16:13.679
you answer this question better off

01:16:13.679 --> 01:16:15.719
using the Jeremy Howard Wikipedia page

01:16:15.719 --> 01:16:18.840
than the Tony Blair Wikipedia pitch

01:16:18.840 --> 01:16:20.280
foreign

01:16:20.280 --> 01:16:23.820
so if you had a few hundred documents

01:16:23.820 --> 01:16:26.940
you were thinking of using to give back

01:16:26.940 --> 01:16:28.500
to the model as context to help it

01:16:28.500 --> 01:16:30.600
answer a question you could literally

01:16:30.600 --> 01:16:33.780
just pass them all through to encode go

01:16:33.780 --> 01:16:35.580
through each one one at a time and see

01:16:35.580 --> 01:16:37.320
which is closest

01:16:37.320 --> 01:16:39.780
when you've got thousands or millions of

01:16:39.780 --> 01:16:41.159
documents

01:16:41.159 --> 01:16:43.199
you can use something called a vector

01:16:43.199 --> 01:16:45.960
database where basically as a one-off

01:16:45.960 --> 01:16:50.520
thing you go through and you encode all

01:16:50.520 --> 01:16:51.900
of your documents

01:16:51.900 --> 01:16:53.760
and so in fact

01:16:53.760 --> 01:16:56.460
um there's there's lots of pre-built

01:16:56.460 --> 01:16:57.780
systems for this

01:16:57.780 --> 01:17:00.060
um here's an example of one called H2O

01:17:00.060 --> 01:17:01.880
GPT

01:17:01.880 --> 01:17:05.340
and this is just something that I've got

01:17:05.340 --> 01:17:07.340
um

01:17:10.320 --> 01:17:12.420
that I've got running here on my

01:17:12.420 --> 01:17:14.159
computer it's just an open source thing

01:17:14.159 --> 01:17:16.320
written in Python and sitting here

01:17:16.320 --> 01:17:19.260
running on Port 7860 and so I just gone

01:17:19.260 --> 01:17:21.860
to localhost 7860

01:17:21.860 --> 01:17:26.100
and what I did was I just uploaded I

01:17:26.100 --> 01:17:27.360
just clicked upload and I've wrapped

01:17:27.360 --> 01:17:30.420
last uploaded a bunch of papers in fact

01:17:30.420 --> 01:17:32.520
I might be able to see it better

01:17:32.520 --> 01:17:35.640
yeah here we go a bunch of papers

01:17:35.640 --> 01:17:39.300
and so you know we could look at

01:17:39.300 --> 01:17:41.520
uh

01:17:41.520 --> 01:17:44.040
let me search yeah I can so for example

01:17:44.040 --> 01:17:46.560
we can look at the ULM fit paper that uh

01:17:46.560 --> 01:17:48.960
so bruter and I did and you can see it's

01:17:48.960 --> 01:17:52.020
taken the PDF and turned it into

01:17:52.020 --> 01:17:55.560
slightly crappily a text format and then

01:17:55.560 --> 01:18:00.300
it's created an embedding for each

01:18:00.300 --> 01:18:02.640
you know each section

01:18:02.640 --> 01:18:04.860
so I could then

01:18:04.860 --> 01:18:06.900
um ask it

01:18:06.900 --> 01:18:12.060
you know what is ULM fit

01:18:12.060 --> 01:18:14.960
and I'll hit enter

01:18:15.000 --> 01:18:16.320
and you can see here it's now actually

01:18:16.320 --> 01:18:17.880
saying based on the information provided

01:18:17.880 --> 01:18:19.739
in the context so it's showing us it's

01:18:19.739 --> 01:18:21.780
been given some context what context did

01:18:21.780 --> 01:18:23.520
it get so here are the things that it

01:18:23.520 --> 01:18:25.820
found

01:18:26.400 --> 01:18:30.860
right so it's being sent this context

01:18:31.500 --> 01:18:34.820
so this is kind of citations

01:18:37.100 --> 01:18:40.380
performance by leveraging the knowledge

01:18:40.380 --> 01:18:42.600
and adapting it to the specific task at

01:18:42.600 --> 01:18:44.100
hand

01:18:44.100 --> 01:18:45.480
um

01:18:45.480 --> 01:18:47.340
how

01:18:47.340 --> 01:18:50.520
what techniques be more specific

01:18:50.520 --> 01:18:54.420
does ULM fit

01:18:54.420 --> 01:18:58.040
uh let's see how it goes

01:19:00.480 --> 01:19:02.340
okay there we go so here's the three

01:19:02.340 --> 01:19:04.739
steps pre-trained fine-tune fine tune

01:19:04.739 --> 01:19:06.300
cool

01:19:06.300 --> 01:19:09.120
um so you can see it's not bad right

01:19:09.120 --> 01:19:10.679
um it's not

01:19:10.679 --> 01:19:13.560
amazing like you know the context in

01:19:13.560 --> 01:19:16.739
this particular case is pretty small

01:19:16.739 --> 01:19:19.260
um and it's and in particular

01:19:19.260 --> 01:19:21.600
if you think about how that embedding

01:19:21.600 --> 01:19:23.820
thing worked you can't really use like

01:19:23.820 --> 01:19:25.980
the normal kind of follow-up so for

01:19:25.980 --> 01:19:27.659
example

01:19:27.659 --> 01:19:28.620
um

01:19:28.620 --> 01:19:31.140
if I so it says fine tuning a classifier

01:19:31.140 --> 01:19:33.420
so I could say

01:19:33.420 --> 01:19:37.080
what classifier is used now the problem

01:19:37.080 --> 01:19:39.719
is that there's no context here being

01:19:39.719 --> 01:19:41.520
sent to the embedding model so it's

01:19:41.520 --> 01:19:42.719
actually going to have no idea I'm

01:19:42.719 --> 01:19:45.120
talking about new lmfit so generally

01:19:45.120 --> 01:19:48.239
speaking it's going to do a terrible job

01:19:48.239 --> 01:19:49.800
yeah I see it says it's used as a

01:19:49.800 --> 01:19:51.420
Roberta model but it's not but if I look

01:19:51.420 --> 01:19:53.880
at the sources it's no longer actually

01:19:53.880 --> 01:19:56.340
referring to Howard and Rooter

01:19:56.340 --> 01:19:58.679
so anyway you can see the basic idea

01:19:58.679 --> 01:20:01.080
this is called retrieval augmented

01:20:01.080 --> 01:20:03.780
generation Reg

01:20:03.780 --> 01:20:04.560
um

01:20:04.560 --> 01:20:07.800
and it's a it's a Nifty approach but you

01:20:07.800 --> 01:20:11.340
have to do it with with some care

01:20:11.340 --> 01:20:13.800
um and so there are lots of these uh

01:20:13.800 --> 01:20:17.520
private GPT things out there

01:20:17.520 --> 01:20:20.219
um actually the H2O GPT web page does a

01:20:20.219 --> 01:20:21.719
fantastic job

01:20:21.719 --> 01:20:25.199
of listing lots of them

01:20:25.199 --> 01:20:28.280
and comparing

01:20:28.280 --> 01:20:33.600
so as you can see if you want to run a

01:20:33.600 --> 01:20:35.340
private GPT

01:20:35.340 --> 01:20:38.060
there's no shortage of options

01:20:38.060 --> 01:20:40.080
and you can have your retrieval

01:20:40.080 --> 01:20:42.900
augmented generation I haven't tried

01:20:42.900 --> 01:20:45.600
I've only tried this one H2O GPT I don't

01:20:45.600 --> 01:20:47.880
love it it's all right

01:20:47.880 --> 01:20:49.260
um

01:20:49.260 --> 01:20:50.580
good

01:20:50.580 --> 01:20:52.199
so finally I want to talk about what's

01:20:52.199 --> 01:20:54.260
perhaps the most interesting

01:20:54.260 --> 01:20:57.060
option we have which is to do our own

01:20:57.060 --> 01:20:59.460
fine tuning and fine tuning is cool

01:20:59.460 --> 01:21:01.440
because rather than just retrieving

01:21:01.440 --> 01:21:03.000
documents which might have useful

01:21:03.000 --> 01:21:05.780
context we can actually change our model

01:21:05.780 --> 01:21:09.360
to behave based on the documents that we

01:21:09.360 --> 01:21:11.100
have available and I'm going to show you

01:21:11.100 --> 01:21:12.420
a really interesting example of fine

01:21:12.420 --> 01:21:15.659
tuning here what we're going to do is

01:21:15.659 --> 01:21:18.600
we're going to fine tune using this um

01:21:18.600 --> 01:21:21.659
no SQL data set

01:21:21.659 --> 01:21:27.659
and it's got examples of like a a schema

01:21:27.659 --> 01:21:30.360
for a table in a database

01:21:30.360 --> 01:21:32.159
a question

01:21:32.159 --> 01:21:35.760
and then the answer is

01:21:35.760 --> 01:21:40.020
the correct SQL

01:21:40.020 --> 01:21:42.300
to solve that question

01:21:42.300 --> 01:21:44.640
using that database schema

01:21:44.640 --> 01:21:47.460
and so I'm hoping we could use this to

01:21:47.460 --> 01:21:49.679
create a

01:21:49.679 --> 01:21:51.719
um you know I kind of it could be a hand

01:21:51.719 --> 01:21:54.179
to use a handy tool for for business

01:21:54.179 --> 01:21:55.739
users where they type some English

01:21:55.739 --> 01:22:00.420
question and SQL generated for them

01:22:00.420 --> 01:22:02.580
automatically don't know if it actually

01:22:02.580 --> 01:22:05.159
work in practice or not but this is just

01:22:05.159 --> 01:22:08.280
a little fun idea I thought we'd try out

01:22:08.280 --> 01:22:10.980
um I know there's lots of uh

01:22:10.980 --> 01:22:12.540
startups and stuff out there trying to

01:22:12.540 --> 01:22:15.659
do this more seriously but this is this

01:22:15.659 --> 01:22:17.159
is quite cool because it actually got it

01:22:17.159 --> 01:22:19.920
working today in just a couple of hours

01:22:19.920 --> 01:22:24.560
so what we do is we use the hugging face

01:22:24.560 --> 01:22:28.560
data sets library and what that does

01:22:28.560 --> 01:22:30.960
just like the hugging face Hub has lots

01:22:30.960 --> 01:22:33.719
of models stored on it hacking face data

01:22:33.719 --> 01:22:36.540
sets has lots of data sets stored on it

01:22:36.540 --> 01:22:38.699
and so instead of using Transformers

01:22:38.699 --> 01:22:40.560
which is what we use to grab models we

01:22:40.560 --> 01:22:43.560
use data sets and we just pass in the

01:22:43.560 --> 01:22:45.840
name of the person and the name of their

01:22:45.840 --> 01:22:48.659
repo and it grabs the data set and so we

01:22:48.659 --> 01:22:50.219
can take a look at it

01:22:50.219 --> 01:22:52.620
and it just has a training set

01:22:52.620 --> 01:22:55.020
with features

01:22:55.020 --> 01:22:58.880
and so then I can

01:23:00.300 --> 01:23:03.000
have a look at the training set so

01:23:03.000 --> 01:23:04.440
here's an example

01:23:04.440 --> 01:23:05.820
which looks a bit like what we've just

01:23:05.820 --> 01:23:06.900
seen

01:23:06.900 --> 01:23:10.800
so what we do now is we want to

01:23:10.800 --> 01:23:13.560
fine-tune a model now we can do that in

01:23:13.560 --> 01:23:17.100
in a notebook from scratch takes I don't

01:23:17.100 --> 01:23:18.960
know 100 or so lines of code it's not

01:23:18.960 --> 01:23:21.000
too much but given the time constraints

01:23:21.000 --> 01:23:24.360
here and also like I thought why not why

01:23:24.360 --> 01:23:25.679
don't we just use something that's ready

01:23:25.679 --> 01:23:27.780
to go so for example there's something

01:23:27.780 --> 01:23:30.600
called Axolotl which is quite nice in my

01:23:30.600 --> 01:23:32.900
opinion

01:23:33.719 --> 01:23:36.120
here it is here lovely another very nice

01:23:36.120 --> 01:23:39.060
open source piece of software and uh

01:23:39.060 --> 01:23:41.580
again you can just pip install it

01:23:41.580 --> 01:23:45.239
and it's got things like gptq and 16 bit

01:23:45.239 --> 01:23:47.300
and so forth ready to go

01:23:47.300 --> 01:23:52.640
and so what I did was a

01:23:52.679 --> 01:23:54.780
um it basically has a whole bunch of

01:23:54.780 --> 01:23:57.060
examples of things that it already knows

01:23:57.060 --> 01:23:58.440
how to do

01:23:58.440 --> 01:24:00.360
it's got llama 2 examples so I copied

01:24:00.360 --> 01:24:04.560
the Llama 2 example and I created a SQL

01:24:04.560 --> 01:24:06.719
example so basically just told it this

01:24:06.719 --> 01:24:09.239
is the path to the data set that I want

01:24:09.239 --> 01:24:12.420
this is the type

01:24:12.420 --> 01:24:14.400
um and everything else pretty much I

01:24:14.400 --> 01:24:16.140
left the same

01:24:16.140 --> 01:24:19.620
and then I just ran this command which

01:24:19.620 --> 01:24:21.360
is from there read me accelerate launch

01:24:21.360 --> 01:24:24.840
Axolotl passed in my yaml and that took

01:24:24.840 --> 01:24:26.060
about an hour

01:24:26.060 --> 01:24:28.440
on my GPU

01:24:28.440 --> 01:24:31.020
and at the end of the hour

01:24:31.020 --> 01:24:36.179
it had created a q Laura out directory Q

01:24:36.179 --> 01:24:37.679
stands for quantize that's because I was

01:24:37.679 --> 01:24:40.440
creating a smaller quantized model Laura

01:24:40.440 --> 01:24:41.820
I'm not going to talk about today but

01:24:41.820 --> 01:24:43.320
Laura is a very cool thing that

01:24:43.320 --> 01:24:45.179
basically another thing that makes your

01:24:45.179 --> 01:24:48.620
models smaller and also handles

01:24:48.620 --> 01:24:52.020
I can use bigger models on smaller gpus

01:24:52.020 --> 01:24:54.360
for training

01:24:54.360 --> 01:24:56.159
um so

01:24:56.159 --> 01:24:58.620
uh I trained it

01:24:58.620 --> 01:25:03.120
and then I thought okay let's uh create

01:25:03.120 --> 01:25:05.520
our own one so we're going to have this

01:25:05.520 --> 01:25:07.440
context

01:25:07.440 --> 01:25:10.440
and

01:25:10.679 --> 01:25:11.640
um

01:25:11.640 --> 01:25:14.840
this question

01:25:15.060 --> 01:25:17.219
get the count of competition hosts by

01:25:17.219 --> 01:25:18.780
theme

01:25:18.780 --> 01:25:20.219
and I'm not going to pass it an answer

01:25:20.219 --> 01:25:23.580
so I'll just ignore that so again I've

01:25:23.580 --> 01:25:27.900
found out what prompt they were using

01:25:27.900 --> 01:25:31.560
um and created a SQL prompt function and

01:25:31.560 --> 01:25:33.300
so here's what I've got to do use the

01:25:33.300 --> 01:25:34.739
following contextual information to

01:25:34.739 --> 01:25:36.360
answer the question

01:25:36.360 --> 01:25:38.460
context create tables there's the

01:25:38.460 --> 01:25:40.860
context question list or competition

01:25:40.860 --> 01:25:43.980
host sorted in ascending order

01:25:43.980 --> 01:25:47.360
and then I

01:25:47.719 --> 01:25:52.520
tokenized that chord generate

01:25:52.520 --> 01:25:54.300
and

01:25:54.300 --> 01:25:56.880
the answer was select count hosts kind

01:25:56.880 --> 01:25:59.940
of theme from Farm competition Group by

01:25:59.940 --> 01:26:02.580
theme that is correct

01:26:02.580 --> 01:26:06.120
so I think that's pretty remarkable we

01:26:06.120 --> 01:26:09.780
have just built it also took me like an

01:26:09.780 --> 01:26:11.940
hour to figure out how to do it and then

01:26:11.940 --> 01:26:15.360
an hour to actually do the training

01:26:15.360 --> 01:26:16.739
um and at the end of that we've actually

01:26:16.739 --> 01:26:20.520
got something which which is converting

01:26:20.520 --> 01:26:21.659
um

01:26:21.659 --> 01:26:25.800
Pros into SQL based on a schema so I

01:26:25.800 --> 01:26:27.120
think that's that's a really exciting

01:26:27.120 --> 01:26:28.800
idea

01:26:28.800 --> 01:26:30.659
um the only other thing I do want to

01:26:30.659 --> 01:26:32.880
briefly mention is

01:26:32.880 --> 01:26:36.900
um is doing stuff on Macs if you've got

01:26:36.900 --> 01:26:40.440
a Mac uh you there's a couple of really

01:26:40.440 --> 01:26:44.179
good options the options are mlc and

01:26:44.179 --> 01:26:48.300
lima.cpp currently mlc in particular I

01:26:48.300 --> 01:26:50.280
think it's kind of underappreciated it's

01:26:50.280 --> 01:26:52.260
a

01:26:52.260 --> 01:26:55.699
you know really nice

01:26:56.340 --> 01:26:58.940
project

01:26:59.219 --> 01:27:00.260
um

01:27:00.260 --> 01:27:03.840
uh where you can run language models on

01:27:03.840 --> 01:27:07.080
literally iPhone Android web browsers

01:27:07.080 --> 01:27:08.280
everything

01:27:08.280 --> 01:27:09.960
it's really cool

01:27:09.960 --> 01:27:12.780
and

01:27:12.780 --> 01:27:17.120
and so I'm now actually on my Mac here

01:27:17.460 --> 01:27:20.719
and I've got a

01:27:20.940 --> 01:27:24.239
tiny little Python program called chat

01:27:24.239 --> 01:27:28.320
and it's going to import chat module and

01:27:28.320 --> 01:27:30.840
it's going to import a

01:27:30.840 --> 01:27:32.639
discretized

01:27:32.639 --> 01:27:35.639
7B

01:27:35.699 --> 01:27:37.440
and that's going to ask the question

01:27:37.440 --> 01:27:40.139
what is the meaning of life

01:27:40.139 --> 01:27:42.120
so let's try it

01:27:42.120 --> 01:27:45.780
python chat.pi again I just installed

01:27:45.780 --> 01:27:47.400
this

01:27:47.400 --> 01:27:49.260
earlier today I haven't done that much

01:27:49.260 --> 01:27:51.540
stuff on Max before but I was pretty

01:27:51.540 --> 01:27:54.239
impressed to see

01:27:54.239 --> 01:27:57.199
that it is

01:27:57.300 --> 01:27:59.520
doing a good job here what is the

01:27:59.520 --> 01:28:00.960
meaning of life is complex and

01:28:00.960 --> 01:28:03.560
philosophical

01:28:03.960 --> 01:28:05.580
some people might find meaning in their

01:28:05.580 --> 01:28:07.860
relationships with others

01:28:07.860 --> 01:28:09.659
their impact in the world et cetera et

01:28:09.659 --> 01:28:12.800
cetera okay and it's doing

01:28:12.800 --> 01:28:15.540
9.6 tokens per second

01:28:15.540 --> 01:28:18.000
so there you go so there is running

01:28:18.000 --> 01:28:20.460
um a model on a Mac and then another

01:28:20.460 --> 01:28:22.080
option that you've probably heard about

01:28:22.080 --> 01:28:24.980
is llama.cpp

01:28:24.980 --> 01:28:27.960
llama.cpp runs on lots of different

01:28:27.960 --> 01:28:30.780
things as well including Max and also on

01:28:30.780 --> 01:28:32.300
Cuda

01:28:32.300 --> 01:28:35.960
it uses a different format called gguf

01:28:35.960 --> 01:28:38.219
and you can again you can use it from

01:28:38.219 --> 01:28:40.139
python even if it was a CPP thing it's

01:28:40.139 --> 01:28:42.420
got a python wrapper so you can just

01:28:42.420 --> 01:28:43.980
download

01:28:43.980 --> 01:28:48.300
again from hugging face at gguf

01:28:48.300 --> 01:28:49.920
file

01:28:49.920 --> 01:28:52.139
so you can just go through and there's

01:28:52.139 --> 01:28:53.219
lots of different ones they're all

01:28:53.219 --> 01:28:54.780
documented as to what's what you can

01:28:54.780 --> 01:28:56.940
pick how big a file you want you can

01:28:56.940 --> 01:28:59.280
download it and then you just say Okay

01:28:59.280 --> 01:29:02.340
llama model path equals pass in that

01:29:02.340 --> 01:29:03.780
gguf file

01:29:03.780 --> 01:29:05.760
it spits out lots and lots and lots of

01:29:05.760 --> 01:29:07.620
gunk

01:29:07.620 --> 01:29:10.860
and then you can say okay so if I called

01:29:10.860 --> 01:29:13.320
that llm you can then say llm question

01:29:13.320 --> 01:29:16.440
name the planets of the solar system 32

01:29:16.440 --> 01:29:17.940
tokens

01:29:17.940 --> 01:29:20.639
and

01:29:20.639 --> 01:29:22.980
there we are right in Pluto no longer

01:29:22.980 --> 01:29:24.540
considered a planet two mercury three

01:29:24.540 --> 01:29:27.420
Venus poor Earth Mars six oh never run

01:29:27.420 --> 01:29:30.840
out of tokens so again you know it's um

01:29:30.840 --> 01:29:32.699
just to show you here there are all

01:29:32.699 --> 01:29:34.980
these different options

01:29:34.980 --> 01:29:36.300
um

01:29:36.300 --> 01:29:38.520
uh you know I would say you know if

01:29:38.520 --> 01:29:40.320
you've got a

01:29:40.320 --> 01:29:43.620
Nvidia graphics card and your reasonably

01:29:43.620 --> 01:29:45.719
capable python programmer you'd probably

01:29:45.719 --> 01:29:49.260
be one of you use Pi torch and the

01:29:49.260 --> 01:29:51.719
hugging face ecosystem

01:29:51.719 --> 01:29:54.000
um but you know I think you know these

01:29:54.000 --> 01:29:55.320
things might change over time as well

01:29:55.320 --> 01:29:56.520
and certainly a lot of stuff is coming

01:29:56.520 --> 01:29:58.320
into llama pretty quickly now when it's

01:29:58.320 --> 01:29:59.639
developing very fast

01:29:59.639 --> 01:30:01.280
as you can see

01:30:01.280 --> 01:30:03.480
there's a lot of stuff that you can do

01:30:03.480 --> 01:30:07.080
right now with language models

01:30:07.080 --> 01:30:08.639
um particularly if you if you're pretty

01:30:08.639 --> 01:30:11.719
comfortable as a python programmer

01:30:11.719 --> 01:30:13.739
I think it's a really exciting time to

01:30:13.739 --> 01:30:15.300
get involved in some ways it's a

01:30:15.300 --> 01:30:19.199
frustrating time to get involved because

01:30:19.199 --> 01:30:20.699
um

01:30:20.699 --> 01:30:22.920
you know it's very early

01:30:22.920 --> 01:30:26.219
and a lot of stuff has weird little edge

01:30:26.219 --> 01:30:29.040
cases and It's tricky to install and

01:30:29.040 --> 01:30:30.600
stuff like that

01:30:30.600 --> 01:30:32.040
um

01:30:32.040 --> 01:30:34.080
there's a lot of great Discord channels

01:30:34.080 --> 01:30:36.120
however first AI have our own Discord

01:30:36.120 --> 01:30:37.860
channel so feel free to just Google for

01:30:37.860 --> 01:30:41.219
fast AI Discord and drop in we've got a

01:30:41.219 --> 01:30:43.139
channel called generative you feel free

01:30:43.139 --> 01:30:45.179
to ask any questions or

01:30:45.179 --> 01:30:48.060
tell us about what you're finding

01:30:48.060 --> 01:30:49.560
um yeah it's definitely something where

01:30:49.560 --> 01:30:51.179
you want to be getting help from other

01:30:51.179 --> 01:30:52.739
people on this journey because it is

01:30:52.739 --> 01:30:55.620
very early days and

01:30:55.620 --> 01:30:57.060
you know people are still figuring

01:30:57.060 --> 01:30:59.219
things out as we go but I think it's an

01:30:59.219 --> 01:31:01.500
exciting time to be doing this stuff and

01:31:01.500 --> 01:31:04.199
I'm yeah I'm really enjoying it and I

01:31:04.199 --> 01:31:06.780
hope that this has given some of you a

01:31:06.780 --> 01:31:08.460
useful starting point on your own

01:31:08.460 --> 01:31:10.199
Journey so I hope you found this useful

01:31:10.199 --> 01:31:14.120
thanks for listening bye
