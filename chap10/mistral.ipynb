{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import os, tomli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.streamlit/secrets.toml','rb') as f:\n",
    "    secrets = tomli.load(f)\n",
    "    \n",
    "api_key = secrets[\"MISTRAL_API_KEY\"]\n",
    "# api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-tiny\"\n",
    "\n",
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='user', content='What is the best French cheese?')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"What is the best French cheese?\")\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the best French cheese?'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat2struct(chat):\n",
    "    return [{'role': m.role, 'content': m.content} for m in chat]\n",
    "chat2struct(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [{'role': 'system', 'content': 'If I say hello, say world'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role='system', content='If I say hello, say world')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def struct2chat(struct):\n",
    "    return [ChatMessage(role=m['role'], content=m['content']) for m in struct]\n",
    "struct2chat(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role='system', content='If I say hello, say world')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatMessage(role=m[0]['role'], content=m[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is subjective to determine the \"best\" French cheese as it depends on personal preferences. Some popular and highly regarded French cheeses are:\n",
      "\n",
      "1. Roquefort: A blue-veined cheese from the Massif Central region, known for its strong, pungent flavor and distinctive tang.\n",
      "\n",
      "2. Comté: A nutty, buttery, and slightly sweet cheese from the Franche-Comté region, made from unpasteurized cow's milk.\n",
      "\n",
      "3. Camembert de Normandie: A soft, Earthy, and tangy cheese from the Normandy region, famous for its white mold rind.\n",
      "\n",
      "4. Brie de Meaux: A creamy and mild soft-ripened cheese from the Île-de-France region, known for its edible white rind and rich, buttery flavor.\n",
      "\n",
      "5. Munster: A pungent and smelly cheese from the Alsace region, characterized by its orange-red rind and strong, distinctive flavor.\n",
      "\n",
      "6. Chaource: A bloomy-rind, mild, and creamy cheese from the Île-de-France region, with a slightly tangy taste and a buttery texture.\n",
      "\n",
      "7. Époisses de Bourgogne: A soft, smelly, and runny cheese from the Burgundy region, known for its pungent aroma and rich, savory flavor.\n",
      "\n",
      "Ultimately, the \"best\" French cheese depends on your personal taste preferences, so it's recommended to try a variety and find the one that suits you best.\n"
     ]
    }
   ],
   "source": [
    "# No streaming\n",
    "chat_response = client.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webvtt-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, webvtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YannMike_2023-03-08.vtt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('vtt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'YannMike_2023-03-08.vtt'\n",
    "chat = webvtt.read('vtt/'+file)\n",
    "str = []\n",
    "for caption in chat:\n",
    "    str.append(caption.text)\n",
    "sep = '\\n'\n",
    "convo = sep.join(str)\n",
    "with open('txt/'+file.replace('vtt','txt'),mode='w') as f:\n",
    "    f.write(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "words = convo.split() # split the string into words \n",
    "num_tokens = math.floor(len(words) * 3/4) # 3/4 of the words are tokens\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speaker is planning to conduct an experiment where they record conversations and generate VTT files. They have developed a Python app to process these files and intend to use the ChatGPT API for analysis. However, they are having trouble finding time to implement this due to various commitments.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'summarize the following conversation'\n",
    "model = 'mistral-tiny'\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=context),\n",
    "    ChatMessage(role=\"user\", content=convo)\n",
    "]\n",
    "# messages = [\n",
    "#         {'role': 'system','content': context},\n",
    "#         {'role': 'user', 'content': convo}\n",
    "#             ]\n",
    "completion = client.chat(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.mistral.ai/guides/basic-RAG/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient, ChatMessage\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"book\"\n",
    "filename = \"essay.txt\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "with open(os.path.join(folder, filename), \"wb\") as file:\n",
    "    file.write(response.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(\n",
    "          model=\"mistral-embed\",\n",
    "          input=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Are we going to save democracy?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01547241,  0.00917053,  0.01520538, ..., -0.05145264,\n",
       "         0.01053619, -0.01382446]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not mention or discuss democracy, politics, or saving democracy. Therefore, I cannot provide an answer to this query based on the given context information.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=user_message)\n",
    "    ]\n",
    "    chat_response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impromptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf\"\n",
    "folder = \"book\"\n",
    "filename = \"impromptu-rh.pdf\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Save the file\n",
    "with open(os.path.join(folder, filename), \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"PDF file downloaded and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down into 1 pdf per page\n",
    "import pypdf\n",
    "\n",
    "def pdf_to_pages(file):\n",
    "\t\"extract text (pages) from pdf file\"\n",
    "\tpages = []\n",
    "\tpdf = pypdf.PdfReader(file)\n",
    "\tfor p in range(len(pdf.pages)):\n",
    "\t\tpage = pdf.pages[p]\n",
    "\t\ttext = page.extract_text()\n",
    "\t\tpages += [text]\n",
    "\treturn pages\n",
    "\n",
    "file = \"book/impromptu-rh.pdf\"\n",
    "pages = pdf_to_pages(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "EDUCATION\n",
      "If Hollywood central casting ever wants to portray a \n",
      "beloved instructor from an idealized past, they could do worse \n",
      "than University of Texas at Austin Professor Steven Mintz. Over \n",
      "four decades of teaching, Professor Mintz has published books \n",
      "and articles on topics as diverse as the psychology of prominent \n",
      "Anglo-American literary families and political good vs. evil.\n",
      "In collared shirts, with graying hair, Mintz can’t suppress his \n",
      "smile as he teaches. Students adore him: among hundreds who \n",
      "have anonymously rated Mintz online, his average rating is a \n",
      "perfect five out of five, with posts such as “easily the best orator \n",
      "I’ve ever witnessed,” “his lectures feel more like storytelling \n",
      "than class,” and “passionate about what he teaches.”\n",
      "Professor Mintz, frankly, excelled as a professor long before the \n",
      "development of LLMs. So you might have expected him to have \n",
      "reacted with indifference or hostility to the late 2022 public \n",
      "release of GPT-4’s cousin, ChatGPT.\n",
      "Instead, this seventy-year-old scholar had the same reaction \n",
      "that I did when he saw the power of GPT: he wanted to use it. \n",
      "Right away.\n"
     ]
    }
   ],
   "source": [
    "print(pages[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"book/chap1\"):\n",
    "    os.makedirs(\"book/chap1\")\n",
    "for p in pages[31:53]: # PDF pages 32-54 - chapter 1 on Education\n",
    "    with open(f\"book/chap1/impromptu-rh_ch1_p{pages.index(p)+1}.txt\",mode='w',encoding='utf-8') as f:\n",
    "        f.write(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pages[31:53]\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(\n",
    "          model=\"mistral-embed\",\n",
    "          input=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI can be used in various ways for education, such as:\\n\\n1. Personalized Learning: AI can help teachers create customized learning paths for students based on their prior knowledge, skills, interests, and goals. This can help students learn at their own pace and in their own style.\\n2. Interactive Learning: AI can create simulations, games, and virtual reality experiences to make learning more engaging and interactive. These tools can help students develop critical thinking, problem-solving, and teamwork skills.\\n3. Collaborative Learning: AI can facilitate collaborative learning experiences by generating prompts and scenarios to foster creative problem-solving, communication, and teamwork.\\n4. Feedback and Assessment: AI can provide immediate feedback and assessment to students, helping them identify their strengths and areas for improvement. This can help teachers monitor student progress and adjust instruction accordingly.\\n5. Content Creation: AI can help teachers create and curate content, giving them more time to focus on engaging and inspiring their students. AI can also help create accessible content for students with disabilities.\\n\\nOverall, AI has the potential to transform the way we learn and deliver instruction, making education more personalized, engaging, and effective.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can AI be used for Education?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=user_message)\n",
    "    ]\n",
    "    chat_response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
