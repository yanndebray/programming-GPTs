{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import os, tomli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../.streamlit/secrets.toml','rb') as f:\n",
    "    secrets = tomli.load(f)\n",
    "    \n",
    "api_key = secrets[\"MISTRAL_API_KEY\"]\n",
    "# api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-tiny\"\n",
    "\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the best French cheese?'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the best French cheese?\"}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"best\" French cheese can be subjective and depends on personal taste preferences. However, some of the most popular and highly regarded French cheeses include:\n",
      "\n",
      "1. Roquefort: A blue-veined cheese made from sheep's milk. It's known for its strong, pungent flavor and is often used in salads or as a topping.\n",
      "\n",
      "2. Brie de Meaux: A soft, creamy cheese with a white rind. It's known for its mild, buttery flavor and is often served with fruit or bread.\n",
      "\n",
      "3. Camembert: A soft, creamy cheese that's similar to Brie. It's known for its strong, mushroomy flavor and is often eaten at room temperature.\n",
      "\n",
      "4. Comté: A semi-hard cheese made from cow's milk. It's known for its nutty, sweet flavor and is often used in cooking or as a table cheese.\n",
      "\n",
      "5. Reblochon: A soft, creamy cheese with a rind. It's known for its strong, pungent flavor and is often used in fondue or as a topping.\n",
      "\n",
      "6. Époisses de Bourgogne: A soft, smelly cheese that's often served with walnuts. It's known for its strong, pungent flavor and is often eaten as a dessert cheese.\n"
     ]
    }
   ],
   "source": [
    "# No streaming\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webvtt-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, webvtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captions.vtt',\n",
       " 'sample.vtt',\n",
       " 'subtitles-2xxziIWmaSA.vtt',\n",
       " 'subtitles-f9_BWhCI4Zo.vtt',\n",
       " 'subtitles-jkrNMKz9pWU.vtt',\n",
       " 'subtitles-zjkBMFhNj_g.vtt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/vtt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'sample.vtt'\n",
    "chat = webvtt.read('../data/vtt/'+file)\n",
    "str = []\n",
    "for caption in chat:\n",
    "    str.append(caption.text)\n",
    "sep = '\\n'\n",
    "convo = sep.join(str)\n",
    "with open('../data/txt/'+file.replace('vtt','txt'),mode='w') as f:\n",
    "    f.write(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "words = convo.split() # split the string into words \n",
    "num_tokens = math.floor(len(words) * 3/4) # 3/4 of the words are tokens\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user is planning an experiment involving recording conversations and processing the resulting VTT (WebVTT) file using a Python app they developed. They aim to use the ChatGPT API for analysis, but are currently struggling to find time due to other commitments.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'summarize the following conversation'\n",
    "model = 'mistral-tiny'\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": context},\n",
    "    {\"role\": \"user\", \"content\": convo}\n",
    "]\n",
    "# messages = [\n",
    "#         {'role': 'system','content': context},\n",
    "#         {'role': 'user', 'content': convo}\n",
    "#             ]\n",
    "completion = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.mistral.ai/guides/basic-RAG/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"book\"\n",
    "filename = \"essay.txt\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "    \n",
    "with open(os.path.join(folder, filename), \"wb\") as file:\n",
    "    file.write(response.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Are we going to save democracy?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01547241,  0.00917053,  0.01520538, ..., -0.05145264,\n",
       "         0.01053619, -0.01382446]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context information provided does not mention or discuss anything related to democracy or saving it. Therefore, it is not possible to provide an accurate answer to this query based solely on the given context.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impromptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf\"\n",
    "folder = \"book\"\n",
    "filename = \"impromptu-rh.pdf\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Save the file\n",
    "with open(os.path.join(folder, filename), \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"PDF file downloaded and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down into 1 pdf per page\n",
    "import pypdf\n",
    "\n",
    "def pdf_to_pages(file):\n",
    "\t\"extract text (pages) from pdf file\"\n",
    "\tpages = []\n",
    "\tpdf = pypdf.PdfReader(file)\n",
    "\tfor p in range(len(pdf.pages)):\n",
    "\t\tpage = pdf.pages[p]\n",
    "\t\ttext = page.extract_text()\n",
    "\t\tpages += [text]\n",
    "\treturn pages\n",
    "\n",
    "file = \"book/impromptu-rh.pdf\"\n",
    "pages = pdf_to_pages(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "EDUCATION\n",
      "If Hollywood central casting ever wants to portray a \n",
      "beloved instructor from an idealized past, they could do worse \n",
      "than University of Texas at Austin Professor Steven Mintz. Over \n",
      "four decades of teaching, Professor Mintz has published books \n",
      "and articles on topics as diverse as the psychology of prominent \n",
      "Anglo-American literary families and political good vs. evil.\n",
      "In collared shirts, with graying hair, Mintz can’t suppress his \n",
      "smile as he teaches. Students adore him: among hundreds who \n",
      "have anonymously rated Mintz online, his average rating is a \n",
      "perfect five out of five, with posts such as “easily the best orator \n",
      "I’ve ever witnessed,” “his lectures feel more like storytelling \n",
      "than class,” and “passionate about what he teaches.”\n",
      "Professor Mintz, frankly, excelled as a professor long before the \n",
      "development of LLMs. So you might have expected him to have \n",
      "reacted with indifference or hostility to the late 2022 public \n",
      "release of GPT-4’s cousin, ChatGPT.\n",
      "Instead, this seventy-year-old scholar had the same reaction \n",
      "that I did when he saw the power of GPT: he wanted to use it. \n",
      "Right away.\n"
     ]
    }
   ],
   "source": [
    "print(pages[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"book/chap1\"):\n",
    "    os.makedirs(\"book/chap1\")\n",
    "for p in pages[31:53]: # PDF pages 32-54 - chapter 1 on Education\n",
    "    with open(f\"book/chap1/impromptu-rh_ch1_p{pages.index(p)+1}.txt\",mode='w',encoding='utf-8') as f:\n",
    "        f.write(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pages[31:53]\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "          model=\"mistral-embed\",\n",
    "          inputs=input\n",
    "      )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI can be used in various ways for education. For instance, teachers can use AI to create personalized learning paths for students, analyze their prior knowledge, skills, interests, and goals, and generate adaptive and engaging content, activities, and assessments that match their needs and preferences. Large language models can also help design and facilitate collaborative learning experiences for large classes of students, provide feedback and guidance, and help facilitate interactive discussions or debates among students on various topics or issues. Additionally, AI can be used to provide students with immediate personalized feedback on their work, freeing up teachers to focus on other aspects of instruction. AI can also help teachers create customized quizzes or tests for each student based on their learning goals, progress, and preferences, and provide immediate feedback for each answer. Furthermore, AI can be used to develop customized lesson plans and activity guides based on teachers' preferences and suggest personalized interventions or strategies to address specific learning challenges. Finally, AI can help synthesize a wide range of resources and materials that suit teachers' curriculum goals and pedagogical approaches.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can AI be used for Education?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "D, I = index.search(question_embeddings, k=2) # distance, index\n",
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n",
    "run_mistral(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
